
@article{kang_ganwriting_2020,
	title = {{GANwriting}: Content-Conditioned Generation of Styled Handwritten Word Images},
	url = {http://arxiv.org/abs/2003.02567},
	shorttitle = {{GANwriting}},
	abstract = {Although current image generation methods have reached impressive quality levels, they are still unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing by hand, a great variability is observed across different writers, and even when analyzing words scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a step closer to producing realistic and varied artificially rendered handwritten words. We propose a novel method that is able to produce credible handwritten word images by conditioning the generative process with both calligraphic style features and textual content. Our generator is guided by three complementary learning objectives: to produce realistic images, to imitate a certain handwriting style and to convey a specific textual content. Our model is unconstrained to any predefined vocabulary, being able to render whatever input word. Given a sample writer, it is also able to mimic its calligraphic features in a few-shot setup. We significantly advance over prior art and demonstrate with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically produced images.},
	journaltitle = {{arXiv}:2003.02567 [cs]},
	author = {Kang, Lei and Riba, Pau and Wang, Yaxing and Rusiñol, Marçal and Fornés, Alicia and Villegas, Mauricio},
	urldate = {2020-03-06},
	date = {2020-03-05},
	eprinttype = {arxiv},
	eprint = {2003.02567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/WB9XFM33/2003.html:text/html}
}

@article{neumann_evolutionary_2020,
	title = {Evolutionary Image Transition and Painting Using Random Walks},
	url = {http://arxiv.org/abs/2003.01517},
	abstract = {We present a study demonstrating how random walk algorithms can be used for evolutionary image transition. We design different mutation operators based on uniform and biased random walks and study how their combination with a baseline mutation operator can lead to interesting image transition processes in terms of visual effects and artistic features. Using feature-based analysis we investigate the evolutionary image transition behaviour with respect to different features and evaluate the images constructed during the image transition process. Afterwards, we investigate how modifications of our biased random walk approaches can be used for evolutionary image painting. We introduce an evolutionary image painting approach whose underlying biased random walk can be controlled by a parameter influencing the bias of the random walk and thereby creating different artistic painting effects.},
	journaltitle = {{arXiv}:2003.01517 [cs]},
	author = {Neumann, Aneta and Alexander, Bradley and Neumann, Frank},
	urldate = {2020-03-04},
	date = {2020-03-02},
	eprinttype = {arxiv},
	eprint = {2003.01517},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing}
}

@article{he_momentum_2019,
	title = {Momentum Contrast for Unsupervised Visual Representation Learning},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast ({MoCo}) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. {MoCo} provides competitive results under the common linear protocol on {ImageNet} classification. More importantly, the representations learned by {MoCo} transfer well to downstream tasks. {MoCo} can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on {PASCAL} {VOC}, {COCO}, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	journaltitle = {{arXiv}:1911.05722 [cs]},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	urldate = {2020-03-03},
	date = {2019-11-14},
	eprinttype = {arxiv},
	eprint = {1911.05722},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/KPKWZ28X/1911.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/YBFDUNZ4/He et al. - 2019 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf}
}

@article{tian_contrastive_2019,
	title = {Contrastive Multiview Coding},
	url = {http://arxiv.org/abs/1906.05849},
	abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a "dog" can be seen, heard, and felt). We hypothesize that a powerful representation is one that models view-invariant factors. Based on this hypothesis, we investigate a contrastive coding scheme, in which a self-supervsied representation is learned that aims to maximize mutual information between different views but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. The resulting learned representations perform above the state of the art for downstream tasks such as object classification, compared to formulations based on predictive learning or single view reconstruction, and improve as more views are added. On the Imagenet linear readoff benchmark, we achieve 68.4\% top-1 and 88.2\% top-5 accuracies. Code and reference implementations are released on our project page: http://github.com/{HobbitLong}/{CMC}/.},
	journaltitle = {{arXiv}:1906.05849 [cs]},
	author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	urldate = {2020-03-03},
	date = {2019-10-20},
	eprinttype = {arxiv},
	eprint = {1906.05849},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/GKXK68D5/1906.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/CBC39XKM/Tian et al. - 2019 - Contrastive Multiview Coding.pdf:application/pdf}
}

@article{gartner_multi-instance_2003,
	title = {Multi-Instance Kernels},
	abstract = {Learning from structured data is becoming increasingly important. However, most prior work on kernel methods has focused on learning from attribute-value data. Only recently, research started investigating kernels for structured data. This paper considers kernels for multi-instance problems - a class of concepts on individuals represented by sets. The main result of this paper is a kernel on multi-instance data that can be shown to separate positive and negative sets under natural assumptions. This kernel compares favorably with state of the art multi-instance learning algorithms in an empirical study. Finally, we give some concluding remarks and propose future work that might further improve the results.},
	journaltitle = {Proceedings of 19th International Conference on Machine Learning},
	shortjournal = {Proceedings of 19th International Conference on Machine Learning},
	author = {Gartner, Thomas and Flach, Peter and Kowalczyk, Adam and Smola, Alex},
	date = {2003-11-14},
	keywords = {toread},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/D666IAT2/Gartner et al. - 2003 - Multi-Instance Kernels.pdf:application/pdf}
}

@article{amores_multiple_2013,
	title = {Multiple instance classification: Review, taxonomy and comparative study},
	doi = {10.1016/j.artint.2013.06.003},
	shorttitle = {Multiple instance classification},
	abstract = {Multiple Instance Learning ({MIL}) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classification task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new {MIL} methods.},
	journaltitle = {Artif. Intell.},
	author = {Amores, Jaume},
	date = {2013},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/EVVE4RXL/Amores - 2013 - Multiple instance classification Review, taxonomy.pdf:application/pdf}
}

@article{shao_channel_2020,
	title = {Channel Equilibrium Networks for Learning Deep Representation},
	url = {http://arxiv.org/abs/2003.00214},
	abstract = {Convolutional Neural Networks ({CNNs}) are typically constructed by stacking multiple building blocks, each of which contains a normalization layer such as batch normalization ({BN}) and a rectified linear function such as {ReLU}. However, this work shows that the combination of normalization and rectified linear function leads to inhibited channels, which have small magnitude and contribute little to the learned feature representation, impeding the generalization ability of {CNNs}. Unlike prior arts that simply removed the inhibited channels, we propose to "wake them up" during training by designing a novel neural building block, termed Channel Equilibrium ({CE}) block, which enables channels at the same layer to contribute equally to the learned representation. We show that {CE} is able to prevent inhibited channels both empirically and theoretically. {CE} has several appealing benefits. (1) It can be integrated into many advanced {CNN} architectures such as {ResNet} and {MobileNet}, outperforming their original networks. (2) {CE} has an interesting connection with the Nash Equilibrium, a well-known solution of a non-cooperative game. (3) Extensive experiments show that {CE} achieves state-of-the-art performance on various challenging benchmarks such as {ImageNet} and {COCO}.},
	journaltitle = {{arXiv}:2003.00214 [cs]},
	author = {Shao, Wenqi and Tang, Shitao and Pan, Xingang and Tan, Ping and Wang, Xiaogang and Luo, Ping},
	urldate = {2020-03-03},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {2003.00214},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/CYMRGFBE/2003.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/LUHAULEM/Shao et al. - 2020 - Channel Equilibrium Networks for Learning Deep Rep.pdf:application/pdf}
}

@article{ohkawa_augmented_2020,
	title = {Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image Translation},
	url = {http://arxiv.org/abs/2003.00187},
	abstract = {Unpaired image-to-image (I2I) translation has received considerable attention in pattern recognition and computer vision because of recent advancements in generative adversarial networks ({GANs}). However, due to the lack of explicit supervision, unpaired I2I models often fail to generate realistic images, especially in challenging datasets with different backgrounds and poses. Hence, stabilization is indispensable for real-world applications and {GANs}. Herein, we propose Augmented Cyclic Consistency Regularization ({ACCR}), a novel regularization method for unpaired I2I translation. Our main idea is to enforce consistency regularization originating from semi-supervised learning on the discriminators leveraging real, fake, reconstructed, and augmented samples. We regularize the discriminators to output similar predictions when fed pairs of original and perturbed images. We qualitatively clarify the generation property between unpaired I2I models and standard {GANs}, and explain why consistency regularization on fake and reconstructed samples works well. Quantitatively, our method outperforms the consistency regularized {GAN} ({CR}-{GAN}) in digit translations and demonstrates efficacy against several data augmentation variants and cycle-consistent constraints.},
	journaltitle = {{arXiv}:2003.00187 [cs]},
	author = {Ohkawa, Takehiko and Inoue, Naoto and Kataoka, Hirokatsu and Inoue, Nakamasa},
	urldate = {2020-03-03},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {2003.00187},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/YUVNVSEX/2003.html:text/html}
}

@article{hausser_tumour_2020,
	title = {Tumour heterogeneity and the evolutionary trade-offs of cancer},
	issn = {1474-175X, 1474-1768},
	url = {http://www.nature.com/articles/s41568-020-0241-6},
	doi = {10.1038/s41568-020-0241-6},
	abstract = {Tumours vary in gene expression programmes and genetic alterations. Understanding this diversity and its biological meaning requires a theoretical framework, which could in turn guide the development of more accurate prognosis and therapy. Here, we review the theory of multi-task evolution of cancer, which is based upon the premise that tumours evolve in the host and face selection trade-o ffs between multiple biological functions. This theory can help identify the major biological tasks that cancer cells perform and the trade-offs between these tasks. It introduces the concept of specialist tumours, which focus on one task, and generalist tumours, which perform several tasks. Specialist tumours are suggested to be sensitive to therapy targeting their main task. Driver mutations tune gene expression towards specific tasks in a tissue-dependent manner and thus help to determine whether a tumour is specialist or generalist. We discuss potential applications of the theory of multi-t ask evolution to interpret the spatial organization of tumours and intratumour heterogeneity.},
	journaltitle = {Nature Reviews Cancer},
	shortjournal = {Nat Rev Cancer},
	author = {Hausser, Jean and Alon, Uri},
	urldate = {2020-03-03},
	date = {2020-02-24},
	langid = {english},
	file = {s41568-020-0241-6.pdf:/home/lazard/Documents/cbio/papiers/s41568-020-0241-6.pdf:application/pdf}
}

@article{frankle_training_2020,
	title = {Training {BatchNorm} and Only {BatchNorm}: On the Expressive Power of Random Features in {CNNs}},
	url = {http://arxiv.org/abs/2003.00152},
	shorttitle = {Training {BatchNorm} and Only {BatchNorm}},
	abstract = {Batch normalization ({BatchNorm}) has become an indispensable tool for training deep neural networks, yet it is still poorly understood. Although previous work has typically focused on its normalization component, {BatchNorm} also adds two per-feature trainable parameters: a coefficient and a bias. However, the role and expressive power of these parameters remains unclear. To study this question, we investigate the performance achieved when training only these parameters and freezing all others at their random initializations. We find that doing so leads to surprisingly high performance. For example, a sufficiently deep {ResNet} reaches 83\% accuracy on {CIFAR}-10 in this configuration. Interestingly, {BatchNorm} achieves this performance in part by naturally learning to disable around a third of the random features without any changes to the training objective. Not only do these results highlight the under-appreciated role of the affine parameters in {BatchNorm}, but - in a broader sense - they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.},
	journaltitle = {{arXiv}:2003.00152 [cs, stat]},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	urldate = {2020-03-03},
	date = {2020-02-28},
	eprinttype = {arxiv},
	eprint = {2003.00152},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/QVJUN3WS/2003.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/V8V8G7M3/Frankle et al. - 2020 - Training BatchNorm and Only BatchNorm On the Expr.pdf:application/pdf}
}

@article{henaff_data-efficient_2019,
	title = {Data-Efficient Image Recognition with Contrastive Predictive Coding},
	url = {https://openreview.net/forum?id=rJerHlrYwH},
	abstract = {Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient...},
	author = {Henaff, Olivier J. and Srinivas, Aravind and Fauw, Jeffrey De and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and Oord, Aaron van den},
	urldate = {2020-03-02},
	date = {2019-09-25},
	file = {Snapshot:/Users/trislaz/Zotero/storage/XY392JWL/forum.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/KAKIURLQ/Henaff et al. - 2019 - Data-Efficient Image Recognition with Contrastive .pdf:application/pdf}
}

@article{chen_simple_2020,
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents {SimCLR}: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in deﬁning effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning beneﬁts from larger batch sizes and more training steps compared to supervised learning. By combining these ﬁndings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on {ImageNet}. A linear classiﬁer trained on self-supervised representations learned by {SimCLR} achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-ofthe-art, matching the performance of a supervised {ResNet}-50. When ﬁne-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming {AlexNet} with 100× fewer labels.},
	journaltitle = {{arXiv}:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	urldate = {2020-02-27},
	date = {2020-02-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.05709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{yang_rethinking_2020,
	title = {Rethinking Bias-Variance Trade-off for Generalization of Neural Networks},
	url = {http://arxiv.org/abs/2002.11328},
	abstract = {The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.},
	journaltitle = {{arXiv}:2002.11328 [cs, stat]},
	author = {Yang, Zitong and Yu, Yaodong and You, Chong and Steinhardt, Jacob and Ma, Yi},
	urldate = {2020-02-27},
	date = {2020-02-26},
	eprinttype = {arxiv},
	eprint = {2002.11328},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/A4F9ALUL/2002.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/T5FJCNHJ/Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf:application/pdf}
}

@article{unterthiner_predicting_nodate,
	title = {Predicting Neural Network Accuracy from Weights},
	abstract = {We study the prediction of the accuracy of a neural network given only its weights with the goal of better understanding network training and performance. To do so, we propose a formal setting which frames this task and connects to previous work in this area. We collect (and release) a large dataset of almost 80k convolutional neural networks trained on four image datasets. We demonstrate that strong predictors of accuracy exist. Moreover, they can achieve good predictions while only using simple statistics of the weights. Surprisingly, these predictors are able to rank networks trained on unobserved datasets or using different architectures.},
	pages = {17},
	author = {Unterthiner, Thomas and Keysers, Daniel and Gelly, Sylvain and Bousquet, Olivier and Tolstikhin, Ilya},
	langid = {english},
	file = {Unterthiner et al. - Predicting Neural Network Accuracy from Weights.pdf:/Users/trislaz/Zotero/storage/DM2W3DVP/Unterthiner et al. - Predicting Neural Network Accuracy from Weights.pdf:application/pdf}
}

@article{popova_ploidy_2012,
	title = {Ploidy and Large-Scale Genomic Instability Consistently Identify Basal-like Breast Carcinomas with {BRCA}1/2 Inactivation},
	volume = {72},
	issn = {0008-5472, 1538-7445},
	url = {http://cancerres.aacrjournals.org/cgi/doi/10.1158/0008-5472.CAN-12-1470},
	doi = {10.1158/0008-5472.CAN-12-1470},
	abstract = {{BRCA}1 inactivation is a frequent event in basal-like breast carcinomas ({BLC}). However, {BRCA}1 can be inactivated by multiple mechanisms and determining its status is not a trivial issue. As an alternate approach, we proﬁled 65 {BLC} cases using single-nucleotide polymorphism arrays to deﬁne a signature of {BRCA}1-associated genomic instability. Large-scale state transitions ({LST}), deﬁned as chromosomal break between adjacent regions of at least 10 Mb, were found to be a robust indicator of {BRCA}1 status in this setting. Two major ploidy-speciﬁc cutoffs in {LST} distributions were sufﬁcient to distinguish highly rearranged {BLCs} with 85\% of proven {BRCA}1-inactivated cases from less rearranged {BLCs} devoid of proven {BRCA}1-inactivated cases. The genomic signature we deﬁned was validated in a second independent series of 55 primary {BLC} cases and 17 {BLC}-derived tumor cell lines. High numbers of {LSTs} resembling {BRCA}1inactivated {BLC} were observed in 4 primary {BLC} cases and 2 {BLC} cell lines that harbored {BRCA}2 mutations. Overall, the genomic signature we deﬁned predicted {BRCA}1/2 inactivation in {BLCs} with 100\% sensitivity and 90\% speciﬁcity (97\% accuracy). This assay may ease the challenge of selecting patients for genetic testing or recruitment to clinical trials of novel emerging therapies that target {DNA} repair deﬁciencies in cancer. Cancer Res; 72(21); 5454–62. Ó2012 {AACR}.},
	pages = {5454--5462},
	number = {21},
	journaltitle = {Cancer Research},
	shortjournal = {Cancer Research},
	author = {Popova, T. and Manie, E. and Rieunier, G. and Caux-Moncoutier, V. and Tirapo, C. and Dubois, T. and Delattre, O. and Sigal-Zafrani, B. and Bollet, M. and Longy, M. and Houdayer, C. and Sastre-Garau, X. and Vincent-Salomon, A. and Stoppa-Lyonnet, D. and Stern, M.-H.},
	urldate = {2020-02-17},
	date = {2012-11-01},
	langid = {english},
	file = {Popova et al. - 2012 - Ploidy and Large-Scale Genomic Instability Consist.pdf:/Users/trislaz/Zotero/storage/QZJLB4DY/Popova et al. - 2012 - Ploidy and Large-Scale Genomic Instability Consist.pdf:application/pdf}
}

@article{denil_predicting_nodate,
	title = {Predicting Parameters in Deep Learning},
	abstract = {We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95\% of the weights of a network without any drop in accuracy.},
	pages = {9},
	author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio},
	langid = {english},
	file = {Denil et al. - Predicting Parameters in Deep Learning.pdf:/Users/trislaz/Zotero/storage/VY9HH26Z/Denil et al. - Predicting Parameters in Deep Learning.pdf:application/pdf}
}

@article{kanaar_molecular_1998,
	title = {Molecular mechanisms of {DNA} double- strand break repair},
	volume = {8},
	pages = {7},
	journaltitle = {trends in {CELL} {BIOLOGY}},
	author = {Kanaar, Roland and Hoeijmakers, Jan H J and van Gent, Dik C},
	date = {1998},
	langid = {english},
	file = {kanaar1998.pdf:/home/lazard/Documents/cbio/papiers/kanaar1998.pdf:application/pdf}
}

@article{pfeiffer_mechanisms_2000,
	title = {Mechanisms of {DNA} double-strand break repair and their potential to induce chromosomal aberrations},
	volume = {15},
	issn = {0267-8357},
	url = {https://academic.oup.com/mutage/article/15/4/289/1143400},
	doi = {10.1093/mutage/15.4.289},
	abstract = {Abstract.  {DNA} double-strand breaks ({DSB}) are considered to be critical primary lesions in the formation of chromosomal aberrations. {DSB} may be induced by exoge},
	pages = {289--302},
	number = {4},
	journaltitle = {Mutagenesis},
	shortjournal = {Mutagenesis},
	author = {Pfeiffer, Petra and Goedecke, Wolfgang and Obe, Günter},
	urldate = {2020-02-13},
	date = {2000-07-01},
	langid = {english},
	file = {Snapshot:/Users/trislaz/Zotero/storage/JBGICQ5H/1143400.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/2NDUGPXH/Pfeiffer et al. - 2000 - Mechanisms of DNA double-strand break repair and t.pdf:application/pdf}
}

@article{chang_non-homologous_2017,
	title = {Non-homologous {DNA} end joining and alternative pathways to double-strand break repair},
	volume = {18},
	doi = {10.1038/nrm.2017.48},
	abstract = {{DNA} double-strand breaks ({DSBs}) are the most dangerous type of {DNA} damage because they can result in the loss of large chromosomal regions. In all mammalian cells, {DSBs} that occur throughout the cell cycle are repaired predominantly by the non-homologous {DNA} end joining ({NHEJ}) pathway. Defects in {NHEJ} result in sensitivity to ionizing radiation and the ablation of lymphocytes. The {NHEJ} pathway utilizes proteins that recognize, resect, polymerize and ligate the {DNA} ends in a flexible manner. This flexibility permits {NHEJ} to function on a wide range of {DNA}-end configurations, with the resulting repaired {DNA} junctions often containing mutations. In this Review, we discuss the most recent findings regarding the relative involvement of the different {NHEJ} proteins in the repair of various {DNA}-end configurations. We also discuss the shunting of {DNA}-end repair to the auxiliary pathways of alternative end joining (a-{EJ}) or single-strand annealing ({SSA}) and the relevance of these different pathways to human disease.},
	journaltitle = {Nature reviews. Molecular cell biology},
	shortjournal = {Nature reviews. Molecular cell biology},
	author = {Chang, Howard and Pannunzio, Nicholas and Adachi, Noritaka and Lieber, Michael},
	date = {2017-05-17},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/E3WMAC63/Chang et al. - 2017 - Non-homologous DNA end joining and alternative pat.pdf:application/pdf}
}

@article{featherstone_dna_nodate,
	title = {{DNA} double-strand break repair},
	pages = {3},
	author = {Featherstone, Carol and Jackson, Stephen P},
	langid = {english},
	file = {Featherstone et Jackson - DNA double-strand break repair.pdf:/Users/trislaz/Zotero/storage/JEMPGNSP/Featherstone et Jackson - DNA double-strand break repair.pdf:application/pdf}
}

@article{doersch_tutorial_2016,
	title = {Tutorial on Variational Autoencoders},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], {CIFAR} images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	journaltitle = {{arXiv}:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	urldate = {2020-02-06},
	date = {2016-08-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.05908},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:/Users/trislaz/Zotero/storage/574HUNW7/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf}
}

@article{foulds_review_2010,
	title = {A review of multi-instance learning assumptions},
	volume = {25},
	issn = {0269-8889, 1469-8005},
	url = {https://www.cambridge.org/core/product/identifier/S026988890999035X/type/journal_article},
	doi = {10.1017/S026988890999035X},
	abstract = {Multi-instance ({MI}) learning is a variant of inductive machine learning where each learning example contains a bag of instances instead of a single feature vector. The term commonly refers to the supervised setting, where each bag is associated with a label. This type of representation is a natural ﬁt for a number of real-world learning scenarios, including drug activity prediction and image classiﬁcation, hence many multi-instance learning algorithms have been proposed. Any {MI} learning method must relate instances to bag-level class labels, but many types of relationships between instances and class labels are possible. Although all early work in {MI} learning assumes a speciﬁc {MI} concept class known to be appropriate for a drug activity prediction domain, this “standard {MI} assumption” is not guaranteed to hold in other domains. Much of the recent work in {MI} learning has concentrated on a relaxed view of the {MI} problem, where the standard {MI} assumption is dropped, and alternative assumptions are considered instead. However, often it is not clearly stated what particular assumption is used and how it relates to other assumptions that have been proposed. In this paper, we aim to clarify the use of alternative {MI} assumptions by reviewing the work done in this area.},
	pages = {1--25},
	number = {1},
	journaltitle = {The Knowledge Engineering Review},
	shortjournal = {The Knowledge Engineering Review},
	author = {Foulds, James and Frank, Eibe},
	urldate = {2020-02-05},
	date = {2010-03},
	langid = {english},
	file = {Foulds et Frank - 2010 - A review of multi-instance learning assumptions.pdf:/Users/trislaz/Zotero/storage/WJWTEVB3/Foulds et Frank - 2010 - A review of multi-instance learning assumptions.pdf:application/pdf;FouldsAndFrankMIreview.pdf:/Users/trislaz/Documents/papiers/FouldsAndFrankMIreview.pdf:application/pdf}
}

@article{dimitriou_deep_2019,
	title = {Deep Learning for Whole Slide Image Analysis: An Overview},
	volume = {6},
	issn = {2296-858X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6882930/},
	doi = {10.3389/fmed.2019.00264},
	shorttitle = {Deep Learning for Whole Slide Image Analysis},
	abstract = {The widespread adoption of whole slide imaging has increased the demand for effective and efficient gigapixel image analysis. Deep learning is at the forefront of computer vision, showcasing significant improvements over previous methodologies on visual understanding. However, whole slide images have billions of pixels and suffer from high morphological heterogeneity as well as from different types of artifacts. Collectively, these impede the conventional use of deep learning. For the clinical translation of deep learning solutions to become a reality, these challenges need to be addressed. In this paper, we review work on the interdisciplinary attempt of training deep neural networks using whole slide images, and highlight the different ideas underlying these methodologies.},
	journaltitle = {Frontiers in Medicine},
	shortjournal = {Front Med (Lausanne)},
	author = {Dimitriou, Neofytos and Arandjelović, Ognjen and Caie, Peter D.},
	urldate = {2020-02-04},
	date = {2019-11-22},
	pmid = {31824952},
	pmcid = {PMC6882930},
	file = {PubMed Central Full Text PDF:/Users/trislaz/Zotero/storage/U28MNZSS/Dimitriou et al. - 2019 - Deep Learning for Whole Slide Image Analysis An O.pdf:application/pdf}
}

@article{bejnordi_context-aware_2017,
	title = {Context-aware stacked convolutional neural networks for classification of breast carcinomas in whole-slide histopathology images},
	volume = {4},
	issn = {2329-4302},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5729919/},
	doi = {10.1117/1.JMI.4.4.044504},
	abstract = {Currently, histopathological tissue examination by a pathologist represents the gold standard for breast lesion diagnostics. Automated classification of histopathological whole-slide images ({WSIs}) is challenging owing to the wide range of appearances of benign lesions and the visual similarity of ductal carcinoma in-situ ({DCIS}) to invasive lesions at the cellular level. Consequently, analysis of tissue at high resolutions with a large contextual area is necessary. We present context-aware stacked convolutional neural networks ({CNN}) for classification of breast {WSIs} into normal/benign, {DCIS}, and invasive ductal carcinoma ({IDC}). We first train a {CNN} using high pixel resolution to capture cellular level information. The feature responses generated by this model are then fed as input to a second {CNN}, stacked on top of the first. Training of this stacked architecture with large input patches enables learning of fine-grained (cellular) details and global tissue structures. Our system is trained and evaluated on a dataset containing 221 {WSIs} of hematoxylin and eosin stained breast tissue specimens. The system achieves an {AUC} of 0.962 for the binary classification of nonmalignant and malignant slides and obtains a three-class accuracy of 81.3\% for classification of {WSIs} into normal/benign, {DCIS}, and {IDC}, demonstrating its potential for routine diagnostics.},
	number = {4},
	journaltitle = {Journal of Medical Imaging},
	shortjournal = {J Med Imaging (Bellingham)},
	author = {Bejnordi, Babak Ehteshami and Zuidhof, Guido and Balkenhol, Maschenka and Hermsen, Meyke and Bult, Peter and van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and van der Laak, Jeroen},
	urldate = {2020-02-04},
	date = {2017-10},
	pmid = {29285517},
	pmcid = {PMC5729919},
	keywords = {toread},
	file = {PubMed Central Full Text PDF:/Users/trislaz/Zotero/storage/PDF9SWFD/Bejnordi et al. - 2017 - Context-aware stacked convolutional neural network.pdf:application/pdf}
}

@article{feng_breast_2018,
	title = {Breast cancer cell nuclei classification in histopathology images using deep neural networks},
	volume = {13},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-017-1663-9},
	doi = {10.1007/s11548-017-1663-9},
	abstract = {{PurposeCell} nuclei classification in breast cancer histopathology images plays an important role in effective diagnose since breast cancer can often be characterized by its expression in cell nuclei. However, due to the small and variant sizes of cell nuclei, and heavy noise in histopathology images, traditional machine learning methods cannot achieve desirable recognition accuracy. To address this challenge, this paper aims to present a novel deep neural network which performs representation learning and cell nuclei recognition in an end-to-end manner.{MethodsThe} proposed model hierarchically maps raw medical images into a latent space in which robustness is achieved by employing a stacked denoising autoencoder. A supervised classifier is further developed to improve the discrimination of the model by maximizing inter-subject separability in the latent space. The proposed method involves a cascade model which jointly learns a set of nonlinear mappings and a classifier from the given raw medical images. Such an on-the-shelf learning strategy makes obtaining discriminative features possible, thus leading to better recognition performance.{ResultsExtensive} experiments with benign and malignant breast cancer datasets are conducted to verify the effectiveness of the proposed method. Better performance was obtained when compared with other feature extraction methods, and higher recognition rate was achieved when compared with other seven classification methods.{ConclusionsWe} propose an end-to-end {DNN} model for cell nuclei and non-nuclei classification of histopathology images. It demonstrates that the proposed method can achieve promising performance in cell nuclei classification, and the proposed method is suitable for the cell nuclei classification task.},
	pages = {179--191},
	number = {2},
	journaltitle = {International Journal of Computer Assisted Radiology and Surgery},
	shortjournal = {Int J {CARS}},
	author = {Feng, Yangqin and Zhang, Lei and Yi, Zhang},
	urldate = {2019-11-28},
	date = {2018-02-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/trislaz/Zotero/storage/X8YCN65L/Feng et al. - 2018 - Breast cancer cell nuclei classification in histop.pdf:application/pdf;Springer Full Text PDF:/Users/trislaz/Zotero/storage/28TBGSYB/Feng et al. - 2018 - Breast cancer cell nuclei classification in histop.pdf:application/pdf}
}

@article{bekkers_roto-translation_2018,
	title = {Roto-Translation Covariant Convolutional Networks for Medical Image Analysis},
	url = {http://arxiv.org/abs/1804.03393},
	abstract = {We propose a framework for rotation and translation covariant deep learning using \${SE}(2)\$ group convolutions. The group product of the special Euclidean motion group \${SE}(2)\$ describes how a concatenation of two roto-translations results in a net roto-translation. We encode this geometric structure into convolutional neural networks ({CNNs}) via \${SE}(2)\$ group convolutional layers, which fit into the standard 2D {CNN} framework, and which allow to generically deal with rotated input samples without the need for data augmentation. We introduce three layers: a lifting layer which lifts a 2D (vector valued) image to an \${SE}(2)\$-image, i.e., 3D (vector valued) data whose domain is \${SE}(2)\$; a group convolution layer from and to an \${SE}(2)\$-image; and a projection layer from an \${SE}(2)\$-image to a 2D image. The lifting and group convolution layers are \${SE}(2)\$ covariant (the output roto-translates with the input). The final projection layer, a maximum intensity projection over rotations, makes the full {CNN} rotation invariant. We show with three different problems in histopathology, retinal imaging, and electron microscopy that with the proposed group {CNNs}, state-of-the-art performance can be achieved, without the need for data augmentation by rotation and with increased performance compared to standard {CNNs} that do rely on augmentation.},
	journaltitle = {{arXiv}:1804.03393 [cs, math]},
	author = {Bekkers, Erik J. and Lafarge, Maxime W. and Veta, Mitko and Eppenhof, Koen {AJ} and Pluim, Josien {PW} and Duits, Remco},
	urldate = {2019-04-04},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1804.03393},
	file = {Bekkers et al. - 2018 - Roto-Translation Covariant Convolutional Networks .pdf:/Users/trislaz/Zotero/storage/AQ6X9FXE/Bekkers et al. - 2018 - Roto-Translation Covariant Convolutional Networks .pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/RJNV4TXX/1804.html:text/html;arXiv\:1804.03393 PDF:/Users/trislaz/Zotero/storage/UWLWKK48/Bekkers et al. - 2018 - Roto-Translation Covariant Convolutional Networks .pdf:application/pdf}
}

@article{alizadeh_toward_2015,
	title = {Toward understanding and exploiting tumor heterogeneity},
	volume = {21},
	issn = {1546-170X},
	doi = {10.1038/nm.3915},
	abstract = {The extent of tumor heterogeneity is an emerging theme that researchers are only beginning to understand. How genetic and epigenetic heterogeneity affects tumor evolution and clinical progression is unknown. The precise nature of the environmental factors that influence this heterogeneity is also yet to be characterized. Nature Medicine, Nature Biotechnology and the Volkswagen Foundation organized a meeting focused on identifying the obstacles that need to be overcome to advance translational research in and tumor heterogeneity. Once these key questions were established, the attendees devised potential solutions. Their ideas are presented here.},
	pages = {846--853},
	number = {8},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat. Med.},
	author = {Alizadeh, Ash A. and Aranda, Victoria and Bardelli, Alberto and Blanpain, Cedric and Bock, Christoph and Borowski, Christine and Caldas, Carlos and Califano, Andrea and Doherty, Michael and Elsner, Markus and Esteller, Manel and Fitzgerald, Rebecca and Korbel, Jan O. and Lichter, Peter and Mason, Christopher E. and Navin, Nicholas and Pe'er, Dana and Polyak, Kornelia and Roberts, Charles W. M. and Siu, Lillian and Snyder, Alexandra and Stower, Hannah and Swanton, Charles and Verhaak, Roel G. W. and Zenklusen, Jean C. and Zuber, Johannes and Zucman-Rossi, Jessica},
	date = {2015-08},
	pmid = {26248267},
	pmcid = {PMC4785013},
	file = {alizadeh2015.pdf:/home/lazard/Documents/cbio/papiers/alizadeh2015.pdf:application/pdf;alizadeh2015.pdf:/home/lazard/Documents/cbio/papiers/alizadeh2015.pdf:application/pdf;Version acceptée:/Users/trislaz/Zotero/storage/G2D4DC4U/Alizadeh et al. - 2015 - Toward understanding and exploiting tumor heteroge.pdf:application/pdf}
}

@article{mnih_recurrent_2014,
	title = {Recurrent Models of Visual Attention},
	url = {http://arxiv.org/abs/1406.6247},
	abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
	journaltitle = {{arXiv}:1406.6247 [cs, stat]},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
	urldate = {2020-01-27},
	date = {2014-06-24},
	eprinttype = {arxiv},
	eprint = {1406.6247}
}

@article{xu_look_2019,
	title = {Look, Investigate, and Classify: A Deep Hybrid Attention Method for Breast Cancer Classification},
	url = {http://arxiv.org/abs/1902.10946},
	shorttitle = {Look, Investigate, and Classify},
	abstract = {One issue with computer based histopathology image analysis is that the size of the raw image is usually very large. Taking the raw image as input to the deep learning model would be computationally expensive while resizing the raw image to low resolution would incur information loss. In this paper, we present a novel deep hybrid attention approach to breast cancer classification. It first adaptively selects a sequence of coarse regions from the raw image by a hard visual attention algorithm, and then for each such region it is able to investigate the abnormal parts based on a soft-attention mechanism. A recurrent network is then built to make decisions to classify the image region and also to predict the location of the image region to be investigated at the next time step. As the region selection process is non-differentiable, we optimize the whole network through a reinforcement approach to learn an optimal policy to classify the regions. Based on this novel Look, Investigate and Classify approach, we only need to process a fraction of the pixels in the raw image resulting in significant saving in computational resources without sacrificing performances. Our approach is evaluated on a public breast cancer histopathology database, where it demonstrates superior performance to the state-of-the-art deep learning approaches, achieving around 96{\textbackslash}\% classification accuracy while only 15\% of raw pixels are used.},
	journaltitle = {{arXiv}:1902.10946 [cs]},
	author = {Xu, Bolei and Liu, Jingxin and Hou, Xianxu and Liu, Bozhi and Garibaldi, Jon and Ellis, Ian O. and Green, Andy and Shen, Linlin and Qiu, Guoping},
	urldate = {2020-01-27},
	date = {2019-02-28},
	eprinttype = {arxiv},
	eprint = {1902.10946},
	file = {1902.10946.pdf:/home/lazard/Documents/cbio/papiers/1902.10946.pdf:application/pdf}
}

@article{katharopoulos_processing_2019,
	title = {Processing Megapixel Images with Deep Attention-Sampling Models},
	url = {http://arxiv.org/abs/1905.03711},
	abstract = {Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single {GPU} setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal {SGD} procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.},
	journaltitle = {{arXiv}:1905.03711 [cs, stat]},
	author = {Katharopoulos, Angelos and Fleuret, François},
	urldate = {2020-01-27},
	date = {2019-07-17},
	eprinttype = {arxiv},
	eprint = {1905.03711}
}

@article{yao_weakly_2018,
	title = {Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions},
	url = {http://arxiv.org/abs/1803.07703},
	abstract = {Diagnostic imaging often requires the simultaneous identification of a multitude of findings of varied size and appearance. Beyond global indication of said findings, the prediction and display of localization information improves trust in and understanding of results when augmenting clinical workflow. Medical training data rarely includes more than global image-level labels as segmentations are time-consuming and expensive to collect. We introduce an approach to managing these practical constraints by applying a novel architecture which learns at multiple resolutions while generating saliency maps with weak supervision. Further, we parameterize the Log-Sum-Exp pooling function with a learnable lower-bounded adaptation ({LSE}-{LBA}) to build in a sharpness prior and better handle localizing abnormalities of different sizes using only image-level labels. Applying this approach to interpreting chest x-rays, we set the state of the art on 9 abnormalities in the {NIH}'s {CXR}14 dataset while generating saliency maps with the highest resolution to date.},
	journaltitle = {{arXiv}:1803.07703 [cs]},
	author = {Yao, Li and Prosky, Jordan and Poblenz, Eric and Covington, Ben and Lyman, Kevin},
	urldate = {2020-01-27},
	date = {2018-03-20},
	eprinttype = {arxiv},
	eprint = {1803.07703}
}

@article{cheplygina_not-so-supervised_2019,
	title = {Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis},
	volume = {54},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518307588},
	doi = {10.1016/j.media.2019.03.009},
	shorttitle = {Not-so-supervised},
	pages = {280--296},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Cheplygina, Veronika and de Bruijne, Marleen and Pluim, Josien P.W.},
	urldate = {2020-01-27},
	date = {2019-05},
	langid = {english},
	file = {cheplygina2019.pdf:/home/lazard/Documents/cbio/papiers/cheplygina2019.pdf:application/pdf}
}

@article{courtiol_classification_2018,
	title = {Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach},
	url = {http://arxiv.org/abs/1802.02212},
	shorttitle = {Classification and Disease Localization in Histopathology Using Only Global Labels},
	abstract = {Analysis of histopathology slides is a critical step for many diagnoses, and in particular in oncology where it defines the gold standard. In the case of digital histopathological analysis, highly trained pathologists must review vast whole-slide-images of extreme digital resolution (\$100,000{\textasciicircum}2\$ pixels) across multiple zoom levels in order to locate abnormal regions of cells, or in some cases single cells, out of millions. The application of deep learning to this problem is hampered not only by small sample sizes, as typical datasets contain only a few hundred samples, but also by the generation of ground-truth localized annotations for training interpretable classification and segmentation models. We propose a method for disease localization in the context of weakly supervised learning, where only image-level labels are available during training. Even without pixel-level annotations, we are able to demonstrate performance comparable with models trained with strong annotations on the Camelyon-16 lymph node metastases detection challenge. We accomplish this through the use of pre-trained deep convolutional networks, feature embedding, as well as learning via top instances and negative evidence, a multiple instance learning technique from the field of semantic segmentation and object detection.},
	journaltitle = {{arXiv}:1802.02212 [cs, stat]},
	author = {Courtiol, Pierre and Tramel, Eric W. and Sanselme, Marc and Wainrib, Gilles},
	urldate = {2020-01-23},
	date = {2018-02-01},
	eprinttype = {arxiv},
	eprint = {1802.02212},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/MH9Q5RYM/1802.html:text/html;1802.02212.pdf:/Users/trislaz/Documents/cbio/papiers/1802.02212.pdf:application/pdf}
}

@article{xu_attention_2019,
	title = {Attention by Selection: A Deep Selective Attention Approach to Breast Cancer Classification},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8941117/},
	doi = {10.1109/TMI.2019.2962013},
	shorttitle = {Attention by Selection},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	shortjournal = {{IEEE} Trans. Med. Imaging},
	author = {Xu, Bolei and Liu, Jingxin and Hou, Xianxu and Liu, Bozhi and Garibaldi, Jon and Ellis, Ian O. and Green, Andy and Shen, Linlin and Qiu, Guoping},
	urldate = {2020-01-23},
	date = {2019},
	langid = {english},
	file = {Attention by Selection.pdf:/home/lazard/Documents/cbio/papiers/Attention by Selection.pdf:application/pdf}
}

@article{xu_attention_2019-1,
	title = {Attention by Selection: A Deep Selective Attention Approach to Breast Cancer Classification},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8941117/},
	doi = {10.1109/TMI.2019.2962013},
	shorttitle = {Attention by Selection},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	shortjournal = {{IEEE} Trans. Med. Imaging},
	author = {Xu, Bolei and Liu, Jingxin and Hou, Xianxu and Liu, Bozhi and Garibaldi, Jon and Ellis, Ian O. and Green, Andy and Shen, Linlin and Qiu, Guoping},
	urldate = {2020-01-23},
	date = {2019},
	langid = {english},
	file = {xu2019.pdf:/Users/trislaz/Documents/cbio/papiers/xu2019.pdf:application/pdf}
}

@article{wei_pathologist-level_2019,
	title = {Pathologist-level classification of histologic patterns on resected lung adenocarcinoma slides with deep neural networks},
	volume = {9},
	rights = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-40041-7},
	doi = {10.1038/s41598-019-40041-7},
	abstract = {Classification of histologic patterns in lung adenocarcinoma is critical for determining tumor grade and treatment for patients. However, this task is often challenging due to the heterogeneous nature of lung adenocarcinoma and the subjective criteria for evaluation. In this study, we propose a deep learning model that automatically classifies the histologic patterns of lung adenocarcinoma on surgical resection slides. Our model uses a convolutional neural network to identify regions of neoplastic cells, then aggregates those classifications to infer predominant and minor histologic patterns for any given whole-slide image. We evaluated our model on an independent set of 143 whole-slide images. It achieved a kappa score of 0.525 and an agreement of 66.6\% with three pathologists for classifying the predominant patterns, slightly higher than the inter-pathologist kappa score of 0.485 and agreement of 62.7\% on this test set. All evaluation metrics for our model and the three pathologists were within 95\% confidence intervals of agreement. If confirmed in clinical practice, our model can assist pathologists in improving classification of lung adenocarcinoma patterns by automatically pre-screening and highlighting cancerous regions prior to review. Our approach can be generalized to any whole-slide image classification task, and code is made publicly available at 
                  https://github.com/{BMIRDS}/deepslide
                  
                .},
	pages = {1--8},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Wei, Jason W. and Tafe, Laura J. and Linnik, Yevgeniy A. and Vaickus, Louis J. and Tomita, Naofumi and Hassanpour, Saeed},
	urldate = {2020-01-23},
	date = {2019-03-04},
	langid = {english},
	file = {Snapshot:/Users/trislaz/Zotero/storage/SKZVABAH/s41598-019-40041-7.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/CPY6YBHY/Wei et al. - 2019 - Pathologist-level classification of histologic pat.pdf:application/pdf}
}

@article{qaiser_learning_2019,
	title = {Learning Where to See: A Novel Attention Model for Automated Immunohistochemical Scoring},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8672928/},
	doi = {10.1109/TMI.2019.2907049},
	shorttitle = {Learning Where to See},
	pages = {2620--2631},
	number = {11},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	shortjournal = {{IEEE} Trans. Med. Imaging},
	author = {Qaiser, Talha and Rajpoot, Nasir M.},
	urldate = {2020-01-23},
	date = {2019-11},
	langid = {english},
	file = {Qaiser et Rajpoot - 2019 - Learning Where to See A Novel Attention Model for.pdf:/Users/trislaz/Zotero/storage/QBDUF2TE/Qaiser et Rajpoot - 2019 - Learning Where to See A Novel Attention Model for.pdf:application/pdf}
}

@incollection{frangi_predicting_2018,
	location = {Cham},
	title = {Predicting Cancer with a Recurrent Visual Attention Model for Histopathology Images},
	volume = {11071},
	isbn = {978-3-030-00933-5 978-3-030-00934-2},
	url = {http://link.springer.com/10.1007/978-3-030-00934-2_15},
	abstract = {Automatically recognizing cancers from multi-gigapixel whole slide histopathology images is one of the challenges facing machine and deep learning based solutions for digital pathology. Currently, most automatic systems for histopathology are not scalable to large images and hence require a patch-based representation; a sub-optimal solution as it results in important additional computational costs but more importantly in the loss of contextual information. We present a novel attentionbased model for predicting cancer from histopathology whole slide images. The proposed model is capable of attending to the most discriminative regions of an image by adaptively selecting a limited sequence of locations and only processing the selected areas of tissues. We demonstrate the utility of the proposed model on the slide-based prediction of macro and micro metastases in sentinel lymph nodes of breast cancer patients. We achieve competitive results with state-of-the-art convolutional networks while automatically identifying discriminative areas of tissues.},
	pages = {129--137},
	booktitle = {Medical Image Computing and Computer Assisted Intervention – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {{BenTaieb}, Aïcha and Hamarneh, Ghassan},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	urldate = {2020-01-23},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-030-00934-2_15},
	file = {BenTaieb et Hamarneh - 2018 - Predicting Cancer with a Recurrent Visual Attentio.pdf:/Users/trislaz/Zotero/storage/8N3U43UI/BenTaieb et Hamarneh - 2018 - Predicting Cancer with a Recurrent Visual Attentio.pdf:application/pdf}
}

@article{spanhol_dataset_2016,
	title = {A Dataset for Breast Cancer Histopathological Image Classification},
	volume = {63},
	issn = {1558-2531},
	doi = {10.1109/TBME.2015.2496264},
	abstract = {Today, medical image analysis papers require solid experiments to prove the usefulness of proposed methods. However, experiments are often performed on data selected by the researchers, which may come from different institutions, scanners, and populations. Different evaluation measures may be used, making it difficult to compare the methods. In this paper, we introduce a dataset of 7909 breast cancer histopathology images acquired on 82 patients, which is now publicly available from http://web.inf.ufpr.br/vri/breast-cancer-database. The dataset includes both benign and malignant images. The task associated with this dataset is the automated classification of these images in two classes, which would be a valuable computer-aided diagnosis tool for the clinician. In order to assess the difficulty of this task, we show some preliminary results obtained with state-of-the-art image classification systems. The accuracy ranges from 80\% to 85\%, showing room for improvement is left. By providing this dataset and a standardized evaluation protocol to the scientific community, we hope to gather researchers in both the medical and the machine learning field to advance toward this clinical application.},
	pages = {1455--1462},
	number = {7},
	journaltitle = {{IEEE} Transactions on Biomedical Engineering},
	author = {Spanhol, Fabio A. and Oliveira, Luiz S. and Petitjean, Caroline and Heutte, Laurent},
	date = {2016-07},
	file = {IEEE Xplore Abstract Record:/Users/trislaz/Zotero/storage/9C2RPYSC/7312934.html:text/html;IEEE Xplore Full Text PDF:/Users/trislaz/Zotero/storage/498RTVGI/Spanhol et al. - 2016 - A Dataset for Breast Cancer Histopathological Imag.pdf:application/pdf}
}

@article{he_momentum_2019-1,
	title = {Momentum Contrast for Unsupervised Visual Representation Learning},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast ({MoCo}) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. {MoCo} provides competitive results under the common linear protocol on {ImageNet} classification. More importantly, the representations learned by {MoCo} transfer well to downstream tasks. {MoCo} can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on {PASCAL} {VOC}, {COCO}, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	journaltitle = {{arXiv}:1911.05722 [cs]},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	urldate = {2020-01-15},
	date = {2019-11-14},
	eprinttype = {arxiv},
	eprint = {1911.05722},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/2Z2U8WMF/1911.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/BKMJB2DT/He et al. - 2019 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf}
}

@article{misra_self-supervised_2019,
	title = {Self-Supervised Learning of Pretext-Invariant Representations},
	url = {http://arxiv.org/abs/1912.01991},
	abstract = {The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning ({PIRL}, pronounced as "pearl") that learns invariant representations based on pretext tasks. We use {PIRL} with a commonly used pretext task that involves solving jigsaw puzzles. We find that {PIRL} substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, {PIRL} outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.},
	journaltitle = {{arXiv}:1912.01991 [cs]},
	author = {Misra, Ishan and van der Maaten, Laurens},
	urldate = {2020-01-15},
	date = {2019-12-04},
	eprinttype = {arxiv},
	eprint = {1912.01991},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/PPBL332M/1912.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/F6FKY42L/Misra et van der Maaten - 2019 - Self-Supervised Learning of Pretext-Invariant Repr.pdf:application/pdf}
}

@article{cruz-roa_high-throughput_2018,
	title = {High-throughput adaptive sampling for whole-slide histopathology image analysis ({HASHI}) via convolutional neural networks: Application to invasive breast cancer detection},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196828},
	doi = {10.1371/journal.pone.0196828},
	shorttitle = {High-throughput adaptive sampling for whole-slide histopathology image analysis ({HASHI}) via convolutional neural networks},
	abstract = {Precise detection of invasive cancer on whole-slide images ({WSI}) is a critical first step in digital pathology tasks of diagnosis and grading. Convolutional neural network ({CNN}) is the most popular representation learning method for computer vision tasks, which have been successfully applied in digital pathology, including tumor and mitosis detection. However, {CNNs} are typically only tenable with relatively small image sizes (200 × 200 pixels). Only recently, Fully convolutional networks ({FCN}) are able to deal with larger image sizes (500 × 500 pixels) for semantic segmentation. Hence, the direct application of {CNNs} to {WSI} is not computationally feasible because for a {WSI}, a {CNN} would require billions or trillions of parameters. To alleviate this issue, this paper presents a novel method, High-throughput Adaptive Sampling for whole-slide Histopathology Image analysis ({HASHI}), which involves: i) a new efficient adaptive sampling method based on probability gradient and quasi-Monte Carlo sampling, and, ii) a powerful representation learning classifier based on {CNNs}. We applied {HASHI} to automated detection of invasive breast cancer on {WSI}. {HASHI} was trained and validated using three different data cohorts involving near 500 cases and then independently tested on 195 studies from The Cancer Genome Atlas. The results show that (1) the adaptive sampling method is an effective strategy to deal with {WSI} without compromising prediction accuracy by obtaining comparative results of a dense sampling (∼6 million of samples in 24 hours) with far fewer samples (∼2,000 samples in 1 minute), and (2) on an independent test dataset, {HASHI} is effective and robust to data from multiple sites, scanners, and platforms, achieving an average Dice coefficient of 76\%.},
	pages = {e0196828},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Cruz-Roa, Angel and Gilmore, Hannah and Basavanhally, Ajay and Feldman, Michael and Ganesan, Shridar and Shih, Natalie and Tomaszewski, John and Madabhushi, Anant and González, Fabio},
	urldate = {2020-01-14},
	date = {2018-05-24},
	langid = {english},
	file = {Snapshot:/Users/trislaz/Zotero/storage/VVT52VJ4/article.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/8JJ6LXWI/Cruz-Roa et al. - 2018 - High-throughput adaptive sampling for whole-slide .pdf:application/pdf}
}

@article{maron_framework_nodate,
	title = {A Framework for Multiple-Instance Learning},
	abstract = {Multiple-instance learning is a variation on supervised learning, where the task is to learn a concept given positive and negative bags of instances. Each bag may contain many instances, but a bag is labeled positive even if only one of the instances in it falls within the concept. A bag is labeled negative only if all the instances in it are negative. We describe a new general framework, called Diverse Density, for solving multiple-instance learning problems. We apply this framework to learn a simple description of a person from a series of images (bags) containing that person, to a stock selection problem, and to the drug activity prediction problem.},
	pages = {7},
	author = {Maron, Oded and Lozano-Perez, Tomas},
	langid = {english},
	file = {Maron et Lozano-Perez - A Framework for Multiple-Instance Learning.pdf:/Users/trislaz/Zotero/storage/2WCT955X/Maron et Lozano-Perez - A Framework for Multiple-Instance Learning.pdf:application/pdf}
}

@incollection{hutchison_link_2013,
	location = {Berlin, Heidelberg},
	title = {The Link between Multiple-Instance Learning and Learning from Only Positive and Unlabelled Examples},
	volume = {7872},
	isbn = {978-3-642-38066-2 978-3-642-38067-9},
	url = {http://link.springer.com/10.1007/978-3-642-38067-9_14},
	abstract = {This paper establishes a link between two supervised learning frameworks, namely multiple-instance learning ({MIL}) and learning from only positive and unlabelled examples ({LOPU}). {MIL} represents an object as a bag of instances. It is studied under the assumption that its instances are drawn from a mixture distribution of the concept and the non-concept. Based on this assumption, the classiﬁcation of bags can be formulated as a classiﬁer combining problem and the Bayes classiﬁer for instances is shown to be closely related to the classiﬁcation in {LOPU}. This relationship provides a possibility to adopt methods from {LOPU} to {MIL} or vice versa. In particular, we examine a parameter estimator in {LOPU} being applied to {MIL}. Experiments demonstrate the eﬀectiveness of the instance classiﬁer and the parameter estimator.},
	pages = {157--166},
	booktitle = {Multiple Classifier Systems},
	publisher = {Springer Berlin Heidelberg},
	author = {Li, Yan and Tax, David M. J. and Duin, Robert P. W. and Loog, Marco},
	editor = {Zhou, Zhi-Hua and Roli, Fabio and Kittler, Josef},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2020-01-14},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-38067-9_14},
	file = {Li et al. - 2013 - The Link between Multiple-Instance Learning and Le.pdf:/Users/trislaz/Zotero/storage/IT29SBSL/Li et al. - 2013 - The Link between Multiple-Instance Learning and Le.pdf:application/pdf}
}

@article{quellec_multiple-instance_2017,
	title = {Multiple-Instance Learning for Medical Image and Video Analysis},
	volume = {{PP}},
	doi = {10.1109/RBME.2017.2651164},
	abstract = {Multiple-Instance Learning ({MIL}) is a recent machine learning paradigm that is particularly well suited to Medical Image and Video Analysis ({MIVA}) tasks. Based solely on class labels assigned globally to images or videos, {MIL} algorithms learn to detect relevant patterns locally in images or videos. These patterns are then used for classification at a global level. Because supervision relies on global labels, manual segmentations are not needed to train {MIL} algorithms, unlike traditional Single-Instance Learning ({SIL}) algorithms. Consequently, these solutions are attracting increasing interest from the {MIVA} community: since the term was coined by Dietterich et al. in 1997, 73 research papers about {MIL} have been published in the {MIVA} literature. This paper reviews the existing strategies for modeling {MIVA} tasks as {MIL} problems, recommends general-purpose {MIL} algorithms for each type of {MIVA} tasks and discusses {MIVA}-specific {MIL} algorithms. Various experiments performed in medical image and video datasets are compiled in order to back up these discussions. This meta-analysis shows that, besides being more convenient than {SIL} solutions, {MIL} algorithms are also more accurate in many cases. In other words, {MIL} is the ideal solution for many {MIVA} tasks. Recent trends are discussed and future directions are proposed for this emerging paradigm.},
	journaltitle = {{IEEE} Reviews in Biomedical Engineering},
	shortjournal = {{IEEE} Reviews in Biomedical Engineering},
	author = {Quellec, Gwenole and Cazuguel, Guy and Cochener, Béatrice and Lamard, Mathieu},
	date = {2017-01-10},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/CRD986EV/Quellec et al. - 2017 - Multiple-Instance Learning for Medical Image and V.pdf:application/pdf}
}

@report{park_gaussian_2019,
	title = {Gaussian Process Based Heteroscedastic Noise Modeling for Tumor Mutation Burden Prediction from Whole Slide Images},
	url = {http://biorxiv.org/lookup/doi/10.1101/554261},
	abstract = {Tumor mutation burden ({TMB}) is a quantitative measurement of how many mutations present in tumor cells from a patient tumor as assessed by next-generation sequencing ({NGS}) technology. High {TMB} is used as a predictive biomarker to select patients that likely respond to immunotherapy in many cancer types, thus it is critical to accurately measure {TMB} for cancer patients who need to receive the immunotherapy. Recent studies showed that image features from histopathology whole slide images can be used to predict genetic features (e.g., mutation status) or clinical outcome of cancer patients. In this study, we develop a computational method to predict the {TMB} level from cancer patients’ histopathology whole slide images. The prediction problem is formulated as multiple instance learning ({MIL}) because a whole slide image (a bag) has to be divided into multiple image blocks (instances) due to computational reasons but a single label is available only to an entire whole slide image not to each image block. In particular, we propose a novel heteroscedastic noise model for {MIL} based on the framework of Gaussian process ({GP}), where the noise variance is assumed to be a latent function of image level features. This noise variance can encode the confidence in predicting the {TMB} level from each training image and make the method to put different levels of effort to classify images according to how difficult each image can be correctly classified. The method tries to fit an easier image well while it does not put much effort in classifying a harder (ambiguous) image correctly. Expectation and propagation ({EP}) is employed to efficiently infer our model and to find the optimal hyper-parameters. We have demonstrated from synthetic and real-world data sets that our method outperforms on {TMB} prediction from whole slide images base-line methods, including a special case of our method that does not include the heteroscedastic noise modeling and multiple instance ordinal regression ({MIOR}) that is one of few algorithms to solve ordinal regression in the {MIL} setting.},
	institution = {Bioinformatics},
	type = {preprint},
	author = {Park, Sunho and Xu, Hongming and Hwang, Tae Hyun},
	urldate = {2020-01-13},
	date = {2019-02-18},
	langid = {english},
	doi = {10.1101/554261},
	file = {Park et al. - 2019 - Gaussian Process Based Heteroscedastic Noise Model.pdf:/Users/trislaz/Zotero/storage/GJL32UB4/Park et al. - 2019 - Gaussian Process Based Heteroscedastic Noise Model.pdf:application/pdf}
}

@article{saltz_spatial_2018,
	title = {Spatial Organization and Molecular Correlation of Tumor-Infiltrating Lymphocytes Using Deep Learning on Pathology Images},
	volume = {23},
	issn = {22111247},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2211124718304479},
	doi = {10.1016/j.celrep.2018.03.086},
	abstract = {Beyond sample curation and basic pathologic characterization, the digitized H\&E-stained images of {TCGA} samples remain underutilized. To highlight this resource, we present mappings of tumor-inﬁltrating lymphocytes ({TILs}) based on H\&E images from 13 {TCGA} tumor types. These {TIL} maps are derived through computational staining using a convolutional neural network trained to classify patches of images. Afﬁnity propagation revealed local spatial structure in {TIL} patterns and correlation with overall survival. {TIL} map structural patterns were grouped using standard histopathological parameters. These patterns are enriched in particular T cell subpopulations derived from molecular measures. {TIL} densities and spatial structure were differentially enriched among tumor types, immune subtypes, and tumor molecular subtypes, implying that spatial inﬁltrate state could reﬂect particular tumor cell aberration states. Obtaining spatial lymphocytic patterns linked to the rich genomic characterization of {TCGA} samples demonstrates one use for the {TCGA} image archives with insights into the tumor-immune microenvironment.},
	pages = {181--193.e7},
	number = {1},
	journaltitle = {Cell Reports},
	shortjournal = {Cell Reports},
	author = {Saltz, Joel and Gupta, Rajarsi and Hou, Le and Kurc, Tahsin and Singh, Pankaj and Nguyen, Vu and Samaras, Dimitris and Shroyer, Kenneth R. and Zhao, Tianhao and Batiste, Rebecca and Van Arnam, John and Shmulevich, Ilya and Rao, Arvind U.K. and Lazar, Alexander J. and Sharma, Ashish and Thorsson, Vésteinn and Caesar-Johnson, Samantha J. and Demchok, John A. and Felau, Ina and Kasapi, Melpomeni and Ferguson, Martin L. and Hutter, Carolyn M. and Sofia, Heidi J. and Tarnuzzer, Roy and Wang, Zhining and Yang, Liming and Zenklusen, Jean C. and Zhang, Jiashan (Julia) and Chudamani, Sudha and Liu, Jia and Lolla, Laxmi and Naresh, Rashi and Pihl, Todd and Sun, Qiang and Wan, Yunhu and Wu, Ye and Cho, Juok and {DeFreitas}, Timothy and Frazer, Scott and Gehlenborg, Nils and Getz, Gad and Heiman, David I. and Kim, Jaegil and Lawrence, Michael S. and Lin, Pei and Meier, Sam and Noble, Michael S. and Saksena, Gordon and Voet, Doug and Zhang, Hailei and Bernard, Brady and Chambwe, Nyasha and Dhankani, Varsha and Knijnenburg, Theo and Kramer, Roger and Leinonen, Kalle and Liu, Yuexin and Miller, Michael and Reynolds, Sheila and Shmulevich, Ilya and Thorsson, Vesteinn and Zhang, Wei and Akbani, Rehan and Broom, Bradley M. and Hegde, Apurva M. and Ju, Zhenlin and Kanchi, Rupa S. and Korkut, Anil and Li, Jun and Liang, Han and Ling, Shiyun and Liu, Wenbin and Lu, Yiling and Mills, Gordon B. and Ng, Kwok-Shing and Rao, Arvind and Ryan, Michael and Wang, Jing and Weinstein, John N. and Zhang, Jiexin and Abeshouse, Adam and Armenia, Joshua and Chakravarty, Debyani and Chatila, Walid K. and de Bruijn, Ino and Gao, Jianjiong and Gross, Benjamin E. and Heins, Zachary J. and Kundra, Ritika and La, Konnor and Ladanyi, Marc and Luna, Augustin and Nissan, Moriah G. and Ochoa, Angelica and Phillips, Sarah M. and Reznik, Ed and Sanchez-Vega, Francisco and Sander, Chris and Schultz, Nikolaus and Sheridan, Robert and Sumer, S. Onur and Sun, Yichao and Taylor, Barry S. and Wang, Jioajiao and Zhang, Hongxin and Anur, Pavana and Peto, Myron and Spellman, Paul and Benz, Christopher and Stuart, Joshua M. and Wong, Christopher K. and Yau, Christina and Hayes, D. Neil and Parker, Joel S. and Wilkerson, Matthew D. and Ally, Adrian and Balasundaram, Miruna and Bowlby, Reanne and Brooks, Denise and Carlsen, Rebecca and Chuah, Eric and Dhalla, Noreen and Holt, Robert and Jones, Steven J.M. and Kasaian, Katayoon and Lee, Darlene and Ma, Yussanne and Marra, Marco A. and Mayo, Michael and Moore, Richard A. and Mungall, Andrew J. and Mungall, Karen and Robertson, A. Gordon and Sadeghi, Sara and Schein, Jacqueline E. and Sipahimalani, Payal and Tam, Angela and Thiessen, Nina and Tse, Kane and Wong, Tina and Berger, Ashton C. and Beroukhim, Rameen and Cherniack, Andrew D. and Cibulskis, Carrie and Gabriel, Stacey B. and Gao, Galen F. and Ha, Gavin and Meyerson, Matthew and Schumacher, Steven E. and Shih, Juliann and Kucherlapati, Melanie H. and Kucherlapati, Raju S. and Baylin, Stephen and Cope, Leslie and Danilova, Ludmila and Bootwalla, Moiz S. and Lai, Phillip H. and Maglinte, Dennis T. and Van Den Berg, David J. and Weisenberger, Daniel J. and Auman, J. Todd and Balu, Saianand and Bodenheimer, Tom and Fan, Cheng and Hoadley, Katherine A. and Hoyle, Alan P. and Jefferys, Stuart R. and Jones, Corbin D. and Meng, Shaowu and Mieczkowski, Piotr A. and Mose, Lisle E. and Perou, Amy H. and Perou, Charles M. and Roach, Jeffrey and Shi, Yan and Simons, Janae V. and Skelly, Tara and Soloway, Matthew G. and Tan, Donghui and Veluvolu, Umadevi and Fan, Huihui and Hinoue, Toshinori and Laird, Peter W. and Shen, Hui and Zhou, Wanding and Bellair, Michelle and Chang, Kyle and Covington, Kyle and Creighton, Chad J. and Dinh, Huyen and Doddapaneni, {HarshaVardhan} and Donehower, Lawrence A. and Drummond, Jennifer and Gibbs, Richard A. and Glenn, Robert and Hale, Walker and Han, Yi and Hu, Jianhong and Korchina, Viktoriya and Lee, Sandra and Lewis, Lora and Li, Wei and Liu, Xiuping and Morgan, Margaret and Morton, Donna and Muzny, Donna and Santibanez, Jireh and Sheth, Margi and Shinbrot, Eve and Wang, Linghua and Wang, Min and Wheeler, David A. and Xi, Liu and Zhao, Fengmei and Hess, Julian and Appelbaum, Elizabeth L. and Bailey, Matthew and Cordes, Matthew G. and Ding, Li and Fronick, Catrina C. and Fulton, Lucinda A. and Fulton, Robert S. and Kandoth, Cyriac and Mardis, Elaine R. and {McLellan}, Michael D. and Miller, Christopher A. and Schmidt, Heather K. and Wilson, Richard K. and Crain, Daniel and Curley, Erin and Gardner, Johanna and Lau, Kevin and Mallery, David and Morris, Scott and Paulauskis, Joseph and Penny, Robert and Shelton, Candace and Shelton, Troy and Sherman, Mark and Thompson, Eric and Yena, Peggy and Bowen, Jay and Gastier-Foster, Julie M. and Gerken, Mark and Leraas, Kristen M. and Lichtenberg, Tara M. and Ramirez, Nilsa C. and Wise, Lisa and Zmuda, Erik and Corcoran, Niall and Costello, Tony and Hovens, Christopher and Carvalho, Andre L. and de Carvalho, Ana C. and Fregnani, José H. and Longatto-Filho, Adhemar and Reis, Rui M. and Scapulatempo-Neto, Cristovam and Silveira, Henrique C.S. and Vidal, Daniel O. and Burnette, Andrew and Eschbacher, Jennifer and Hermes, Beth and Noss, Ardene and Singh, Rosy and Anderson, Matthew L. and Castro, Patricia D. and Ittmann, Michael and Huntsman, David and Kohl, Bernard and Le, Xuan and Thorp, Richard and Andry, Chris and Duffy, Elizabeth R. and Lyadov, Vladimir and Paklina, Oxana and Setdikova, Galiya and Shabunin, Alexey and Tavobilov, Mikhail and {McPherson}, Christopher and Warnick, Ronald and Berkowitz, Ross and Cramer, Daniel and Feltmate, Colleen and Horowitz, Neil and Kibel, Adam and Muto, Michael and Raut, Chandrajit P. and Malykh, Andrei and Barnholtz-Sloan, Jill S. and Barrett, Wendi and Devine, Karen and Fulop, Jordonna and Ostrom, Quinn T. and Shimmel, Kristen and Wolinsky, Yingli and Sloan, Andrew E. and De Rose, Agostino and Giuliante, Felice and Goodman, Marc and Karlan, Beth Y. and Hagedorn, Curt H. and Eckman, John and Harr, Jodi and Myers, Jerome and Tucker, Kelinda and Zach, Leigh Anne and Deyarmin, Brenda and Hu, Hai and Kvecher, Leonid and Larson, Caroline and Mural, Richard J. and Somiari, Stella and Vicha, Ales and Zelinka, Tomas and Bennett, Joseph and Iacocca, Mary and Rabeno, Brenda and Swanson, Patricia and Latour, Mathieu and Lacombe, Louis and Têtu, Bernard and Bergeron, Alain and {McGraw}, Mary and Staugaitis, Susan M. and Chabot, John and Hibshoosh, Hanina and Sepulveda, Antonia and Su, Tao and Wang, Timothy and Potapova, Olga and Voronina, Olga and Desjardins, Laurence and Mariani, Odette and Roman-Roman, Sergio and Sastre, Xavier and Stern, Marc-Henri and Cheng, Feixiong and Signoretti, Sabina and Berchuck, Andrew and Bigner, Darell and Lipp, Eric and Marks, Jeffrey and {McCall}, Shannon and {McLendon}, Roger and Secord, Angeles and Sharp, Alexis and Behera, Madhusmita and Brat, Daniel J. and Chen, Amy and Delman, Keith and Force, Seth and Khuri, Fadlo and Magliocca, Kelly and Maithel, Shishir and Olson, Jeffrey J. and Owonikoko, Taofeek and Pickens, Alan and Ramalingam, Suresh and Shin, Dong M. and Sica, Gabriel and Van Meir, Erwin G. and Zhang, Hongzheng and Eijckenboom, Wil and Gillis, Ad and Korpershoek, Esther and Looijenga, Leendert and Oosterhuis, Wolter and Stoop, Hans and van Kessel, Kim E. and Zwarthoff, Ellen C. and Calatozzolo, Chiara and Cuppini, Lucia and Cuzzubbo, Stefania and {DiMeco}, Francesco and Finocchiaro, Gaetano and Mattei, Luca and Perin, Alessandro and Pollo, Bianca and Chen, Chu and Houck, John and Lohavanichbutr, Pawadee and Hartmann, Arndt and Stoehr, Christine and Stoehr, Robert and Taubert, Helge and Wach, Sven and Wullich, Bernd and Kycler, Witold and Murawa, Dawid and Wiznerowicz, Maciej and Chung, Ki and Edenfield, W. Jeffrey and Martin, Julie and Baudin, Eric and Bubley, Glenn and Bueno, Raphael and De Rienzo, Assunta and Richards, William G. and Kalkanis, Steven and Mikkelsen, Tom and Noushmehr, Houtan and Scarpace, Lisa and Girard, Nicolas and Aymerich, Marta and Campo, Elias and Giné, Eva and Guillermo, Armando López and Van Bang, Nguyen and Hanh, Phan Thi and Phu, Bui Duc and Tang, Yufang and Colman, Howard and Evason, Kimberley and Dottino, Peter R. and Martignetti, John A. and Gabra, Hani and Juhl, Hartmut and Akeredolu, Teniola and Stepa, Serghei and Hoon, Dave and Ahn, Keunsoo and Kang, Koo Jeong and Beuschlein, Felix and Breggia, Anne and Birrer, Michael and Bell, Debra and Borad, Mitesh and Bryce, Alan H. and Castle, Erik and Chandan, Vishal and Cheville, John and Copland, John A. and Farnell, Michael and Flotte, Thomas and Giama, Nasra and Ho, Thai and Kendrick, Michael and Kocher, Jean-Pierre and Kopp, Karla and Moser, Catherine and Nagorney, David and O’Brien, Daniel and O’Neill, Brian Patrick and Patel, Tushar and Petersen, Gloria and Que, Florencia and Rivera, Michael and Roberts, Lewis and Smallridge, Robert and Smyrk, Thomas and Stanton, Melissa and Thompson, R. Houston and Torbenson, Michael and Yang, Ju Dong and Zhang, Lizhi and Brimo, Fadi and Ajani, Jaffer A. and Gonzalez, Ana Maria Angulo and Behrens, Carmen and Bondaruk, Jolanta and Broaddus, Russell and Czerniak, Bogdan and Esmaeli, Bita and Fujimoto, Junya and Gershenwald, Jeffrey and Guo, Charles and Lazar, Alexander J. and Logothetis, Christopher and Meric-Bernstam, Funda and Moran, Cesar and Ramondetta, Lois and Rice, David and Sood, Anil and Tamboli, Pheroze and Thompson, Timothy and Troncoso, Patricia and Tsao, Anne and Wistuba, Ignacio and Carter, Candace and Haydu, Lauren and Hersey, Peter and Jakrot, Valerie and Kakavand, Hojabr and Kefford, Richard and Lee, Kenneth and Long, Georgina and Mann, Graham and Quinn, Michael and Saw, Robyn and Scolyer, Richard and Shannon, Kerwin and Spillane, Andrew and Stretch, onathan and Synott, Maria and Thompson, John and Wilmott, James and Al-Ahmadie, Hikmat and Chan, Timothy A. and Ghossein, Ronald and Gopalan, Anuradha and Levine, Douglas A. and Reuter, Victor and Singer, Samuel and Singh, Bhuvanesh and Tien, Nguyen Viet and Broudy, Thomas and Mirsaidi, Cyrus and Nair, Praveen and Drwiega, Paul and Miller, Judy and Smith, Jennifer and Zaren, Howard and Park, Joong-Won and Hung, Nguyen Phi and Kebebew, Electron and Linehan, W. Marston and Metwalli, Adam R. and Pacak, Karel and Pinto, Peter A. and Schiffman, Mark and Schmidt, Laura S. and Vocke, Cathy D. and Wentzensen, Nicolas and Worrell, Robert and Yang, Hannah and Moncrieff, Marc and Goparaju, Chandra and Melamed, Jonathan and Pass, Harvey and Botnariuc, Natalia and Caraman, Irina and Cernat, Mircea and Chemencedji, Inga and Clipca, Adrian and Doruc, Serghei and Gorincioi, Ghenadie and Mura, Sergiu and Pirtac, Maria and Stancul, Irina and Tcaciuc, Diana and Albert, Monique and Alexopoulou, Iakovina and Arnaout, Angel and Bartlett, John and Engel, Jay and Gilbert, Sebastien and Parfitt, Jeremy and Sekhon, Harman and Thomas, George and Rassl, Doris M. and Rintoul, Robert C. and Bifulco, Carlo and Tamakawa, Raina and Urba, Walter and Hayward, Nicholas and Timmers, Henri and Antenucci, Anna and Facciolo, Francesco and Grazi, Gianluca and Marino, Mirella and Merola, Roberta and de Krijger, Ronald and Gimenez-Roqueplo, Anne-Paule and Piché, Alain and Chevalier, Simone and {McKercher}, Ginette and Birsoy, Kivanc and Barnett, Gene and Brewer, Cathy and Farver, Carol and Naska, Theresa and Pennell, Nathan A. and Raymond, Daniel and Schilero, Cathy and Smolenski, Kathy and Williams, Felicia and Morrison, Carl and Borgia, Jeffrey A. and Liptay, Michael J. and Pool, Mark and Seder, Christopher W. and Junker, Kerstin and Omberg, Larsson and Dinkin, Mikhail and Manikhas, George and Alvaro, Domenico and Bragazzi, Maria Consiglia and Cardinale, Vincenzo and Carpino, Guido and Gaudio, Eugenio and Chesla, David and Cottingham, Sandra and Dubina, Michael and Moiseenko, Fedor and Dhanasekaran, Renumathy and Becker, Karl-Friedrich and Janssen, Klaus-Peter and Slotta-Huspenina, Julia and Abdel-Rahman, Mohamed H. and Aziz, Dina and Bell, Sue and Cebulla, Colleen M. and Davis, Amy and Duell, Rebecca and Elder, J. Bradley and Hilty, Joe and Kumar, Bahavna and Lang, James and Lehman, Norman L. and Mandt, Randy and Nguyen, Phuong and Pilarski, Robert and Rai, Karan and Schoenfield, Lynn and Senecal, Kelly and Wakely, Paul and Hansen, Paul and Lechan, Ronald and Powers, James and Tischler, Arthur and Grizzle, William E. and Sexton, Katherine C. and Kastl, Alison and Henderson, Joel and Porten, Sima and Waldmann, Jens and Fassnacht, Martin and Asa, Sylvia L. and Schadendorf, Dirk and Couce, Marta and Graefen, Markus and Huland, Hartwig and Sauter, Guido and Schlomm, Thorsten and Simon, Ronald and Tennstedt, Pierre and Olabode, Oluwole and Nelson, Mark and Bathe, Oliver and Carroll, Peter R. and Chan, June M. and Disaia, Philip and Glenn, Pat and Kelley, Robin K. and Landen, Charles N. and Phillips, Joanna and Prados, Michael and Simko, Jeffry and Smith-{McCune}, Karen and {VandenBerg}, Scott and Roggin, Kevin and Fehrenbach, Ashley and Kendler, Ady and Sifri, Suzanne and Steele, Ruth and Jimeno, Antonio and Carey, Francis and Forgie, Ian and Mannelli, Massimo and Carney, Michael and Hernandez, Brenda and Campos, Benito and Herold-Mende, Christel and Jungk, Christin and Unterberg, Andreas and von Deimling, Andreas and Bossler, Aaron and Galbraith, Joseph and Jacobus, Laura and Knudson, Michael and Knutson, Tina and Ma, Deqin and Milhem, Mohammed and Sigmund, Rita and Godwin, Andrew K. and Madan, Rashna and Rosenthal, Howard G. and Adebamowo, Clement and Adebamowo, Sally N. and Boussioutas, Alex and Beer, David and Giordano, Thomas and Mes-Masson, Anne-Marie and Saad, Fred and Bocklage, Therese and Landrum, Lisa and Mannel, Robert and Moore, Kathleen and Moxley, Katherine and Postier, Russel and Walker, Joan and Zuna, Rosemary and Feldman, Michael and Valdivieso, Federico and Dhir, Rajiv and Luketich, James and Pinero, Edna M. Mora and Quintero-Aguilo, Mario and Carlotti, Carlos Gilberto and Dos Santos, Jose Sebastião and Kemp, Rafael and Sankarankuty, Ajith and Tirapelli, Daniela and Catto, James and Agnew, Kathy and Swisher, Elizabeth and Creaney, Jenette and Robinson, Bruce and Shelley, Carl Simon and Godwin, Eryn M. and Kendall, Sara and Shipman, Cassaundra and Bradford, Carol and Carey, Thomas and Haddad, Andrea and Moyer, Jeffey and Peterson, Lisa and Prince, Mark and Rozek, Laura and Wolf, Gregory and Bowman, Rayleen and Fong, Kwun M. and Yang, Ian and Korst, Robert and Rathmell, W. Kimryn and Fantacone-Campbell, J. Leigh and Hooke, Jeffrey A. and Kovatich, Albert J. and Shriver, Craig D. and {DiPersio}, John and Drake, Bettina and Govindan, Ramaswamy and Heath, Sharon and Ley, Timothy and Van Tine, Brian and Westervelt, Peter and Rubin, Mark A. and Lee, Jung Il and Aredes, Natália D. and Mariamidze, Armaz},
	urldate = {2020-01-12},
	date = {2018-04},
	langid = {english},
	file = {Saltz et al. - 2018 - Spatial Organization and Molecular Correlation of .pdf:/Users/trislaz/Zotero/storage/QGJ4SF89/Saltz et al. - 2018 - Spatial Organization and Molecular Correlation of .pdf:application/pdf}
}

@article{smith_dont_2018,
	title = {{DON}’T {DECAY} {THE} {LEARNING} {RATE}, {INCREASE} {THE} {BATCH} {SIZE}},
	abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent ({SGD}), {SGD} with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate and scaling the batch size B ∝ . Finally, one can increase the momentum coefﬁcient m and scale B ∝ 1/(1 − m), although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train {ResNet}-50 on {ImageNet} to 76.1\% validation accuracy in under 30 minutes.},
	pages = {11},
	author = {Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
	date = {2018},
	langid = {english},
	file = {Smith et al. - 2018 - DON’T DECAY THE LEARNING RATE, INCREASE THE BATCH .pdf:/Users/trislaz/Zotero/storage/RRLIAHMB/Smith et al. - 2018 - DON’T DECAY THE LEARNING RATE, INCREASE THE BATCH .pdf:application/pdf}
}

@online{noauthor_gdc_nodate,
	title = {{GDC}},
	url = {https://portal.gdc.cancer.gov/},
	urldate = {2019-12-15},
	file = {GDC:/Users/trislaz/Zotero/storage/54ZHVGEF/portal.gdc.cancer.gov.html:text/html}
}

@article{kirillov_panoptic_2019,
	title = {Panoptic Segmentation},
	url = {http://arxiv.org/abs/1801.00868},
	abstract = {We propose and study a task we name panoptic segmentation ({PS}). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality ({PQ}) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for {PS} on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.},
	journaltitle = {{arXiv}:1801.00868 [cs]},
	author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
	urldate = {2019-12-13},
	date = {2019-04-10},
	eprinttype = {arxiv},
	eprint = {1801.00868},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/XE7ELINT/1801.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/AFHI3AXN/Kirillov et al. - 2019 - Panoptic Segmentation.pdf:application/pdf}
}

@online{noauthor_ieee_nodate,
	title = {{IEEE} Xplore Full-Text {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8880654},
	urldate = {2019-12-12},
	file = {IEEE Xplore Full-Text PDF\::/Users/trislaz/Zotero/storage/W49CUYR2/stamp.html:text/html}
}

@article{miyato_spectral_2018,
	title = {Spectral Normalization for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1802.05957},
	abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on {CIFAR}10, {STL}-10, and {ILSVRC}2012 dataset, and we experimentally confirmed that spectrally normalized {GANs} ({SN}-{GANs}) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
	journaltitle = {{arXiv}:1802.05957 [cs, stat]},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	urldate = {2019-12-12},
	date = {2018-02-16},
	eprinttype = {arxiv},
	eprint = {1802.05957},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/FP3XVPQA/1802.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/YD7KXPA5/Miyato et al. - 2018 - Spectral Normalization for Generative Adversarial .pdf:application/pdf}
}

@article{miyato_spectral_2018-1,
	title = {Spectral Normalization for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1802.05957},
	abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on {CIFAR}10, {STL}-10, and {ILSVRC}2012 dataset, and we experimentally confirmed that spectrally normalized {GANs} ({SN}-{GANs}) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
	journaltitle = {{arXiv}:1802.05957 [cs, stat]},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	urldate = {2019-12-12},
	date = {2018-02-16},
	eprinttype = {arxiv},
	eprint = {1802.05957},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/TZC4AZ2E/1802.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/CXRZA4U3/Miyato et al. - 2018 - Spectral Normalization for Generative Adversarial .pdf:application/pdf}
}

@article{mahmood_deep_2019,
	title = {Deep Adversarial Training for Multi-Organ Nuclei Segmentation in Histopathology Images},
	issn = {1558-254X},
	doi = {10.1109/TMI.2019.2927182},
	abstract = {Nuclei segmentation is a fundamental task for various computational pathology applications, including nuclei morphology analysis, cell type classification, and cancer grading. Deep learning has emerged as a powerful approach to segmenting nuclei, but the accuracy of convolutional neural networks ({CNNs}) depends on the volume and quality of labeled histopathology data for training. In particular, conventional {CNN}-based approaches lack structured prediction capabilities, which are required to distinguish overlapping and clumped nuclei. Here, we present an approach to nuclei segmentation that overcomes these challenges by utilizing a conditional generative adversarial network ({cGAN}) trained with synthetic and real data. We generate a large dataset of H\&E training images with perfect nuclei segmentation labels using an unpaired {GAN} framework. This synthetic data along with real histopathology data from six different organs are used to train a conditional {GAN} with spectral normalization and gradient penalty for nuclei segmentation. This adversarial regression framework enforces higher-order spacial-consistency when compared to conventional {CNN} models. We demonstrate that this nuclei segmentation approach generalizes across different organs, sites, patients and disease states, and outperforms conventional approaches, especially in isolating individual and overlapping nuclei.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	author = {Mahmood, Faisal and Borders, Daniel and Chen, Richard and {McKay}, Gregory N. and Salimian, Kevan J. and Baras, Alexander and Durr, Nicholas J.},
	date = {2019},
	file = {IEEE Xplore Abstract Record:/Users/trislaz/Zotero/storage/HV3V3UVU/8756037.html:text/html;IEEE Xplore Full Text PDF:/Users/trislaz/Zotero/storage/5WJS8FTA/Mahmood et al. - 2019 - Deep Adversarial Training for Multi-Organ Nuclei S.pdf:application/pdf}
}

@article{kumar_dataset_2017,
	title = {A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology},
	volume = {36},
	doi = {10.1109/TMI.2017.2677499},
	abstract = {Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques such as Otsu thresholding and watershed segmentation do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms require datasets of images in which a vast number of nuclei have been annotated. Publicly accessible and annotated datasets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible dataset of H\&E stained tissue images with more than 21,000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our dataset is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H\&E stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	shortjournal = {{IEEE} Transactions on Medical Imaging},
	author = {Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
	date = {2017-03-06},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/JVUCVEWD/Kumar et al. - 2017 - A Dataset and a Technique for Generalized Nuclear .pdf:application/pdf}
}

@article{sharma_non-genetic_2019,
	title = {Non-Genetic Intra-Tumor Heterogeneity Is a Major Predictor of Phenotypic Heterogeneity and Ongoing Evolutionary Dynamics in Lung Tumors},
	volume = {29},
	issn = {2211-1247},
	url = {https://www.cell.com/cell-reports/abstract/S2211-1247(19)31359-2},
	doi = {10.1016/j.celrep.2019.10.045},
	pages = {2164--2174.e5},
	number = {8},
	journaltitle = {Cell Reports},
	shortjournal = {Cell Reports},
	author = {Sharma, Anchal and Merritt, Elise and Hu, Xiaoju and Cruz, Angelique and Jiang, Chuan and Sarkodie, Halle and Zhou, Zhan and Malhotra, Jyoti and Riedlinger, Gregory M. and De, Subhajyoti},
	urldate = {2019-12-03},
	date = {2019-11-19},
	pmid = {31747591},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/38UUMQPI/Sharma et al. - 2019 - Non-Genetic Intra-Tumor Heterogeneity Is a Major P.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/3BC85ZV9/S2211-1247(19)31359-2.html:text/html}
}

@article{hagele_resolving_2019,
	title = {Resolving challenges in deep learning-based analyses of histopathological images using explanation methods},
	url = {http://arxiv.org/abs/1908.06943},
	abstract = {Deep learning has recently gained popularity in digital pathology due to its high prediction quality. However, the medical domain requires explanation and insight for a better understanding beyond standard quantitative performance evaluation. Recently, explanation methods have emerged, which are so far still rarely used in medicine. This work shows their application to generate heatmaps that allow to resolve common challenges encountered in deep learning-based digital histopathology analyses. These challenges comprise biases typically inherent to histopathology data. We study binary classification tasks of tumor tissue discrimination in publicly available haematoxylin and eosin slides of various tumor entities and investigate three types of biases: (1) biases which affect the entire dataset, (2) biases which are by chance correlated with class labels and (3) sampling biases. While standard analyses focus on patch-level evaluation, we advocate pixel-wise heatmaps, which offer a more precise and versatile diagnostic instrument and furthermore help to reveal biases in the data. This insight is shown to not only detect but also to be helpful to remove the effects of common hidden biases, which improves generalization within and across datasets. For example, we could see a trend of improved area under the receiver operating characteristic curve by 5\% when reducing a labeling bias. Explanation techniques are thus demonstrated to be a helpful and highly relevant tool for the development and the deployment phases within the life cycle of real-world applications in digital pathology.},
	journaltitle = {{arXiv}:1908.06943 [cs, eess, q-bio]},
	author = {Hägele, Miriam and Seegerer, Philipp and Lapuschkin, Sebastian and Bockmayr, Michael and Samek, Wojciech and Klauschen, Frederick and Müller, Klaus-Robert and Binder, Alexander},
	urldate = {2019-12-03},
	date = {2019-08-15},
	eprinttype = {arxiv},
	eprint = {1908.06943},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/KPDEQUPJ/1908.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/AQHB6NXR/Hägele et al. - 2019 - Resolving challenges in deep learning-based analys.pdf:application/pdf}
}

@article{binder_towards_2018,
	title = {Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles},
	url = {http://arxiv.org/abs/1805.11178},
	shorttitle = {Towards computational fluorescence microscopy},
	abstract = {Recent advances in cancer research largely rely on new developments in microscopic or molecular profiling techniques offering high level of detail with respect to either spatial or molecular features, but usually not both. Here, we present a novel machine learning-based computational approach that allows for the identification of morphological tissue features and the prediction of molecular properties from breast cancer imaging data. This integration of microanatomic information of tumors with complex molecular profiling data, including protein or gene expression, copy number variation, gene methylation and somatic mutations, provides a novel means to computationally score molecular markers with respect to their relevance to cancer and their spatial associations within the tumor microenvironment.},
	journaltitle = {{arXiv}:1805.11178 [cs]},
	author = {Binder, Alexander and Bockmayr, Michael and Hägele, Miriam and Wienert, Stephan and Heim, Daniel and Hellweg, Katharina and Stenzinger, Albrecht and Parlow, Laura and Budczies, Jan and Goeppert, Benjamin and Treue, Denise and Kotani, Manato and Ishii, Masaru and Dietel, Manfred and Hocke, Andreas and Denkert, Carsten and Müller, Klaus-Robert and Klauschen, Frederick},
	urldate = {2019-12-03},
	date = {2018-05-28},
	eprinttype = {arxiv},
	eprint = {1805.11178},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/B5SF3VQI/1805.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/GG4ANHCS/Binder et al. - 2018 - Towards computational fluorescence microscopy Mac.pdf:application/pdf}
}

@inproceedings{koyun_adversarial_2019,
	title = {Adversarial Nuclei Segmentation on H E Stained Histopathology Images},
	doi = {10.1109/INISTA.2019.8778369},
	abstract = {Computer aided methods in pathology are advancing rapidly. Problems like segmentation, classification and detection of pathology images are solved with machine learning and image processing techniques. State-of-the-art methods in nuclei segmentation problem include supervised deep learning techniques. However, labeling process of pathology images is an expensive and time consuming process. In this work, nuclei segmentation problem is formulated as image-to-image translation problem and using Cycle-Consistent Generative Adversarial Networks, an unsupervised segmentation scheme is proposed for hematoxylin \& eosin stained histopathology data.},
	eventtitle = {2019 {IEEE} International Symposium on {INnovations} in Intelligent {SysTems} and Applications ({INISTA})},
	pages = {1--5},
	booktitle = {2019 {IEEE} International Symposium on {INnovations} in Intelligent {SysTems} and Applications ({INISTA})},
	author = {Koyun, Onur Can and Yildirim, Tülay},
	date = {2019-07},
	note = {{ISSN}: null},
	file = {IEEE Xplore Abstract Record:/Users/trislaz/Zotero/storage/5SLBHPF6/8778369.html:text/html;IEEE Xplore Full Text PDF:/Users/trislaz/Zotero/storage/NJJSLH7D/Koyun et Yildirim - 2019 - Adversarial Nuclei Segmentation on H E Stained His.pdf:application/pdf}
}

@article{de_bel_stain-transforming_nodate,
	title = {Stain-Transforming Cycle-Consistent Generative Adversarial Networks for Improved Segmentation of Renal Histopathology},
	abstract = {The performance of deep learning applications in digital histopathology can deteriorate signiﬁcantly due to staining variations across centers. We employ cycle-consistent generative adversarial networks ({cycleGANs}) for unpaired image-to-image translation, facilitating between-center stain transformation. We ﬁnd that modiﬁcations to the original {cycleGAN} architecture make it more suitable for stain transformation, creating artiﬁcially stained images of high quality. Speciﬁcally, changing the generator model to a smaller U-net-like architecture, adding an identity loss term, increasing the batch size and the learning all led to improved training stability and performance. Furthermore, we propose a method for dealing with tiling artifacts when applying the network on whole slide images ({WSIs}). We apply our stain transformation method on two datasets of {PASstained} (Periodic Acid-Schiff) renal tissue sections from different centers. We show that stain transformation is beneﬁcial to the performance of cross-center segmentation, raising the Dice coefﬁcient from 0.36 to 0.85 and from 0.45 to 0.73 on the two datasets.},
	pages = {13},
	author = {de Bel, Thomas and Hermsen, Meyke and Kers, Jesper},
	langid = {english},
	file = {de Bel et al. - Stain-Transforming Cycle-Consistent Generative Adv.pdf:/Users/trislaz/Zotero/storage/AQ9RVK2F/de Bel et al. - Stain-Transforming Cycle-Consistent Generative Adv.pdf:application/pdf}
}

@article{rodrigues_ciblage_nodate,
	title = {Ciblage des défauts de la recombinaison homologue par les inhibiteurs de {PARP}},
	abstract = {Homologous recombination deficiencies are known to be a common biological characteristic in many cancers. Most of these tumors are sensitive to {PARP} inhibitors. This new therapeutic class has now entered into our daily practice with olaparib in Europe as well as rucaparib and niraparib in the United States. New therapeutic combinations are now being developed.},
	pages = {6},
	journaltitle = {{MISE} {AU} {POINT}},
	author = {Rodrigues, M and Manié, É and Cottu, P and Diéras, V and Bataillon, G and Popova, T and Stern, M H},
	langid = {french},
	file = {Ciblage des défauts de la recombinaison homologue par les inhibiteurs de PARP.pdf:/home/lazard/Documents/cbio/docs_curie/Ciblage des défauts de la recombinaison homologue par les inhibiteurs de PARP.pdf:application/pdf}
}

@article{rodrigues_brcaness/defauts_2016,
	title = {{BRCAness}/défauts de la recombinaison homologue dans les cancers : mécanismes, diagnostic et conséquences thérapeutiques},
	pages = {6},
	journaltitle = {Mis e au point},
	author = {Rodrigues, M and Manié, É and Popova, T and Stern, M H},
	date = {2016},
	langid = {french},
	file = {BRCAness-défauts de la recombinaison homologue dans les cancers- mécanismes, diagnostic et conséquences thérapeutiques.pdf:/home/lazard/Documents/cbio/docs_curie/BRCAness-défauts de la recombinaison homologue dans les cancers- mécanismes, diagnostic et conséquences thérapeutiques.pdf:application/pdf}
}

@article{lafarge_learning_2019,
	title = {Learning Domain-Invariant Representations of Histological Images},
	volume = {6},
	issn = {2296-858X},
	doi = {10.3389/fmed.2019.00162},
	abstract = {Histological images present high appearance variability due to inconsistent latent parameters related to the preparation and scanning procedure of histological slides, as well as the inherent biological variability of tissues. Machine-learning models are trained with images from a limited set of domains, and are expected to generalize to images from unseen domains. Methodological design choices have to be made in order to yield domain invariance and proper generalization. In digital pathology, standard approaches focus either on ad-hoc normalization of the latent parameters based on prior knowledge, such as staining normalization, or aim at anticipating new variations of these parameters via data augmentation. Since every histological image originates from a unique data distribution, we propose to consider every histological slide of the training data as a domain and investigated the alternative approach of domain-adversarial training to learn features that are invariant to this available domain information. We carried out a comparative analysis with staining normalization and data augmentation on two different tasks: generalization to images acquired in unseen pathology labs for mitosis detection and generalization to unseen organs for nuclei segmentation. We report that the utility of each method depends on the type of task and type of data variability present at training and test time. The proposed framework for domain-adversarial training is able to improve generalization performances on top of conventional methods.},
	pages = {162},
	journaltitle = {Frontiers in Medicine},
	shortjournal = {Front Med (Lausanne)},
	author = {Lafarge, Maxime W. and Pluim, Josien P. W. and Eppenhof, Koen A. J. and Veta, Mitko},
	date = {2019},
	pmid = {31380377},
	pmcid = {PMC6646468},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/42EECN88/Lafarge et al. - 2019 - Learning Domain-Invariant Representations of Histo.pdf:application/pdf}
}

@article{ren_unsupervised_2019,
	title = {Unsupervised Domain Adaptation for Classification of Histopathology Whole-Slide Images},
	volume = {7},
	issn = {2296-4185},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6529804/},
	doi = {10.3389/fbioe.2019.00102},
	abstract = {Computational image analysis is one means for evaluating digitized histopathology specimens that can increase the reproducibility and reliability with which cancer diagnoses are rendered while simultaneously providing insight as to the underlying mechanisms of disease onset and progression. A major challenge that is confronted when analyzing samples that have been prepared at disparate laboratories and institutions is that the algorithms used to assess the digitized specimens often exhibit heterogeneous staining characteristics because of slight differences in incubation times and the protocols used to prepare the samples. Unfortunately, such variations can render a prediction model learned from one batch of specimens ineffective for characterizing an ensemble originating from another site. In this work, we propose to adopt unsupervised domain adaptation to effectively transfer the discriminative knowledge obtained from any given source domain to the target domain without requiring any additional labeling or annotation of images at the target site. In this paper, our team investigates the use of two approaches for performing the adaptation: (1) color normalization and (2) adversarial training. The adversarial training strategy is implemented through the use of convolutional neural networks to find an invariant feature space and Siamese architecture within the target domain to add a regularization that is appropriate for the entire set of whole-slide images. The adversarial adaptation results in significant classification improvement compared with the baseline models under a wide range of experimental settings.},
	journaltitle = {Frontiers in Bioengineering and Biotechnology},
	shortjournal = {Front Bioeng Biotechnol},
	author = {Ren, Jian and Hacihaliloglu, Ilker and Singer, Eric A. and Foran, David J. and Qi, Xin},
	urldate = {2019-11-28},
	date = {2019-05-15},
	pmid = {31158269},
	pmcid = {PMC6529804},
	file = {PubMed Central Full Text PDF:/Users/trislaz/Zotero/storage/PRDY4GEV/Ren et al. - 2019 - Unsupervised Domain Adaptation for Classification .pdf:application/pdf}
}

@article{manie_genomic_2016,
	title = {Genomic hallmarks of homologous recombination deficiency in invasive breast carcinomas},
	volume = {138},
	issn = {1097-0215},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ijc.29829},
	doi = {10.1002/ijc.29829},
	abstract = {Therapeutic strategies targeting Homologous Recombination Deficiency ({HRD}) in breast cancer requires patient stratification. The {LST} (Large-scale State Transitions) genomic signature previously validated for triple-negative breast carcinomas ({TNBC}) was evaluated as biomarker of {HRD} in luminal (hormone receptor positive) and {HER}2-overexpressing ({HER}2+) tumors. The {LST} genomic signature related to the number of large-scale chromosomal breakpoints in {SNP}-array tumor profile was applied to identify {HRD} in in-house and {TCGA} sets of breast tumors, in which the status of {BRCA}1/2 and other genes was also investigated. In the in-house dataset, {HRD} was predicted in 5\% (20/385) of sporadic tumors luminal or {HER}2+ by the {LST} genomic signature and the inactivation of {BRCA}1, {BRCA}2 or {RAD}51C confirmed this prediction in 75\% (12/16) of the tested cases. In 14\% (6/43) of tumors occurring in {BRCA}1/2 mutant carriers, the corresponding wild-type allele was retained emphasizing the importance of determining the tumor status. In the {TCGA} luminal and {HER}2+ subtypes {HRD} incidence was estimated at 5\% (18/329, 95\%{CI}: 5–8\%) and 2\% (1/59, 95\%{CI}: 2–9\%), respectively. In {TNBC} cisplatin-based neo-adjuvant clinical trials, {HRD} is shown to be a necessary condition for cisplatin sensitivity. This analysis demonstrates the high performance of the {LST} genomic signature for {HRD} detection in breast cancers, which suggests its potential as a biomarker for genetic testing and patient stratification for clinical trials evaluating platinum salts and {PARP} inhibitors.},
	pages = {891--900},
	number = {4},
	journaltitle = {International Journal of Cancer},
	author = {Manié, Elodie and Popova, Tatiana and Battistella, Aude and Tarabeux, Julien and Caux‐Moncoutier, Virginie and Golmard, Lisa and Smith, Nicholas K. and Mueller, Christopher R. and Mariani, Odette and Sigal‐Zafrani, Brigitte and Dubois, Thierry and Vincent‐Salomon, Anne and Houdayer, Claude and Stoppa‐Lyonnet, Dominique and Stern, Marc-Henri},
	urldate = {2019-11-26},
	date = {2016},
	langid = {english},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/IU5U2JGX/Manié et al. - 2016 - Genomic hallmarks of homologous recombination defi.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/2VCE3IKZ/ijc.html:text/html}
}

@article{sirinukunwattana_locality_2016,
	title = {Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images},
	volume = {35},
	issn = {1558-254X},
	doi = {10.1109/TMI.2016.2525803},
	abstract = {Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network ({SC}-{CNN}) to perform nucleus detection. {SC}-{CNN} regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor ({NEP}) coupled with {CNN} to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed {SC}-{CNN} and {NEP} produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.},
	pages = {1196--1206},
	number = {5},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	author = {Sirinukunwattana, Korsuk and Raza, Shan E Ahmed and Tsang, Yee-Wah and Snead, David R. J. and Cree, Ian A. and Rajpoot, Nasir M.},
	date = {2016-05},
	file = {IEEE Xplore Abstract Record:/Users/trislaz/Zotero/storage/ID83CJUS/7399414.html:text/html;IEEE Xplore Full Text PDF:/Users/trislaz/Zotero/storage/WWRL28ET/Sirinukunwattana et al. - 2016 - Locality Sensitive Deep Learning for Detection and.pdf:application/pdf}
}

@article{wang_automatic_2016,
	title = {Automatic cell nuclei segmentation and classification of breast cancer histopathology images},
	volume = {122},
	issn = {0165-1684},
	url = {http://www.sciencedirect.com/science/article/pii/S0165168415003916},
	doi = {10.1016/j.sigpro.2015.11.011},
	abstract = {Breast cancer is the leading type of malignant tumor observed in women and the effective treatment depends on its early diagnosis. Diagnosis from histopathological images remains the "gold standard" for breast cancer. The complexity of breast cell histopathology ({BCH}) images makes reliable segmentation and classification hard. In this paper, an automatic quantitative image analysis technique of {BCH} images is proposed. For the nuclei segmentation, top-bottom hat transform is applied to enhance image quality. Wavelet decomposition and multi-scale region-growing ({WDMR}) are combined to obtain regions of interest ({ROIs}) thereby realizing precise location. A double-strategy splitting model ({DSSM}) containing adaptive mathematical morphology and Curvature Scale Space ({CSS}) corner detection method is applied to split overlapped cells for better accuracy and robustness. For the classification of cell nuclei, 4 shape-based features and 138 textural features based on color spaces are extracted. Optimal feature set is obtained by support vector machine ({SVM}) with chain-like agent genetic algorithm ({CAGA}). The proposed method was tested on 68 {BCH} images containing more than 3600 cells. Experimental results show that the mean segmentation sensitivity was 91.53\% (±4.05\%) and specificity was 91.64\% (±4.07\%). The classification performance of normal and malignant cell images can achieve 96.19\% (±0.31\%) for accuracy, 99.05\% (±0.27\%) for sensitivity and 93.33\% (±0.81\%) for specificity.},
	pages = {1--13},
	journaltitle = {Signal Processing},
	shortjournal = {Signal Processing},
	author = {Wang, Pin and Hu, Xianling and Li, Yongming and Liu, Qianqian and Zhu, Xinjian},
	urldate = {2019-11-25},
	date = {2016-05-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/trislaz/Zotero/storage/U5UBBE8P/Wang et al. - 2016 - Automatic cell nuclei segmentation and classificat.pdf:application/pdf;ScienceDirect Snapshot:/Users/trislaz/Zotero/storage/WANNRWEV/S0165168415003916.html:text/html}
}

@article{naylor_segmentation_2019,
	title = {Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map},
	volume = {38},
	issn = {1558-254X},
	doi = {10.1109/TMI.2018.2865709},
	abstract = {The advent of digital pathology provides us with the challenging opportunity to automatically analyze whole slides of diseased tissue in order to derive quantitative profiles that can be used for diagnosis and prognosis tasks. In particular, for the development of interpretable models, the detection and segmentation of cell nuclei is of the utmost importance. In this paper, we describe a new method to automatically segment nuclei from Haematoxylin and Eosin (H\&E) stained histopathology data with fully convolutional networks. In particular, we address the problem of segmenting touching nuclei by formulating the segmentation problem as a regression task of the distance map. We demonstrate superior performance of this approach as compared to other approaches using Convolutional Neural Networks.},
	pages = {448--459},
	number = {2},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	author = {Naylor, Peter and Laé, Marick and Reyal, Fabien and Walter, Thomas},
	date = {2019-02},
	file = {IEEE Xplore Abstract Record:/Users/trislaz/Zotero/storage/FKGH93WW/8438559.html:text/html;IEEE Xplore Full Text PDF:/Users/trislaz/Zotero/storage/HXAY8J84/Naylor et al. - 2019 - Segmentation of Nuclei in Histopathology Images by.pdf:application/pdf}
}

@article{graham_hover-net:_2019,
	title = {{HoVer}-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images},
	url = {http://arxiv.org/abs/1812.06499},
	shorttitle = {{HoVer}-Net},
	abstract = {Nuclear segmentation and classification within Haematoxylin \& Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow. The development of automated methods for nuclear segmentation and classification enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classification is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classification that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then for each segmented instance, the network predicts the type of nucleus via a devoted up-sampling branch. We demonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work, we introduce a new dataset of Haematoxylin \& Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with associated class labels.},
	journaltitle = {{arXiv}:1812.06499 [cs]},
	author = {Graham, Simon and Vu, Quoc Dang and Raza, Shan E. Ahmed and Azam, Ayesha and Tsang, Yee Wah and Kwak, Jin Tae and Rajpoot, Nasir},
	urldate = {2019-11-25},
	date = {2019-11-13},
	eprinttype = {arxiv},
	eprint = {1812.06499},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/XI8HY3E5/1812.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/E932I89V/Graham et al. - 2019 - HoVer-Net Simultaneous Segmentation and Classific.pdf:application/pdf}
}

@article{jamal-hanjani_translational_2015,
	title = {Translational implications of tumor heterogeneity},
	volume = {21},
	issn = {1078-0432},
	doi = {10.1158/1078-0432.CCR-14-1429},
	abstract = {Advances in next-generation sequencing and bioinformatics have led to an unprecedented view of the cancer genome and its evolution. Genomic studies have demonstrated the complex and heterogeneous clonal landscape of tumors of different origins and the potential impact of intratumor heterogeneity on treatment response and resistance, cancer progression, and the risk of disease relapse. However, the significance of subclonal mutations, in particular mutations in driver genes, and their evolution through time and their dynamics in response to cancer therapies, is yet to be determined. The necessary tools are now available to prospectively determine whether clonal heterogeneity can be used as a biomarker of clinical outcome and to what extent subclonal somatic alterations might influence clinical outcome. Studies that use longitudinal tissue sampling, integrating both genomic and clinical data, have the potential to reveal the subclonal composition and track the evolution of tumors to address these questions and to begin to define the breadth of genetic diversity in different tumor types and its relevance to patient outcome. Such studies may provide further evidence for drug-resistance mechanisms informing combinatorial, adaptive, and tumor immune therapies placed within the context of tumor evolution.},
	pages = {1258--1266},
	number = {6},
	journaltitle = {Clinical Cancer Research: An Official Journal of the American Association for Cancer Research},
	shortjournal = {Clin. Cancer Res.},
	author = {Jamal-Hanjani, Mariam and Quezada, Sergio A. and Larkin, James and Swanton, Charles},
	date = {2015-03-15},
	pmid = {25770293},
	pmcid = {PMC4374162},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/XG8ZYYBW/Jamal-Hanjani et al. - 2015 - Translational implications of tumor heterogeneity.pdf:application/pdf}
}

@article{ma_tumor_2019,
	title = {Tumor Cell Biodiversity Drives Microenvironmental Reprogramming in Liver Cancer},
	volume = {36},
	issn = {1878-3686},
	doi = {10.1016/j.ccell.2019.08.007},
	abstract = {Cellular diversity in tumors is a key factor for therapeutic failures and lethal outcomes of solid malignancies. Here, we determined the single-cell transcriptomic landscape of liver cancer biospecimens from 19 patients. We found varying degrees of heterogeneity in malignant cells within and between tumors and diverse landscapes of tumor microenvironment ({TME}). Strikingly, tumors with higher transcriptomic diversity were associated with patient's worse overall survival. We found a link between hypoxia-dependent vascular endothelial growth factor expression in tumor diversity and {TME} polarization. Moreover, T cells from higher heterogeneous tumors showed lower cytolytic activities. Consistent results were found using bulk genomic and transcriptomic profiles of 765 liver tumors. Our results offer insight into the diverse ecosystem of liver cancer and its impact on patient prognosis.},
	pages = {418--430.e6},
	number = {4},
	journaltitle = {Cancer Cell},
	shortjournal = {Cancer Cell},
	author = {Ma, Lichun and Hernandez, Maria O. and Zhao, Yongmei and Mehta, Monika and Tran, Bao and Kelly, Michael and Rae, Zachary and Hernandez, Jonathan M. and Davis, Jeremy L. and Martin, Sean P. and Kleiner, David E. and Hewitt, Stephen M. and Ylaya, Kris and Wood, Bradford J. and Greten, Tim F. and Wang, Xin Wei},
	date = {2019-10-14},
	pmid = {31588021},
	pmcid = {PMC6801104},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/RYABSNI8/Ma et al. - 2019 - Tumor Cell Biodiversity Drives Microenvironmental .pdf:application/pdf}
}

@article{mcgranahan_biological_2015,
	title = {Biological and Therapeutic Impact of Intratumor Heterogeneity in Cancer Evolution},
	volume = {27},
	issn = {1535-6108},
	url = {http://www.sciencedirect.com/science/article/pii/S1535610814005108},
	doi = {10.1016/j.ccell.2014.12.001},
	abstract = {Precision medicine requires an understanding of cancer genes and mutational processes, as well as an appreciation of the extent to which these are found heterogeneously in cancer cells during tumor evolution. Here, we explore the processes shaping the cancer genome, placing these within the context of tumor evolution and their impact on intratumor heterogeneity and drug development. We review evidence for constraints and contingencies to tumor evolution and highlight the clinical implications of diversity within tumors. We outline the limitations of genome-driven targeted therapies and explore future strategies, including immune and adaptive approaches, to address this therapeutic challenge.},
	pages = {15--26},
	number = {1},
	journaltitle = {Cancer Cell},
	shortjournal = {Cancer Cell},
	author = {{McGranahan}, Nicholas and Swanton, Charles},
	urldate = {2019-11-21},
	date = {2015-01-12},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/trislaz/Zotero/storage/Y8AS6CIS/McGranahan et Swanton - 2015 - Biological and Therapeutic Impact of Intratumor He.pdf:application/pdf;ScienceDirect Snapshot:/Users/trislaz/Zotero/storage/G7DW3BDE/S1535610814005108.html:text/html}
}

@article{luecken_current_2019,
	title = {Current best practices in single-cell {RNA}-seq analysis: a tutorial},
	volume = {15},
	issn = {1744-4292},
	url = {https://www.embopress.org/doi/10.15252/msb.20188746},
	doi = {10.15252/msb.20188746},
	shorttitle = {Current best practices in single-cell {RNA}-seq analysis},
	abstract = {Abstract Single-cell {RNA}-seq has enabled gene expression to be studied at an unprecedented resolution. The promise of this technology is attracting a growing user base for single-cell analysis methods. As more analysis tools are becoming available, it is becoming increasingly difficult to navigate this landscape and produce an up-to-date workflow to analyse one's data. Here, we detail the steps of a typical single-cell {RNA}-seq analysis, including pre-processing (quality control, normalization, data correction, feature selection, and dimensionality reduction) and cell- and gene-level downstream analysis. We formulate current best-practice recommendations for these steps based on independent comparison studies. We have integrated these best-practice recommendations into a workflow, which we apply to a public dataset to further illustrate how these steps work in practice. Our documented case study can be found at https://www.github.com/theislab/single-cell-tutorial. This review will serve as a workflow tutorial for new entrants into the field, and help established users update their analysis pipelines.},
	pages = {e8746},
	number = {6},
	journaltitle = {Molecular Systems Biology},
	shortjournal = {Molecular Systems Biology},
	author = {Luecken, Malte D and Theis, Fabian J},
	urldate = {2019-11-21},
	date = {2019-06-01},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/NU6I5NLF/Luecken et Theis - 2019 - Current best practices in single-cell RNA-seq anal.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/IQCMJEQI/msb.html:text/html}
}

@article{beerenwinkel_cancer_2015,
	title = {Cancer Evolution: Mathematical Models and Computational Inference},
	volume = {64},
	issn = {1063-5157},
	url = {https://academic.oup.com/sysbio/article/64/1/e1/2848310},
	doi = {10.1093/sysbio/syu081},
	shorttitle = {Cancer Evolution},
	abstract = {Abstract.  Cancer is a somatic evolutionary process characterized by the accumulation of mutations, which contribute to tumor growth, clinical progression, immu},
	pages = {e1--e25},
	number = {1},
	journaltitle = {Systematic Biology},
	shortjournal = {Syst Biol},
	author = {Beerenwinkel, Niko and Schwarz, Roland F. and Gerstung, Moritz and Markowetz, Florian},
	urldate = {2019-11-19},
	date = {2015-01-01},
	langid = {english},
	file = {Snapshot:/Users/trislaz/Zotero/storage/QPGZ9UJN/2848310.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/7WKE82JM/Beerenwinkel et al. - 2015 - Cancer Evolution Mathematical Models and Computat.pdf:application/pdf}
}

@article{navin_tracing_2010,
	title = {Tracing the tumor lineage},
	volume = {4},
	issn = {1574-7891},
	url = {http://www.sciencedirect.com/science/article/pii/S1574789110000323},
	doi = {10.1016/j.molonc.2010.04.010},
	series = {Thematic Issue: The Molecular Biology of Breast Cancer},
	abstract = {Defining the pathways through which tumors progress is critical to our understanding and treatment of cancer. We do not routinely sample patients at multiple time points during the progression of their disease, and thus our research is limited to inferring progression a posteriori from the examination of a single tumor sample. Despite this limitation, inferring progression is possible because the tumor genome contains a natural history of the mutations that occur during the formation of the tumor mass. There are two approaches to reconstructing a lineage of progression: (1) inter-tumor comparisons, and (2) intra-tumor comparisons. The inter-tumor approach consists of taking single samples from large collections of tumors and comparing the complexity of the genomes to identify early and late mutations. The intra-tumor approach involves taking multiple samples from individual heterogeneous tumors to compare divergent clones and reconstruct a phylogenetic lineage. Here we discuss how these approaches can be used to interpret the current models for tumor progression. We also compare data from primary and metastatic copy number profiles to shed light on the final steps of breast cancer progression. Finally, we discuss how recent technical advances in single cell genomics will herald a new era in understanding the fundamental basis of tumor heterogeneity and progression.},
	pages = {267--283},
	number = {3},
	journaltitle = {Molecular Oncology},
	shortjournal = {Molecular Oncology},
	author = {Navin, Nicholas E. and Hicks, James},
	urldate = {2019-11-19},
	date = {2010-06-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/trislaz/Zotero/storage/HR7LSF99/Navin et Hicks - 2010 - Tracing the tumor lineage.pdf:application/pdf;ScienceDirect Snapshot:/Users/trislaz/Zotero/storage/MYS9AWRK/S1574789110000323.html:text/html}
}

@article{lagaert_modelisation_nodate,
	title = {Modélisation de la croissance tumorale: estimation de paramètres d'un modèle de croissance et introduction d'un modèle spécifique aux gliomes de tout grade},
	abstract = {This thesis deals with mathematical modeling of tumor growth. Firstly, we present a parameter estimation method. More precisely, it consists in recovering the position of the tumor blood vessel, starting from imaging. The ﬁrst step is to design a particular vascularization, then we compute the tumor growth with this blood-vessel network by using a model based on partial diﬀerential equations and hence we try to recover the initial vascularization solving the inverse problem. We show that the estimated vasculature could be used to eﬃciently predict the future tumor growth.},
	pages = {191},
	author = {Lagaert, Jean-Baptiste},
	langid = {french},
	file = {Lagaert - Modélisation de la croissance tumorale estimation.pdf:/Users/trislaz/Zotero/storage/TB9NTNSD/Lagaert - Modélisation de la croissance tumorale estimation.pdf:application/pdf}
}

@article{mcinnes_umap:_2018,
	title = {{UMAP}: Uniform Manifold Approximation and Projection for Dimension Reduction},
	url = {http://arxiv.org/abs/1802.03426},
	shorttitle = {{UMAP}},
	abstract = {{UMAP} (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. {UMAP} is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The {UMAP} algorithm is competitive with t-{SNE} for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, {UMAP} has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	journaltitle = {{arXiv}:1802.03426 [cs, stat]},
	author = {{McInnes}, Leland and Healy, John and Melville, James},
	urldate = {2019-11-19},
	date = {2018-12-06},
	eprinttype = {arxiv},
	eprint = {1802.03426},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/YFMHSNLR/1802.html:text/html;arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/HUTGGS6H/McInnes et al. - 2018 - UMAP Uniform Manifold Approximation and Projectio.pdf:application/pdf}
}

@article{moschetta_brca_2016,
	title = {{BRCA} somatic mutations and epigenetic {BRCA} modifications in serous ovarian cancer},
	volume = {27},
	issn = {1569-8041},
	doi = {10.1093/annonc/mdw142},
	abstract = {The significant activity of poly({ADP}-ribose)polymerase ({PARP}) inhibitors in the treatment of germline {BRCA} mutation-associated ovarian cancer, which represents ∼15\% of {HGS} cases, has recently led to European Medicines Agency and food and drug administration approval of olaparib. Accumulating evidence suggests that {PARP} inhibitors may have a wider application in the treatment of sporadic ovarian cancers. Up to 50\% of {HGS} ovarian cancer patients may exhibit homologous recombination deficiency ({HRD}) through mechanisms including germline {BRCA} mutations, somatic {BRCA} mutations, and {BRCA} promoter methylation. In this review, we discuss the role of somatic {BRCA} mutations and {BRCA} methylation in ovarian cancer. There is accumulating evidence for routine somatic {BRCA} mutation testing, but the relevance of {BRCA} epigenetic modifications is less clear. We explore the challenges that need to be addressed if the full potential of these markers of {HRD} is to be utilised in clinical practice.},
	pages = {1449--1455},
	number = {8},
	journaltitle = {Annals of Oncology: Official Journal of the European Society for Medical Oncology},
	shortjournal = {Ann. Oncol.},
	author = {Moschetta, M. and George, A. and Kaye, S. B. and Banerjee, S.},
	date = {2016},
	pmid = {27037296},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/YZTYG8T3/Moschetta et al. - 2016 - BRCA somatic mutations and epigenetic BRCA modific.pdf:application/pdf}
}

@article{george_delivering_2017,
	title = {Delivering widespread {BRCA} testing and {PARP} inhibition to patients with ovarian cancer},
	volume = {14},
	issn = {1759-4782},
	doi = {10.1038/nrclinonc.2016.191},
	abstract = {The treatment of patients with ovarian cancer is rapidly changing following the success of poly [{ADP}-ribose] polymerase ({PARP}) inhibitors in clinical trials. Olaparib is the first {PARP} inhibitor to be approved by the {EMA} and {FDA} for {BRCA}-mutated ovarian cancer. Germ line {BRCA} mutation status is now established as a predictive biomarker of potential benefit from treatment with a {PARP} inhibitor; therefore, knowledge of the {BRCA} status of an individual patient with ovarian cancer is essential, in order to guide treatment decisions. {BRCA} testing was previously offered only to women with a family or personal history of breast and/or ovarian cancer; however, almost 20\% of women with high-grade serous ovarian cancer are now recognized to harbour a germ line {BRCA} mutation, and of these, {\textgreater}40\% might not have a family history of cancer and would not have received {BRCA} testing. A strategy to enable more widespread implementation of {BRCA} testing in routine care is, therefore, necessary. In this Review, we summarize data from key clinical trials of {PARP} inhibitors and discuss how to integrate these agents into the current treatment landscape of ovarian cancer. The validity of germ line {BRCA} testing and other promising biomarkers of homologous-recombination deficiency will also be discussed.},
	pages = {284--296},
	number = {5},
	journaltitle = {Nature Reviews. Clinical Oncology},
	shortjournal = {Nat Rev Clin Oncol},
	author = {George, Angela and Kaye, Stan and Banerjee, Susana},
	date = {2017},
	pmid = {27958297},
	file = {george2016.pdf:/home/lazard/Documents/cbio/papiers/george2016.pdf:application/pdf}
}

@article{lee_brca-associated_2017,
	title = {{BRCA}-associated Cancers: Role of Imaging in Screening, Diagnosis, and Management},
	volume = {37},
	issn = {0271-5333},
	url = {https://pubs.rsna.org/doi/10.1148/rg.2017160144},
	doi = {10.1148/rg.2017160144},
	shorttitle = {{BRCA}-associated Cancers},
	abstract = {Harmful mutations of the {BRCA} tumor suppressor genes result in a                    greater lifetime risk for malignancy—breast and ovarian cancers in                    particular. An increased risk for male breast, fallopian tube, primary                    peritoneal, pancreatic, prostate, and colon cancers also has been reported. The                        {BRCA} gene is inherited in an autosomal dominant pattern and                    tends to be highly penetrant; thus, there is an increased incidence of these                    cancers in affected families. Compared with sporadic tumors,                        {BRCA}-associated malignancies have unique manifestations,                    clinical features, and pathologic profiles. Manifestation at an early patient                    age, high-grade tumors, and an aggressive clinical course are common features of                        {BRCA}-associated malignancies. Understanding the behavior of                    these cancers aids in identification of affected individuals and families, who                    can then make informed decisions regarding their future health. Enhanced                    screening, prophylactic surgery, and chemoprevention are options for managing                    cancer risk factors in these individuals. Imaging has an important role in the                    screening, evaluation, staging, and follow-up of                    {BRCA}-associated malignancies. Supplemental screening of                        {BRCA} mutation carriers often begins at an early age and is                    critical for early and accurate cancer diagnoses. The authors review the                    etiopathogenesis and imaging features of {BRCA}-associated                    malignancies, the importance of a multidisciplinary approach to determining the                    diagnosis, and the treatment of patients who have these mutations to improve                    their outcomes.© {RSNA}, 2017},
	pages = {1005--1023},
	number = {4},
	journaltitle = {{RadioGraphics}},
	shortjournal = {{RadioGraphics}},
	author = {Lee, Michelle                            V. and Katabathina, Venkata                            S. and Bowerson, Michyla                            L. and Mityul, Marina                            I. and Shetty, Anup                        S. and Elsayes, Khaled                            M. and Balachandran, Aparna and Bhosale, Priya                            R. and {McCullough}, Ann                            E. and Menias, Christine                            O.},
	urldate = {2019-11-15},
	date = {2017-05-26},
	file = {Snapshot:/Users/trislaz/Zotero/storage/2QC2HZ5C/rg.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/XWK8TADN/Lee et al. - 2017 - BRCA-associated Cancers Role of Imaging in Screen.pdf:application/pdf}
}

@inproceedings{cholakkal_object_2019,
	title = {Object Counting and Instance Segmentation With Image-Level Supervision},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Cholakkal_Object_Counting_and_Instance_Segmentation_With_Image-Level_Supervision_CVPR_2019_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {12397--12405},
	author = {Cholakkal, Hisham and Sun, Guolei and Khan, Fahad Shahbaz and Shao, Ling},
	urldate = {2019-11-14},
	date = {2019},
	file = {Snapshot:/Users/trislaz/Zotero/storage/MXH3836P/Cholakkal_Object_Counting_and_Instance_Segmentation_With_Image-Level_Supervision_CVPR_2019_pape.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/9VSYU643/Cholakkal et al. - 2019 - Object Counting and Instance Segmentation With Ima.pdf:application/pdf}
}

@article{yuan_quantitative_2012,
	title = {Quantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling},
	volume = {4},
	issn = {1946-6242},
	doi = {10.1126/scitranslmed.3004330},
	abstract = {Solid tumors are heterogeneous tissues composed of a mixture of cancer and normal cells, which complicates the interpretation of their molecular profiles. Furthermore, tissue architecture is generally not reflected in molecular assays, rendering this rich information underused. To address these challenges, we developed a computational approach based on standard hematoxylin and eosin-stained tissue sections and demonstrated its power in a discovery and validation cohort of 323 and 241 breast tumors, respectively. To deconvolute cellular heterogeneity and detect subtle genomic aberrations, we introduced an algorithm based on tumor cellularity to increase the comparability of copy number profiles between samples. We next devised a predictor for survival in estrogen receptor-negative breast cancer that integrated both image-based and gene expression analyses and significantly outperformed classifiers that use single data types, such as microarray expression signatures. Image processing also allowed us to describe and validate an independent prognostic factor based on quantitative analysis of spatial patterns between stromal cells, which are not detectable by molecular assays. Our quantitative, image-based method could benefit any large-scale cancer study by refining and complementing molecular assays of tumor samples.},
	pages = {157ra143},
	number = {157},
	journaltitle = {Science Translational Medicine},
	shortjournal = {Sci Transl Med},
	author = {Yuan, Yinyin and Failmezger, Henrik and Rueda, Oscar M. and Ali, H. Raza and Gräf, Stefan and Chin, Suet-Feung and Schwarz, Roland F. and Curtis, Christina and Dunning, Mark J. and Bardwell, Helen and Johnson, Nicola and Doyle, Sarah and Turashvili, Gulisa and Provenzano, Elena and Aparicio, Sam and Caldas, Carlos and Markowetz, Florian},
	date = {2012-10-24},
	pmid = {23100629}
}

@article{yuan_modelling_2015,
	title = {Modelling the spatial heterogeneity and molecular correlates of lymphocytic infiltration in triple-negative breast cancer},
	volume = {12},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2014.1153},
	doi = {10.1098/rsif.2014.1153},
	abstract = {Lymphocytic infiltration is associated with a favourable prognosis and predicts response to chemotherapy in many cancer types, including the aggressive triple-negative breast cancer ({TNBC}). However, it is not well understood owing to the high levels of spatial heterogeneity within tumours, which is difficult to analyse by traditional pathological assessment. This paper describes an unbiased methodology to statistically model the spatial distribution of lymphocytes among tumour cells based on automated analysis of haematoxylin-and-eosin-stained whole-tumour section images, which is applied to two independent {TNBC} cohorts of 181 patients with matched microarray gene expression data. The novelty of the proposed methodology is the fusion of image analysis and statistical modelling for an integrative understanding of intratumour heterogeneity of lymphocytic infiltration. Using this methodology, a quantitative measure of intratumour lymphocyte ratio is developed and found to be significantly associated with disease-specific survival in both {TNBC} cohorts independent to standard clinical parameters. The proposed image-based measure compares favourably to a number of gene expression signatures of immune infiltration. In addition, heterogeneous immune infiltration at the morphological level is reflected at the molecular scale and correlated with increased expression of {CTLA}4, the target of ipilimumab. Taken together, these results support the fusion of high-throughput image analysis and statistical modelling to offer reproducible and robust biomarkers for the objective identification of patients with poor prognosis and treatment options.},
	pages = {20141153},
	number = {103},
	journaltitle = {Journal of The Royal Society Interface},
	shortjournal = {Journal of The Royal Society Interface},
	author = {Yuan, Yinyin},
	urldate = {2019-11-13},
	date = {2015-02-06},
	file = {Snapshot:/Users/trislaz/Zotero/storage/YXE2EQVQ/rsif.2014.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/4YBCSGXT/Yuan - 2015 - Modelling the spatial heterogeneity and molecular .pdf:application/pdf}
}

@article{chen_computer-aided_2017,
	title = {Computer-aided prognosis on breast cancer with hematoxylin and eosin histopathology images: A review},
	volume = {39},
	issn = {1010-4283},
	url = {https://doi.org/10.1177/1010428317694550},
	doi = {10.1177/1010428317694550},
	shorttitle = {Computer-aided prognosis on breast cancer with hematoxylin and eosin histopathology images},
	abstract = {With the advance of digital pathology, image analysis has begun to show its advantages in information analysis of hematoxylin and eosin histopathology images. Generally, histological features in hematoxylin and eosin images are measured to evaluate tumor grade and prognosis for breast cancer. This review summarized recent works in image analysis of hematoxylin and eosin histopathology images for breast cancer prognosis. First, prognostic factors for breast cancer based on hematoxylin and eosin histopathology images were summarized. Then, usual procedures of image analysis for breast cancer prognosis were systematically reviewed, including image acquisition, image preprocessing, image detection and segmentation, and feature extraction. Finally, the prognostic value of image features and image feature–based prognostic models was evaluated. Moreover, we discussed the issues of current analysis, and some directions for future research.},
	pages = {1010428317694550},
	number = {3},
	journaltitle = {Tumor Biology},
	shortjournal = {Tumour Biol.},
	author = {Chen, Jia-Mei and Li, Yan and Xu, Jun and Gong, Lei and Wang, Lin-Wei and Liu, Wen-Lou and Liu, Juan},
	urldate = {2019-11-07},
	date = {2017-03-01},
	langid = {english},
	file = {SAGE PDF Full Text:/Users/trislaz/Zotero/storage/9U52ECJ8/Chen et al. - 2017 - Computer-aided prognosis on breast cancer with hem.pdf:application/pdf}
}

@inproceedings{zhou_weakly_2018,
	location = {Salt Lake City, {UT}},
	title = {Weakly Supervised Instance Segmentation Using Class Peak Response},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578497/},
	doi = {10.1109/CVPR.2018.00399},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {3791--3800},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Zhou, Yanzhao and Zhu, Yi and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},
	urldate = {2019-11-07},
	date = {2018-06},
	langid = {english},
	file = {Zhou et al. - 2018 - Weakly Supervised Instance Segmentation Using Clas.pdf:/Users/trislaz/Zotero/storage/JRM64I4G/Zhou et al. - 2018 - Weakly Supervised Instance Segmentation Using Clas.pdf:application/pdf}
}

@inproceedings{durand_wildcat:_2017,
	title = {{WILDCAT}: Weakly Supervised Learning of Deep {ConvNets} for Image Classification, Pointwise Localization and Segmentation},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Durand_WILDCAT_Weakly_Supervised_CVPR_2017_paper.html},
	shorttitle = {{WILDCAT}},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {642--651},
	author = {Durand, Thibaut and Mordan, Taylor and Thome, Nicolas and Cord, Matthieu},
	urldate = {2019-11-07},
	date = {2017},
	file = {Snapshot:/Users/trislaz/Zotero/storage/PGYEFMPW/Durand_WILDCAT_Weakly_Supervised_CVPR_2017_paper.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/PTBT94SX/Durand et al. - 2017 - WILDCAT Weakly Supervised Learning of Deep ConvNe.pdf:application/pdf}
}

@article{jegou_aggregating_2012,
	title = {Aggregating Local Image Descriptors into Compact Codes},
	volume = {34},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6104058/},
	doi = {10.1109/TPAMI.2011.235},
	abstract = {This paper addresses the problem of large-scale image search. Three constraints have to be taken into account: search accuracy, efﬁciency, and memory usage. We ﬁrst present and evaluate different ways of aggregating local image descriptors into a vector and show that the Fisher kernel achieves better performance than the reference bag-of-visual words approach for any given vector dimension. We then jointly optimize dimensionality reduction and indexing in order to obtain a precise vector comparison as well as a compact representation. The evaluation shows that the image representation can be reduced to a few dozen bytes while preserving high accuracy. Searching a 100 million image dataset takes about 250 ms on one processor core.},
	pages = {1704--1716},
	number = {9},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Jegou, H. and Perronnin, F. and Douze, M. and Sanchez, J. and Perez, P. and Schmid, C.},
	urldate = {2019-11-07},
	date = {2012-09},
	langid = {english},
	file = {Jegou et al. - 2012 - Aggregating Local Image Descriptors into Compact C.pdf:/Users/trislaz/Zotero/storage/BXTEFJVJ/Jegou et al. - 2012 - Aggregating Local Image Descriptors into Compact C.pdf:application/pdf}
}

@inproceedings{durand_weldon:_2016,
	title = {{WELDON}: Weakly Supervised Learning of Deep Convolutional Neural Networks},
	doi = {10.1109/CVPR.2016.513},
	shorttitle = {{WELDON}},
	pages = {4743--4752},
	author = {Durand, Thibaut and Thome, Nicolas and Cord, Matthieu},
	date = {2016-06-01},
	file = {Version soumise:/Users/trislaz/Zotero/storage/6CNF3CKM/Durand et al. - 2016 - WELDON Weakly Supervised Learning of Deep Convolu.pdf:application/pdf}
}

@article{janowczyk_deep_2016,
	title = {Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases},
	volume = {7},
	issn = {2153-3539},
	url = {http://www.jpathinformatics.org/text.asp?2016/7/1/29/186902},
	doi = {10.4103/2153-3539.186902},
	shorttitle = {Deep learning for digital pathology image analysis},
	pages = {29},
	number = {1},
	journaltitle = {Journal of Pathology Informatics},
	shortjournal = {J Pathol Inform},
	author = {Janowczyk, Andrew and Madabhushi, Anant},
	urldate = {2019-11-04},
	date = {2016},
	langid = {english},
	file = {Janowczyk et Madabhushi - 2016 - Deep learning for digital pathology image analysis.pdf:/Users/trislaz/Zotero/storage/P7V3ZULF/Janowczyk et Madabhushi - 2016 - Deep learning for digital pathology image analysis.pdf:application/pdf}
}

@article{humphrey_gleason_2004,
	title = {Gleason grading and prognostic factors in carcinoma of the prostate},
	volume = {17},
	issn = {0893-3952},
	doi = {10.1038/modpathol.3800054},
	abstract = {Gleason grade of adenocarcinoma of the prostate is an established prognostic indicator that has stood the test of time. The Gleason grading method was devised in the 1960s and 1970s by Dr Donald F Gleason and members of the Veterans Administration Cooperative Urological Research Group. This grading system is based entirely on the histologic pattern of arrangement of carcinoma cells in H\&E-stained sections. Five basic grade patterns are used to generate a histologic score, which can range from 2 to 10. These patterns are illustrated in a standard drawing that can be employed as a guide for recognition of the specific Gleason grades. Increasing Gleason grade is directly related to a number of histopathologic end points, including tumor size, margin status, and pathologic stage. Indeed, models have been developed that allow for pretreatment prediction of pathologic stage based upon needle biopsy Gleason grade, total serum prostate-specific antigen level, and clinical stage. Gleason grade has been linked to a number of clinical end points, including clinical stage, progression to metastatic disease, and survival. Gleason grade is often incorporated into nomograms used to predict response to a specific therapy, such as radiotherapy or surgery. Needle biopsy Gleason grade is routinely used to plan patient management and is also often one of the criteria for eligibility for clinical trials testing new therapies. Gleason grade should be routinely reported for adenocarcinoma of the prostate in all types of tissue samples. Experimental approaches that could be of importance in the future include determination of percentage of high-grade Gleason pattern 4 or 5, and utilization of markers discovered by gene expression profiling or by genetic testing for {DNA} abnormalities. Such markers would be of prognostic usefulness if they provided added value beyond the established indicators of Gleason grade, serum prostate-specific antigen, and stage. Currently, established prognostic factors for prostatic carcinoma recommended for routine reporting are {TNM} stage, surgical margin status, serum prostate-specific antigen, and Gleason grade.},
	pages = {292--306},
	number = {3},
	journaltitle = {Modern Pathology: An Official Journal of the United States and Canadian Academy of Pathology, Inc},
	shortjournal = {Mod. Pathol.},
	author = {Humphrey, Peter A.},
	date = {2004-03},
	pmid = {14976540}
}

@article{genestie_comparison_1998,
	title = {Comparison of the prognostic value of Scarff-Bloom-Richardson and Nottingham histological grades in a series of 825 cases of breast cancer: major importance of the mitotic count as a component of both grading systems},
	volume = {18},
	issn = {0250-7005},
	shorttitle = {Comparison of the prognostic value of Scarff-Bloom-Richardson and Nottingham histological grades in a series of 825 cases of breast cancer},
	abstract = {The most commonly used system in Europe for breast carcinoma was developed by Scarff, Bloom and Richardson ({SBR}). It was recently modified by Elston and Ellis and significant improvement in reproducibility has been shown by using precise grading guidelines. This study investigated whether the use of this new grade (defined as the Nottingham grade, {NG}) would improve the prognostic stratification of patients. The respective prognostic value of the two grading schemes was compared in a retrospective series of 825 patients uniformly treated for a small invasive breast carcinoma and followed for a median of 6 years. Univariate and multivariate analysis showed that both histological grades were strongly correlated to overall and metastasis free survival. We have separately analysed the prognostic value of each of the three components used to assess the two grading systems and found that the mitotic index was the only significant prognostic factor for 5 year survival. Univariate analysis showed the count to be more discriminant in the {NG} scheme (p = 0.0006) than in the {SBR} scheme (p = 0.04). However, in univariate and multivariate analysis, the prognostic value of the global {NG} was not significantly better than {SBR} grade. This may be related, in part, to an uneven distribution of cases reflected by a much lower number of cases with a high mitotic index in the {NG} system (2\%) than in the {SBR} system (10\%). Our study emphasizes the importance of the mitotic count in assessing the prognosis of breast cancers and indicates that the factors which condition this count (tissue processing, microscopic observation, threshold) must be well standardized and controlled.},
	pages = {571--576},
	number = {1},
	journaltitle = {Anticancer Research},
	shortjournal = {Anticancer Res.},
	author = {Genestie, C. and Zafrani, B. and Asselain, B. and Fourquet, A. and Rozan, S. and Validire, P. and Vincent-Salomon, A. and Sastre-Garau, X.},
	date = {1998-02},
	pmid = {9568179}
}

@article{wang_mitosis_2014,
	title = {Mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features},
	volume = {1},
	issn = {2329-4302},
	doi = {10.1117/1.JMI.1.3.034003},
	abstract = {Breast cancer ({BCa}) grading plays an important role in predicting disease aggressiveness and patient outcome. A key component of {BCa} grade is the mitotic count, which involves quantifying the number of cells in the process of dividing (i.e., undergoing mitosis) at a specific point in time. Currently, mitosis counting is done manually by a pathologist looking at multiple high power fields ({HPFs}) on a glass slide under a microscope, an extremely laborious and time consuming process. The development of computerized systems for automated detection of mitotic nuclei, while highly desirable, is confounded by the highly variable shape and appearance of mitoses. Existing methods use either handcrafted features that capture certain morphological, statistical, or textural attributes of mitoses or features learned with convolutional neural networks ({CNN}). Although handcrafted features are inspired by the domain and the particular application, the data-driven {CNN} models tend to be domain agnostic and attempt to learn additional feature bases that cannot be represented through any of the handcrafted features. On the other hand, {CNN} is computationally more complex and needs a large number of labeled training instances. Since handcrafted features attempt to model domain pertinent attributes and {CNN} approaches are largely supervised feature generation methods, there is an appeal in attempting to combine these two distinct classes of feature generation strategies to create an integrated set of attributes that can potentially outperform either class of feature extraction strategies individually. We present a cascaded approach for mitosis detection that intelligently combines a {CNN} model and handcrafted features (morphology, color, and texture features). By employing a light {CNN} model, the proposed approach is far less demanding computationally, and the cascaded strategy of combining handcrafted features and {CNN}-derived features enables the possibility of maximizing the performance by leveraging the disconnected feature sets. Evaluation on the public {ICPR}12 mitosis dataset that has 226 mitoses annotated on 35 {HPFs} ([Formula: see text] magnification) by several pathologists and 15 testing {HPFs} yielded an [Formula: see text]-measure of 0.7345. Our approach is accurate, fast, and requires fewer computing resources compared to existent methods, making this feasible for clinical use.},
	pages = {034003},
	number = {3},
	journaltitle = {Journal of Medical Imaging (Bellingham, Wash.)},
	shortjournal = {J Med Imaging (Bellingham)},
	author = {Wang, Haibo and Cruz-Roa, Angel and Basavanhally, Ajay and Gilmore, Hannah and Shih, Natalie and Feldman, Mike and Tomaszewski, John and Gonzalez, Fabio and Madabhushi, Anant},
	date = {2014-10},
	pmid = {26158062},
	pmcid = {PMC4479031},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/Y7RIQBIC/Wang et al. - 2014 - Mitosis detection in breast cancer pathology image.pdf:application/pdf}
}

@article{coudray_classification_2018,
	title = {Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning},
	volume = {24},
	rights = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-018-0177-5},
	doi = {10.1038/s41591-018-0177-5},
	abstract = {A convolutional neural network model using feature extraction and machine-learning techniques provides a tool for classification of lung cancer histopathology images and predicting mutational status of driver oncogenes},
	pages = {1559--1567},
	number = {10},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Coudray, Nicolas and Ocampo, Paolo Santiago and Sakellaropoulos, Theodore and Narula, Navneet and Snuderl, Matija and Fenyö, David and Moreira, Andre L. and Razavian, Narges and Tsirigos, Aristotelis},
	urldate = {2019-11-04},
	date = {2018-10},
	langid = {english},
	keywords = {toread},
	file = {Snapshot:/Users/trislaz/Zotero/storage/3EQR7KT2/s41591-018-0177-5.html:text/html;coudray2018.pdf:/Users/trislaz/Documents/cbio/papiers/coudray2018.pdf:application/pdf}
}

@article{campanella_clinical-grade_2019,
	title = {Clinical-grade computational pathology using weakly supervised deep learning on whole slide images},
	volume = {25},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-019-0508-1},
	doi = {10.1038/s41591-019-0508-1},
	abstract = {A deep learning model trained on real-world digital pathology data achieves clinical performance in cancer diagnosis.},
	pages = {1301--1309},
	number = {8},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and Silva, Vitor Werneck Krauss and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
	urldate = {2019-11-04},
	date = {2019-08},
	langid = {english},
	file = {Snapshot:/Users/trislaz/Zotero/storage/3BAHDFWQ/s41591-019-0508-1.html:text/html}
}

@article{bera_artificial_2019,
	title = {Artificial intelligence in digital pathology — new tools for diagnosis and precision oncology},
	volume = {16},
	rights = {2019 Springer Nature Limited},
	issn = {1759-4782},
	url = {https://www.nature.com/articles/s41571-019-0252-y},
	doi = {10.1038/s41571-019-0252-y},
	abstract = {The authors of this Perspective critically evaluate various artificial intelligence ({AI})-based computational approaches used for digital pathology and provide a broad framework to incorporate these tools into clinical oncology, discussing challenges such as the need for well-curated validation datasets, regulatory approval and fair reimbursement strategies.},
	pages = {703--715},
	number = {11},
	journaltitle = {Nature Reviews Clinical Oncology},
	shortjournal = {Nat Rev Clin Oncol},
	author = {Bera, Kaustav and Schalper, Kurt A. and Rimm, David L. and Velcheti, Vamsidhar and Madabhushi, Anant},
	urldate = {2019-11-04},
	date = {2019-11},
	langid = {english},
	file = {Snapshot:/Users/trislaz/Zotero/storage/9FYAV5TM/s41571-019-0252-y.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/6KLNZ462/Bera et al. - 2019 - Artificial intelligence in digital pathology — new.pdf:application/pdf}
}

@article{kather_deep_2019,
	title = {Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer},
	volume = {25},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-019-0462-y},
	doi = {10.1038/s41591-019-0462-y},
	abstract = {A deep residual learning framework identifies microsatellite instability in histology slides from patients with cancer and can be used to guide immunotherapy.},
	pages = {1054--1056},
	number = {7},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Kather, Jakob Nikolas and Pearson, Alexander T. and Halama, Niels and Jäger, Dirk and Krause, Jeremias and Loosen, Sven H. and Marx, Alexander and Boor, Peter and Tacke, Frank and Neumann, Ulf Peter and Grabsch, Heike I. and Yoshikawa, Takaki and Brenner, Hermann and Chang-Claude, Jenny and Hoffmeister, Michael and Trautwein, Christian and Luedde, Tom},
	urldate = {2019-11-04},
	date = {2019-07},
	langid = {english},
	file = {Kather et al. - 2019 - Deep learning can predict microsatellite instabili.pdf:/Users/trislaz/Zotero/storage/RU7HU6UY/Kather et al. - 2019 - Deep learning can predict microsatellite instabili.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/ISAHU7R7/s41591-019-0462-y.html:text/html}
}

@article{madabhushi_image_2016,
	title = {Image analysis and machine learning in digital pathology: Challenges and opportunities},
	volume = {33},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841516301141},
	doi = {10.1016/j.media.2016.06.037},
	series = {20th anniversary of the Medical Image Analysis journal ({MedIA})},
	shorttitle = {Image analysis and machine learning in digital pathology},
	abstract = {With the rise in whole slide scanner technology, large numbers of tissue slides are being scanned and represented and archived digitally. While digital pathology has substantial implications for telepathology, second opinions, and education there are also huge research opportunities in image computing with this new source of “big data”. It is well known that there is fundamental prognostic data embedded in pathology images. The ability to mine “sub-visual” image features from digital pathology slide images, features that may not be visually discernible by a pathologist, offers the opportunity for better quantitative modeling of disease appearance and hence possibly improved prediction of disease aggressiveness and patient outcome. However the compelling opportunities in precision medicine offered by big digital pathology data come with their own set of computational challenges. Image analysis and computer assisted detection and diagnosis tools previously developed in the context of radiographic images are woefully inadequate to deal with the data density in high resolution digitized whole slide images. Additionally there has been recent substantial interest in combining and fusing radiologic imaging and proteomics and genomics based measurements with features extracted from digital pathology images for better prognostic prediction of disease aggressiveness and patient outcome. Again there is a paucity of powerful tools for combining disease specific features that manifest across multiple different length scales. The purpose of this review is to discuss developments in computational image analysis tools for predictive modeling of digital pathology images from a detection, segmentation, feature extraction, and tissue classification perspective. We discuss the emergence of new handcrafted feature approaches for improved predictive modeling of tissue appearance and also review the emergence of deep learning schemes for both object detection and tissue classification. We also briefly review some of the state of the art in fusion of radiology and pathology images and also combining digital pathology derived image measurements with molecular “omics” features for better predictive modeling. The review ends with a brief discussion of some of the technical and computational challenges to be overcome and reflects on future opportunities for the quantitation of histopathology.},
	pages = {170--175},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Madabhushi, Anant and Lee, George},
	urldate = {2019-11-04},
	date = {2016-10-01},
	langid = {english},
	file = {Version acceptée:/Users/trislaz/Zotero/storage/P4S8PN44/Madabhushi et Lee - 2016 - Image analysis and machine learning in digital pat.pdf:application/pdf;ScienceDirect Snapshot:/Users/trislaz/Zotero/storage/X8GKFQ3W/S1361841516301141.html:text/html}
}

@article{goodfellow_generative_nodate,
	title = {Generative Adversarial Nets},
	pages = {9},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	langid = {english},
	file = {Goodfellow et al. - Generative Adversarial Nets.pdf:/Users/trislaz/Zotero/storage/J2MIEM9W/Goodfellow et al. - Generative Adversarial Nets.pdf:application/pdf}
}

@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2019-06-14},
	date = {2014-12-22},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/XXU9YNV3/1412.html:text/html;arXiv\:1412.6980 PDF:/Users/trislaz/Zotero/storage/GWXFX67Z/Kingma et Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{combes_learning_2018,
	title = {On the Learning Dynamics of Deep Neural Networks},
	url = {http://arxiv.org/abs/1809.06848},
	abstract = {While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.},
	journaltitle = {{arXiv}:1809.06848 [cs, stat]},
	author = {Combes, Remi Tachet des and Pezeshki, Mohammad and Shabanian, Samira and Courville, Aaron and Bengio, Yoshua},
	urldate = {2019-06-13},
	date = {2018-09-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1809.06848},
	file = {Combes et al. - 2018 - On the Learning Dynamics of Deep Neural Networks.pdf:/Users/trislaz/Zotero/storage/TXQCI9VE/Combes et al. - 2018 - On the Learning Dynamics of Deep Neural Networks.pdf:application/pdf}
}

@article{chen_unsupervised_2019,
	title = {Unsupervised Object Segmentation by Redrawing},
	url = {http://arxiv.org/abs/1905.13539},
	abstract = {Object segmentation is a crucial problem that is usually solved by using supervised learning approaches over very large datasets composed of both images and corresponding object masks. Since the masks have to be provided at pixel level, building such a dataset for any new domain can be very costly. We present {ReDO}, a new model able to extract objects from images without any annotation in an unsupervised way. It relies on the idea that it should be possible to change the textures or colors of the objects without changing the overall distribution of the dataset. Following this assumption, our approach is based on an adversarial architecture where the generator is guided by an input sample: given an image, it extracts the object mask, then redraws a new object at the same location. The generator is controlled by a discriminator that ensures that the distribution of generated images is aligned to the original one. We experiment with this method on different datasets and demonstrate the good quality of extracted masks.},
	journaltitle = {{arXiv}:1905.13539 [cs, stat]},
	author = {Chen, Mickaël and Artières, Thierry and Denoyer, Ludovic},
	urldate = {2019-06-04},
	date = {2019-05-27},
	eprinttype = {arxiv},
	eprint = {1905.13539},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/V8MWWD89/1905.html:text/html;arXiv\:1905.13539 PDF:/Users/trislaz/Zotero/storage/45EQ3QW2/Chen et al. - 2019 - Unsupervised Object Segmentation by Redrawing.pdf:application/pdf}
}

@article{de_bezenac_optimal_2019,
	title = {Optimal Unsupervised Domain Translation},
	url = {https://arxiv.org/abs/1906.01292v1},
	abstract = {Domain Translation is the problem of finding a meaningful correspondence
between two domains. Since in a majority of settings paired supervision is not
available, much work focuses on Unsupervised Domain Translation ({UDT}) where
data samples from each domain are unpaired. Following the seminal work of
{CycleGAN} for {UDT}, many variants and extensions of this model have been
proposed. However, there is still little theoretical understanding behind their
success. We observe that these methods yield solutions which are approximately
minimal w.r.t. a given transportation cost, leading us to reformulate the
problem in the Optimal Transport ({OT}) framework. This viewpoint gives us a new
perspective on Unsupervised Domain Translation and allows us to prove the
existence and uniqueness of the retrieved mapping, given a large family of
transport costs. We then propose a novel framework to efficiently compute
optimal mappings in a dynamical setting. We show that it generalizes previous
methods and enables a more explicit control over the computed optimal mapping.
It also provides smooth interpolations between the two domains. Experiments on
toy and real world datasets illustrate the behavior of our method.},
	author = {de Bézenac, Emmanuel and Ayed, Ibrahim and Gallinari, Patrick},
	urldate = {2019-06-04},
	date = {2019-06-04},
	langid = {english}
}

@article{grathwohl_ffjord:_2018,
	title = {{FFJORD}: Free-form Continuous Dynamics for Scalable Reversible Generative Models},
	url = {http://arxiv.org/abs/1810.01367},
	shorttitle = {{FFJORD}},
	abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is speciﬁed by an ordinary differential equation. In this paper, we use Hutchinson’s trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efﬁcient sampling.},
	journaltitle = {{arXiv}:1810.01367 [cs, stat]},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	urldate = {2019-05-20},
	date = {2018-10-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.01367},
	file = {Grathwohl et al. - 2018 - FFJORD Free-form Continuous Dynamics for Scalable.pdf:/Users/trislaz/Zotero/storage/ARHJ3Z7D/Grathwohl et al. - 2018 - FFJORD Free-form Continuous Dynamics for Scalable.pdf:application/pdf}
}

@article{antognini_pca_2018,
	title = {{PCA} of high dimensional random walks with comparison to neural network training},
	url = {http://arxiv.org/abs/1806.08805},
	abstract = {One technique to visualize the training of neural networks is to perform {PCA} on the parameters over the course of training and to project to the subspace spanned by the ﬁrst few {PCA} components. In this paper we compare this technique to the {PCA} of a high dimensional random walk. We compute the eigenvalues and eigenvectors of the covariance of the trajectory and prove that in the long trajectory and high dimensional limit most of the variance is in the ﬁrst few {PCA} components, and that the projection of the trajectory onto any subspace spanned by {PCA} components is a Lissajous curve. We generalize these results to a random walk with momentum and to an Ornstein-Uhlenbeck processes (i.e., a random walk in a quadratic potential) and show that in high dimensions the walk is not mean reverting, but will instead be trapped at a ﬁxed distance from the minimum. We ﬁnally compare the distribution of {PCA} variances and the {PCA} projected training trajectories of a linear model trained on {CIFAR}-10 and {ResNet}-50-v2 trained on Imagenet and ﬁnd that the distribution of {PCA} variances resembles a random walk with drift.},
	journaltitle = {{arXiv}:1806.08805 [cs, stat]},
	author = {Antognini, Joseph M. and Sohl-Dickstein, Jascha},
	urldate = {2019-04-21},
	date = {2018-06-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.08805},
	file = {Antognini et Sohl-Dickstein - 2018 - PCA of high dimensional random walks with comparis.pdf:/Users/trislaz/Zotero/storage/NM7BNHDJ/Antognini et Sohl-Dickstein - 2018 - PCA of high dimensional random walks with comparis.pdf:application/pdf}
}

@article{frankle_lottery_2018,
	title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
	url = {http://arxiv.org/abs/1803.03635},
	shorttitle = {The Lottery Ticket Hypothesis},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for {MNIST} and {CIFAR}10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	journaltitle = {{arXiv}:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	urldate = {2019-04-16},
	date = {2018-03-09},
	eprinttype = {arxiv},
	eprint = {1803.03635},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/83XG2W38/1803.html:text/html;arXiv\:1803.03635 PDF:/Users/trislaz/Zotero/storage/MW8BYQ8G/Frankle et Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf}
}

@article{real_large-scale_2017,
	title = {Large-Scale Evolution of Image Classifiers},
	url = {http://arxiv.org/abs/1703.01041},
	abstract = {Neural networks have proven effective at solving difﬁcult problems but designing their architectures can be challenging, even for image classiﬁcation problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite signiﬁcant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Speciﬁcally, we employ simple evolutionary techniques at unprecedented scales to discover models for the {CIFAR}-10 and {CIFAR}-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
	journaltitle = {{arXiv}:1703.01041 [cs]},
	author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alex},
	urldate = {2019-04-16},
	date = {2017-03-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.01041},
	file = {Real et al. - 2017 - Large-Scale Evolution of Image Classifiers.pdf:/Users/trislaz/Zotero/storage/JQA77VVF/Real et al. - 2017 - Large-Scale Evolution of Image Classifiers.pdf:application/pdf}
}

@article{blier_learning_2018,
	title = {Learning with Random Learning Rates},
	url = {http://arxiv.org/abs/1810.01322},
	abstract = {In neural networks, the learning rate of the gradient descent strongly affects performance. This prevents reliable out-of-the-box training of a model on a new problem. We propose the All Learning Rates At Once (Alrao) algorithm: each unit or feature in the network gets its own learning rate sampled from a random distribution spanning several orders of magnitude, in the hope that enough units will get a close-to-optimal learning rate. Perhaps surprisingly, stochastic gradient descent ({SGD}) with Alrao performs close to {SGD} with an optimally tuned learning rate, for various network architectures and problems. In our experiments, all Alrao runs were able to learn well without any tuning.},
	journaltitle = {{arXiv}:1810.01322 [cs, stat]},
	author = {Blier, Léonard and Wolinski, Pierre and Ollivier, Yann},
	urldate = {2019-04-16},
	date = {2018-10-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.01322},
	file = {Blier et al. - 2018 - Learning with Random Learning Rates.pdf:/Users/trislaz/Zotero/storage/Y84J3S9M/Blier et al. - 2018 - Learning with Random Learning Rates.pdf:application/pdf}
}

@article{arora_provable_2013,
	title = {Provable Bounds for Learning Some Deep Representations},
	url = {http://arxiv.org/abs/1310.6343},
	abstract = {We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an \$n\$ node multilayer neural net that has degree at most \$n{\textasciicircum}\{{\textbackslash}gamma\}\$ for some \${\textbackslash}gamma {\textless}1\$ and each edge has a random edge weight in \$[-1,1]\$. Our algorithm learns \{{\textbackslash}em almost all\} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.},
	journaltitle = {{arXiv}:1310.6343 [cs, stat]},
	author = {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},
	urldate = {2019-02-14},
	date = {2013-10-23},
	eprinttype = {arxiv},
	eprint = {1310.6343},
	file = {arXiv.org Snapshot:/Users/trislaz/Zotero/storage/QERFI4XC/1310.html:text/html;arXiv\:1310.6343 PDF:/Users/trislaz/Zotero/storage/J7BNG4UA/Arora et al. - 2013 - Provable Bounds for Learning Some Deep Representat.pdf:application/pdf}
}

@article{lin_network_2013,
	title = {Network In Network},
	url = {http://arxiv.org/abs/1312.4400},
	abstract = {We propose a novel deep network structure called “Network In Network”({NIN}) to enhance model discriminability for local patches within the receptive ﬁeld. The conventional convolutional layer uses linear ﬁlters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive ﬁeld. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as {CNN}; they are then fed into the next layer. Deep {NIN} can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classiﬁcation layer, which is easier to interpret and less prone to overﬁtting than traditional fully connected layers. We demonstrated the state-of-the-art classiﬁcation performances with {NIN} on {CIFAR}-10 and {CIFAR}-100, and reasonable performances on {SVHN} and {MNIST} datasets.},
	journaltitle = {{arXiv}:1312.4400 [cs]},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	urldate = {2019-02-14},
	date = {2013-12-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1312.4400},
	file = {Lin et al. - 2013 - Network In Network.pdf:/Users/trislaz/Zotero/storage/SRHDKV9C/Lin et al. - 2013 - Network In Network.pdf:application/pdf}
}

@article{cheplygina_not-so-supervised_2019-1,
	title = {Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis},
	volume = {54},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518307588},
	doi = {10.1016/j.media.2019.03.009},
	shorttitle = {Not-so-supervised},
	abstract = {Machine learning ({ML}) algorithms have made a tremendous impact in the ﬁeld of medical imaging. While medical imaging datasets have been growing in size, a challenge for supervised {ML} algorithms that is frequently mentioned is the lack of annotated data. As a result, various methods that can learn with less/other types of supervision, have been proposed. We give an overview of semi-supervised, multiple instance, and transfer learning in medical imaging, both in diagnosis or segmentation tasks. We also discuss connections between these learning scenarios, and opportunities for future research. A dataset with the details of the surveyed papers is available via https://ﬁgshare.com/articles/Database\_of\_surveyed\_literature\_in\_Not-so-supervised\_a\_survey\_of\_ semi- supervised\_multi- instance\_and\_transfer\_learning\_in\_medical\_image\_analysis\_/7479416.},
	pages = {280--296},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Cheplygina, Veronika and de Bruijne, Marleen and Pluim, Josien P.W.},
	urldate = {2020-03-18},
	date = {2019-05},
	langid = {english},
	file = {cheplygina2019.pdf:/Users/trislaz/Documents/cbio/papiers/cheplygina2019.pdf:application/pdf}
}

@article{mollersen_bag--class_2018,
	title = {A bag-to-class divergence approach to multiple-instance learning},
	url = {http://arxiv.org/abs/1803.02782},
	abstract = {In multi-instance ({MI}) learning, each object (bag) consists of multiple feature vectors (instances), and is most commonly regarded as a set of points in a multidimensional space. A different viewpoint is that the instances are realisations of random vectors with corresponding probability distribution, and that a bag is the distribution, not the realisations. In {MI} classification, each bag in the training set has a class label, but the instances are unlabelled. By introducing the probability distribution space to bag-level classification problems, dissimilarities between probability distributions (divergences) can be applied. The bag-to-bag Kullback-Leibler information is asymptotically the best classifier, but the typical sparseness of {MI} training sets is an obstacle. We introduce bag-to-class divergence to {MI} learning, emphasising the hierarchical nature of the random vectors that makes bags from the same class different. We propose two properties for bag-to-class divergences, and an additional property for sparse training sets.},
	journaltitle = {{arXiv}:1803.02782 [cs, stat]},
	author = {Møllersen, Kajsa and Hardeberg, Jon Yngve and Godtliebsen, Fred},
	urldate = {2020-03-19},
	date = {2018-10-12},
	eprinttype = {arxiv},
	eprint = {1803.02782},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/NW5J7DPN/Møllersen et al. - 2018 - A bag-to-class divergence approach to multiple-ins.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/QF5UMEIC/1803.html:text/html}
}

@article{reiter_deep_2020,
	title = {Deep Multi-Modal Sets},
	url = {http://arxiv.org/abs/2003.01607},
	abstract = {Many vision-related tasks benefit from reasoning over multiple modalities to leverage complementary views of data in an attempt to learn robust embedding spaces. Most deep learning-based methods rely on a late fusion technique whereby multiple feature types are encoded and concatenated and then a multi layer perceptron ({MLP}) combines the fused embedding to make predictions. This has several limitations, such as an unnatural enforcement that all features be present at all times as well as constraining only a constant number of occurrences of a feature modality at any given time. Furthermore, as more modalities are added, the concatenated embedding grows. To mitigate this, we propose Deep Multi-Modal Sets: a technique that represents a collection of features as an unordered set rather than one long ever-growing fixed-size vector. The set is constructed so that we have invariance both to permutations of the feature modalities as well as to the cardinality of the set. We will also show that with particular choices in our model architecture, we can yield interpretable feature performance such that during inference time we can observe which modalities are most contributing to the prediction.With this in mind, we demonstrate a scalable, multi-modal framework that reasons over different modalities to learn various types of tasks. We demonstrate new state-of-the-art performance on two multi-modal datasets (Ads-Parallelity [34] and {MM}-{IMDb} [1]).},
	journaltitle = {{arXiv}:2003.01607 [cs]},
	author = {Reiter, Austin and Jia, Menglin and Yang, Pu and Lim, Ser-Nam},
	urldate = {2020-03-19},
	date = {2020-03-03},
	eprinttype = {arxiv},
	eprint = {2003.01607},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/RZ376NW4/Reiter et al. - 2020 - Deep Multi-Modal Sets.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/R8ZAHCRT/2003.html:text/html}
}

@article{zaheer_deep_2018,
	title = {Deep Sets},
	url = {http://arxiv.org/abs/1703.06114},
	abstract = {We study the problem of designing models for machine learning tasks defined on {\textbackslash}emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics {\textbackslash}cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams {\textbackslash}cite\{Jung15Exploration\}, to cosmology {\textbackslash}cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
	journaltitle = {{arXiv}:1703.06114 [cs, stat]},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
	urldate = {2020-03-19},
	date = {2018-04-14},
	eprinttype = {arxiv},
	eprint = {1703.06114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/TG5TSE5R/Zaheer et al. - 2018 - Deep Sets.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/K7866NC3/1703.html:text/html}
}

@article{zanjani_histopathology_nodate,
	title = {Histopathology Stain-Color Normalization Using Deep Generative Models},
	abstract = {Performance of designed {CAD} algorithms for histopathology image analysis is affected by the amount of variations in the samples such as color and intensity of stained images. Stain-color normalization is a well-studied technique for compensating such effects at the input of {CAD} systems. In this paper, we introduce unsupervised generative neural networks for performing stain-color normalization. For color normalization in stained hematoxylin and eosin (H\&E) images, we present three methods based on three frameworks for deep generative models: variational auto-encoder ({VAE}), generative adversarial networks ({GAN}) and deep convolutional Gaussian mixture models ({DCGMM}). Our contribution is deﬁning the color normalization as a learning generative model that is able to generate various color copies of the input image through a nonlinear parametric transformation. In contrast to earlier generative models proposed for stain-color normalization, our approach does not need any labels for data or any other assumptions about the H\&E image content. Furthermore, our models learn a parametric transformation during training and can convert the color information of an input image to resemble any arbitrary reference image. This property is essential in time-critical {CAD} systems in case of changing the reference image, since our approach does not need retraining in contrast to other proposed generative models for stain-color normalization. Experiments on histopathological H\&E images with high staining variations, collected from different laboratories, show that our proposed models outperform quantitatively state-of-the-art methods in the measure of color constancy with at least 10-15\%, while the converted images are visually in agreement with this performance improvement.},
	pages = {11},
	author = {Zanjani, Farhad Ghazvinian and Zinger, Svitlana and Bejnordi, Babak E},
	langid = {english},
	file = {Zanjani et al. - Histopathology Stain-Color Normalization Using Dee.pdf:/Users/trislaz/Zotero/storage/Q48YTV7G/Zanjani et al. - Histopathology Stain-Color Normalization Using Dee.pdf:application/pdf}
}

@article{zanjani_histopathology_nodate-1,
	title = {Histopathology Stain-Color Normalization Using Deep Generative Models},
	abstract = {Performance of designed {CAD} algorithms for histopathology image analysis is affected by the amount of variations in the samples such as color and intensity of stained images. Stain-color normalization is a well-studied technique for compensating such effects at the input of {CAD} systems. In this paper, we introduce unsupervised generative neural networks for performing stain-color normalization. For color normalization in stained hematoxylin and eosin (H\&E) images, we present three methods based on three frameworks for deep generative models: variational auto-encoder ({VAE}), generative adversarial networks ({GAN}) and deep convolutional Gaussian mixture models ({DCGMM}). Our contribution is deﬁning the color normalization as a learning generative model that is able to generate various color copies of the input image through a nonlinear parametric transformation. In contrast to earlier generative models proposed for stain-color normalization, our approach does not need any labels for data or any other assumptions about the H\&E image content. Furthermore, our models learn a parametric transformation during training and can convert the color information of an input image to resemble any arbitrary reference image. This property is essential in time-critical {CAD} systems in case of changing the reference image, since our approach does not need retraining in contrast to other proposed generative models for stain-color normalization. Experiments on histopathological H\&E images with high staining variations, collected from different laboratories, show that our proposed models outperform quantitatively state-of-the-art methods in the measure of color constancy with at least 10-15\%, while the converted images are visually in agreement with this performance improvement.},
	pages = {11},
	author = {Zanjani, Farhad Ghazvinian and Zinger, Svitlana and Bejnordi, Babak E},
	langid = {english},
	file = {Zanjani et al. - Histopathology Stain-Color Normalization Using Dee.pdf:/Users/trislaz/Zotero/storage/HHF272W5/Zanjani et al. - Histopathology Stain-Color Normalization Using Dee.pdf:application/pdf}
}

@article{zanjani_histopathology_2018,
	title = {Histopathology Stain-Color Normalization Using Deep Generative Models},
	url = {https://openreview.net/forum?id=SkjdxkhoG},
	abstract = {Performance of designed {CAD} algorithms for histopathology image analysis is affected by the amount of variations in the samples such as color and intensity of stained images. Stain-color...},
	author = {Zanjani, Farhad G. and Zinger, Svitlana and Bejnordi, Babak E. and Laak, Jeroen {AWM} van der and With, Peter H. N. de},
	urldate = {2020-03-20},
	date = {2018-04-11},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/CQKRHGBR/Zanjani et al. - 2018 - Histopathology Stain-Color Normalization Using Dee.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/J4MV8QFH/forum.html:text/html}
}

@article{sannai_universal_2019,
	title = {Universal approximations of permutation invariant/equivariant functions by deep neural networks},
	url = {http://arxiv.org/abs/1903.01939},
	abstract = {In this paper, we develop a theory about the relationship between \$G\$-invariant/equivariant functions and deep neural networks for finite group \$G\$. Especially, for a given \$G\$-invariant/equivariant function, we construct its universal approximator by deep neural network whose layers equip \$G\$-actions and each affine transformations are \$G\$-equivariant/invariant. Due to representation theory, we can show that this approximator has exponentially fewer free parameters than usual models.},
	journaltitle = {{arXiv}:1903.01939 [cs, stat]},
	author = {Sannai, Akiyoshi and Takai, Yuuki and Cordonnier, Matthieu},
	urldate = {2020-03-20},
	date = {2019-09-26},
	eprinttype = {arxiv},
	eprint = {1903.01939},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/7ACWA2TB/Sannai et al. - 2019 - Universal approximations of permutation invariant.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/ISPHCDRT/1903.html:text/html}
}

@article{ravanbakhsh_universal_2020,
	title = {Universal Equivariant Multilayer Perceptrons},
	url = {http://arxiv.org/abs/2002.02912},
	abstract = {Group invariant and equivariant Multilayer Perceptrons ({MLP}), also known as Equivariant Networks, have achieved remarkable success in learning on a variety of data structures, such as sequences, images, sets, and graphs. Using tools from group theory, this paper proves the universality of a broad class of equivariant {MLPs} with a single hidden layer. In particular, it is shown that having a hidden layer on which the group acts regularly is sufficient for universal equivariance. Next, Burnside's table of marks is used to decompose product spaces. It is shown that the product of two G-sets always contains an orbit larger than the input orbits. Therefore high order hidden layers inevitably contain a regular orbit, leading to the universality of the corresponding {MLP}. It is shown that with an order larger than the logarithm of the size of the stabilizer group, a high-order equivariant {MLP} is equivariant universal.},
	journaltitle = {{arXiv}:2002.02912 [cs, math, stat]},
	author = {Ravanbakhsh, Siamak},
	urldate = {2020-03-20},
	date = {2020-02-07},
	eprinttype = {arxiv},
	eprint = {2002.02912},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Group Theory},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/ZEZ4DSXV/Ravanbakhsh - 2020 - Universal Equivariant Multilayer Perceptrons.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/QRQTH7GF/2002.html:text/html}
}

@article{katharopoulos_processing_2019-1,
	title = {Processing Megapixel Images with Deep Attention-Sampling Models},
	url = {http://arxiv.org/abs/1905.03711},
	abstract = {Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single {GPU} setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal {SGD} procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.},
	journaltitle = {{arXiv}:1905.03711 [cs, stat]},
	author = {Katharopoulos, Angelos and Fleuret, François},
	urldate = {2020-03-30},
	date = {2019-07-17},
	eprinttype = {arxiv},
	eprint = {1905.03711},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/69VN3KDE/Katharopoulos et Fleuret - 2019 - Processing Megapixel Images with Deep Attention-Sa.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/MGLV3P5U/1905.html:text/html}
}

@article{campanella_clinical-grade_2019-1,
	title = {Clinical-grade computational pathology using weakly supervised deep learning on whole slide images},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-019-0508-1},
	doi = {10.1038/s41591-019-0508-1},
	pages = {1301--1309},
	number = {8},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and Werneck Krauss Silva, Vitor and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
	urldate = {2020-03-31},
	date = {2019-08},
	langid = {english},
	keywords = {toread},
	file = {campanella2019.pdf:/Users/trislaz/Documents/cbio/papiers/campanella2019.pdf:application/pdf}
}

@inproceedings{hou_patch-based_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780635/},
	doi = {10.1109/CVPR.2016.266},
	abstract = {Convolutional Neural Networks ({CNN}) are state-of-theart models for many image classiﬁcation tasks. However, to recognize cancer subtypes automatically, training a {CNN} on gigapixel resolution Whole Slide Tissue Images ({WSI}) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classiﬁer on image patches will perform better than or similar to an image-level classiﬁer. The challenge becomes how to intelligently combine patch-level classiﬁcation results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level {CNNs}, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization ({EM}) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classiﬁcation of glioma and non-small-cell lung carcinoma cases into subtypes. The classiﬁcation accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train {CNNs} on {WSIs}, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based {CNN} can outperform an image-based {CNN}.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2424--2433},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Hou, Le and Samaras, Dimitris and Kurc, Tahsin M. and Gao, Yi and Davis, James E. and Saltz, Joel H.},
	urldate = {2020-04-01},
	date = {2016-06},
	langid = {english},
	keywords = {toread},
	file = {Hou et al. - 2016 - Patch-Based Convolutional Neural Network for Whole.pdf:/Users/trislaz/Zotero/storage/KVVGAPFJ/Hou et al. - 2016 - Patch-Based Convolutional Neural Network for Whole.pdf:application/pdf}
}

@article{wang_deep_2016,
	title = {Deep Learning for Identifying Metastatic Breast Cancer},
	url = {http://arxiv.org/abs/1606.05718},
	abstract = {The International Symposium on Biomedical Imaging ({ISBI}) held a grand challenge to evaluate computational systems for the automated detection of metastatic breast cancer in whole slide images of sentinel lymph node biopsies. Our team won both competitions in the grand challenge, obtaining an area under the receiver operating curve ({AUC}) of 0.925 for the task of whole slide image classiﬁcation and a score of 0.7051 for the tumor localization task. A pathologist independently reviewed the same images, obtaining a whole slide image classiﬁcation {AUC} of 0.966 and a tumor localization score of 0.733. Combining our deep learning system’s predictions with the human pathologist’s diagnoses increased the pathologist’s {AUC} to 0.995, representing an approximately 85 percent reduction in human error rate. These results demonstrate the power of using deep learning to produce signiﬁcant improvements in the accuracy of pathological diagnoses.},
	journaltitle = {{arXiv}:1606.05718 [cs, q-bio]},
	author = {Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Beck, Andrew H.},
	urldate = {2020-04-01},
	date = {2016-06-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.05718},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods},
	file = {Wang et al. - 2016 - Deep Learning for Identifying Metastatic Breast Ca.pdf:/Users/trislaz/Zotero/storage/8HNH4R4Q/Wang et al. - 2016 - Deep Learning for Identifying Metastatic Breast Ca.pdf:application/pdf}
}

@article{lu_semi-supervised_2019,
	title = {Semi-Supervised Histology Classification using Deep Multiple Instance Learning and Contrastive Predictive Coding},
	url = {http://arxiv.org/abs/1910.10825},
	abstract = {Convolutional neural networks can be trained to perform histology slide classification using weak annotations with multiple instance learning ({MIL}). However, given the paucity of labeled histology data, direct application of {MIL} can easily suffer from overfitting and the network is unable to learn rich feature representations due to the weak supervisory signal. We propose to overcome such limitations with a two-stage semi-supervised approach that combines the power of data-efficient self-supervised feature learning via contrastive predictive coding ({CPC}) and the interpretability and flexibility of regularized attention-based {MIL}. We apply our two-stage {CPC} + {MIL} semi-supervised pipeline to the binary classification of breast cancer histology images. Across five random splits, we report state-of-the-art performance with a mean validation accuracy of 95\% and an area under the {ROC} curve of 0.968. We further evaluate the quality of features learned via {CPC} relative to simple transfer learning and show that strong classification performance using {CPC} features can be efficiently leveraged under the {MIL} framework even with the feature encoder frozen.},
	journaltitle = {{arXiv}:1910.10825 [cs, q-bio]},
	author = {Lu, Ming Y. and Chen, Richard J. and Wang, Jingwen and Dillon, Debora and Mahmood, Faisal},
	urldate = {2020-04-01},
	date = {2019-11-02},
	eprinttype = {arxiv},
	eprint = {1910.10825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Tissues and Organs},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/9R24G344/Lu et al. - 2019 - Semi-Supervised Histology Classification using Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/GMHCWJ9I/1910.html:text/html}
}

@article{wang_weakly_2019,
	title = {Weakly Supervised Prostate {TMA} Classification via Graph Convolutional Networks},
	url = {http://arxiv.org/abs/1910.13328},
	abstract = {Histology-based grade classification is clinically important for many cancer types in stratifying patients distinct treatment groups. In prostate cancer, the Gleason score is a grading system used to measure the aggressiveness of prostate cancer from the spatial organization of cells and the distribution of glands. However, the subjective interpretation of Gleason score often suffers from large interobserver and intraobserver variability. Previous work in deep learning-based objective Gleason grading requires manual pixel-level annotation. In this work, we propose a weakly-supervised approach for grade classification in tissue micro-arrays ({TMA}) using graph convolutional networks ({GCNs}), in which we model the spatial organization of cells as a graph to better capture the proliferation and community structure of tumor cells. As node-level features in our graph representation, we learn the morphometry of each cell using a contrastive predictive coding ({CPC})-based self-supervised approach. We demonstrate that on a five-fold cross validation our method can achieve \$0.9659{\textbackslash}pm0.0096\$ {AUC} using only {TMA}-level labels. Our method demonstrates a 39.80{\textbackslash}\% improvement over standard {GCNs} with texture features and a 29.27\% improvement over {GCNs} with {VGG}19 features. Our proposed pipeline can be used to objectively stratify low and high risk cases, reducing inter- and intra-observer variability and pathologist workload.},
	journaltitle = {{arXiv}:1910.13328 [cs, eess, q-bio]},
	author = {Wang, Jingwen and Chen, Richard J. and Lu, Ming Y. and Baras, Alexander and Mahmood, Faisal},
	urldate = {2020-04-01},
	date = {2019-11-06},
	eprinttype = {arxiv},
	eprint = {1910.13328},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Tissues and Organs, Electrical Engineering and Systems Science - Image and Video Processing, toshow},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/72X5UEI8/Wang et al. - 2019 - Weakly Supervised Prostate TMA Classification via .pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/W8DXYCLQ/1910.html:text/html}
}

@article{liu_detecting_2017,
	title = {Detecting Cancer Metastases on Gigapixel Pathology Images},
	url = {http://arxiv.org/abs/1703.02442},
	abstract = {Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network ({CNN}) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4\% of the tumors, relative to 82.7\% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2\% sensitivity. We achieve image-level {AUC} scores above 97\% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.},
	journaltitle = {{arXiv}:1703.02442 [cs]},
	author = {Liu, Yun and Gadepalli, Krishna and Norouzi, Mohammad and Dahl, George E. and Kohlberger, Timo and Boyko, Aleksey and Venugopalan, Subhashini and Timofeev, Aleksei and Nelson, Philip Q. and Corrado, Greg S. and Hipp, Jason D. and Peng, Lily and Stumpe, Martin C.},
	urldate = {2020-04-01},
	date = {2017-03-07},
	eprinttype = {arxiv},
	eprint = {1703.02442},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/NDLSYVZV/Liu et al. - 2017 - Detecting Cancer Metastases on Gigapixel Pathology.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/S94R3P4V/1703.html:text/html}
}

@article{liu_detecting_2017-1,
	title = {Detecting Cancer Metastases on Gigapixel Pathology Images},
	url = {http://arxiv.org/abs/1703.02442},
	abstract = {Each year, the treatment decisions for more than 230, 000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 × 100 pixels in gigapixel microscopy images sized 100, 000×100, 000 pixels. Our method leverages a convolutional neural network ({CNN}) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4\% of the tumors, relative to 82.7\% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2\% sensitivity. We achieve image-level {AUC} scores above 97\% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.},
	journaltitle = {{arXiv}:1703.02442 [cs]},
	author = {Liu, Yun and Gadepalli, Krishna and Norouzi, Mohammad and Dahl, George E. and Kohlberger, Timo and Boyko, Aleksey and Venugopalan, Subhashini and Timofeev, Aleksei and Nelson, Philip Q. and Corrado, Greg S. and Hipp, Jason D. and Peng, Lily and Stumpe, Martin C.},
	urldate = {2020-04-01},
	date = {2017-03-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.02442},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, toread},
	file = {Liu et al. - 2017 - Detecting Cancer Metastases on Gigapixel Pathology.pdf:/Users/trislaz/Zotero/storage/T6DLNKFX/Liu et al. - 2017 - Detecting Cancer Metastases on Gigapixel Pathology.pdf:application/pdf}
}

@article{gadiya_histographs_2019,
	title = {Histographs: Graphs in Histopathology},
	url = {http://arxiv.org/abs/1908.05020},
	shorttitle = {Histographs},
	abstract = {Spatial arrangement of cells of various types, such as tumor infiltrating lymphocytes and the advancing edge of a tumor, are important features for detecting and characterizing cancers. However, convolutional neural networks ({CNNs}) do not explicitly extract intricate features of the spatial arrangements of the cells from histopathology images. In this work, we propose to classify cancers using graph convolutional networks ({GCNs}) by modeling a tissue section as a multi-attributed spatial graph of its constituent cells. Cells are detected using their nuclei in H\&E stained tissue image, and each cell's appearance is captured as a multi-attributed high-dimensional vertex feature. The spatial relations between neighboring cells are captured as edge features based on their distances in a graph. We demonstrate the utility of this approach by obtaining classification accuracy that is competitive with {CNNs}, specifically, Inception-v3, on two tasks-cancerous versus non-cancerous and in situ versus invasive-on the {BACH} breast cancer dataset.},
	journaltitle = {{arXiv}:1908.05020 [cs, eess]},
	author = {Gadiya, Shrey and Anand, Deepak and Sethi, Amit},
	urldate = {2020-04-01},
	date = {2019-08-14},
	eprinttype = {arxiv},
	eprint = {1908.05020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/AFX2WA4M/Gadiya et al. - 2019 - Histographs Graphs in Histopathology.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/6FSXS3FB/1908.html:text/html}
}

@article{oord_representation_2019,
	title = {Representation Learning with Contrastive Predictive Coding},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	journaltitle = {{arXiv}:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	urldate = {2020-04-02},
	date = {2019-01-22},
	eprinttype = {arxiv},
	eprint = {1807.03748},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/EGBFP3TX/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/5J7Y3I3Y/1807.html:text/html}
}

@article{garcia-martin_estimation_2019,
	title = {Estimation of energy consumption in machine learning},
	volume = {134},
	issn = {0743-7315},
	url = {http://www.sciencedirect.com/science/article/pii/S0743731518308773},
	doi = {10.1016/j.jpdc.2019.07.007},
	abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning.},
	pages = {75--88},
	journaltitle = {Journal of Parallel and Distributed Computing},
	shortjournal = {Journal of Parallel and Distributed Computing},
	author = {García-Martín, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Håkan},
	urldate = {2020-04-03},
	date = {2019-12-01},
	langid = {english},
	keywords = {Deep learning, Energy consumption, {GreenAI}, High performance computing, Machine learning},
	file = {ScienceDirect Snapshot:/Users/trislaz/Zotero/storage/V9LLNHSU/S0743731518308773.html:text/html;ScienceDirect Full Text PDF:/Users/trislaz/Zotero/storage/3WX2MXNN/García-Martín et al. - 2019 - Estimation of energy consumption in machine learni.pdf:application/pdf}
}

@article{musgrave_metric_2020,
	title = {A Metric Learning Reality Check},
	url = {http://arxiv.org/abs/2003.08505},
	abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental setup of these papers, and propose a new way to evaluate metric learning algorithms. Finally, we present experimental results that show that the improvements over time have been marginal at best.},
	journaltitle = {{arXiv}:2003.08505 [cs]},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	urldate = {2020-04-09},
	date = {2020-03-18},
	eprinttype = {arxiv},
	eprint = {2003.08505},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/KGWR7UR8/Musgrave et al. - 2020 - A Metric Learning Reality Check.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/VW7AS8N9/2003.html:text/html}
}

@article{tellez_extending_2020,
	title = {Extending Unsupervised Neural Image Compression With Supervised Multitask Learning},
	url = {http://arxiv.org/abs/2004.07041},
	abstract = {We focus on the problem of training convolutional neural networks on gigapixel histopathology images to predict image-level targets. For this purpose, we extend Neural Image Compression ({NIC}), an image compression framework that reduces the dimensionality of these images using an encoder network trained unsupervisedly. We propose to train this encoder using supervised multitask learning ({MTL}) instead. We applied the proposed {MTL} {NIC} to two histopathology datasets and three tasks. First, we obtained state-of-the-art results in the Tumor Proliferation Assessment Challenge of 2016 ({TUPAC}16). Second, we successfully classiﬁed histopathological growth patterns in images with colorectal liver metastasis ({CLM}). Third, we predicted patient risk of death by learning directly from overall survival in the same {CLM} data. Our experimental results suggest that the representations learned by the {MTL} objective are: (1) highly speciﬁc, due to the supervised training signal, and (2) transferable, since the same features perform well across diﬀerent tasks. Additionally, we trained multiple encoders with diﬀerent training objectives, e.g. unsupervised and variants of {MTL}, and observed a positive correlation between the number of tasks in {MTL} and the system performance on the {TUPAC}16 dataset.},
	journaltitle = {{arXiv}:2004.07041 [cs, eess]},
	author = {Tellez, David and Hoppener, Diederik and Verhoef, Cornelis and Grunhagen, Dirk and Nierop, Pieter and Drozdzal, Michal and van der Laak, Jeroen and Ciompi, Francesco},
	urldate = {2020-04-19},
	date = {2020-04-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.07041},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, toread, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Tellez et al. - 2020 - Extending Unsupervised Neural Image Compression Wi.pdf:/Users/trislaz/Zotero/storage/CIYAEEIE/Tellez et al. - 2020 - Extending Unsupervised Neural Image Compression Wi.pdf:application/pdf}
}

@article{tellez_neural_2019,
	title = {Neural Image Compression for Gigapixel Histopathology Image Analysis},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1811.02840},
	doi = {10.1109/TPAMI.2019.2936841},
	abstract = {We propose Neural Image Compression ({NIC}), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels. First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise. Second, a convolutional neural network ({CNN}) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations. We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated {NIC} on a synthetic task and two public histopathology datasets. We found that {NIC} can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information. Furthermore, we visualized the regions of the input gigapixel images where the {CNN} attended to, and confirmed that they overlapped with annotations from human experts.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Tellez, David and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
	urldate = {2020-04-22},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1811.02840},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, toread, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/CEKMPP6F/Tellez et al. - 2019 - Neural Image Compression for Gigapixel Histopathol.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/8R8Y48PA/1811.html:text/html}
}

@article{pinckaers_streaming_2019,
	title = {Streaming convolutional neural networks for end-to-end learning with multi-megapixel images},
	url = {http://arxiv.org/abs/1911.04432},
	abstract = {Due to memory constraints on current hardware, most convolution neural networks ({CNN}) are trained on sub-megapixel images. For example, most popular datasets in computer vision contain images much less than a megapixel in size (0.09MP for {ImageNet} and 0.001MP for {CIFAR}-10). In some domains such as medical imaging, multi-megapixel images are needed to identify the presence of disease accurately. We propose a novel method to directly train convolutional neural networks using any input image size end-to-end. This method exploits the locality of most operations in modern convolutional neural networks by performing the forward and backward pass on smaller tiles of the image. In this work, we show a proof of concept using images of up to 66-megapixels (8192x8192), saving approximately 50GB of memory per image. Using two public challenge datasets, we demonstrate that {CNNs} can learn to extract relevant information from these large images and benefit from increasing resolution. We improved the area under the receiver-operating characteristic curve from 0.580 (4MP) to 0.706 (66MP) for metastasis detection in breast cancer ({CAMELYON}17). We also obtained a Spearman correlation metric approaching state-of-the-art performance on the {TUPAC}16 dataset, from 0.485 (1MP) to 0.570 (16MP). Code to reproduce a subset of the experiments is available at https://github.com/{DIAGNijmegen}/{StreamingCNN}.},
	journaltitle = {{arXiv}:1911.04432 [cs]},
	author = {Pinckaers, Hans and van Ginneken, Bram and Litjens, Geert},
	urldate = {2020-04-22},
	date = {2019-11-11},
	eprinttype = {arxiv},
	eprint = {1911.04432},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, toread},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/BIQZMDIW/Pinckaers et al. - 2019 - Streaming convolutional neural networks for end-to.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/UJXALAJ5/1911.html:text/html}
}

@article{lu_data_2020,
	title = {Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images},
	url = {http://arxiv.org/abs/2004.09666},
	abstract = {The rapidly emerging field of computational pathology has the potential to enable objective diagnosis, therapeutic response prediction and identification of new morphological features of clinical relevance. However, deep learning-based computational pathology approaches either require manual annotation of gigapixel whole slide images ({WSIs}) in fully-supervised settings or thousands of {WSIs} with slide-level labels in a weakly-supervised setting. Moreover, whole slide level computational pathology methods also suffer from domain adaptation and interpretability issues. These challenges have prevented the broad adaptation of computational pathology for clinical and research purposes. Here we present {CLAM} - Clustering-constrained attention multiple instance learning, an easy-to-use, high-throughput, and interpretable {WSI}-level processing and learning method that only requires slide-level labels while being data efficient, adaptable and capable of handling multi-class subtyping problems. {CLAM} is a deep-learning-based weakly-supervised method that uses attention-based learning to automatically identify sub-regions of high diagnostic value in order to accurately classify the whole slide, while also utilizing instance-level clustering over the representative regions identified to constrain and refine the feature space. In three separate analyses, we demonstrate the data efficiency and adaptability of {CLAM} and its superior performance over standard weakly-supervised classification. We demonstrate that {CLAM} models are interpretable and can be used to identify well-known and new morphological features. We further show that models trained using {CLAM} are adaptable to independent test cohorts, cell phone microscopy images, and biopsies. {CLAM} is a general-purpose and adaptable method that can be used for a variety of different computational pathology tasks in both clinical and research settings.},
	journaltitle = {{arXiv}:2004.09666 [cs, eess, q-bio]},
	author = {Lu, Ming Y. and Williamson, Drew F. K. and Chen, Tiffany Y. and Chen, Richard J. and Barbieri, Matteo and Mahmood, Faisal},
	urldate = {2020-04-25},
	date = {2020-04-20},
	eprinttype = {arxiv},
	eprint = {2004.09666},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Tissues and Organs, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/Q9WYM9NN/Lu et al. - 2020 - Data Efficient and Weakly Supervised Computational.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/VW977PSZ/2004.html:text/html}
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-based Localization},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	shorttitle = {Grad-{CAM}},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of {CNN}-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-{CAM}), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-{CAM} is applicable to a wide variety of {CNN} model-families: (1) {CNNs} with fully-connected layers, (2) {CNNs} used for structured outputs, (3) {CNNs} used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-{CAM} with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering ({VQA}) models, including {ResNet}-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and {VQA}, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-{CAM} and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-{CAM} helps users establish appropriate trust in predictions from models and show that Grad-{CAM} helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/{COjUB}9Izk6E.},
	pages = {336--359},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2020-04-30},
	date = {2020-02},
	eprinttype = {arxiv},
	eprint = {1610.02391},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/5J37X4AA/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/3JVDP34T/1610.html:text/html}
}

@article{ilse_attention-based_2018,
	title = {Attention-based Deep Multiple Instance Learning},
	url = {http://arxiv.org/abs/1802.04712},
	abstract = {Multiple instance learning ({MIL}) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the {MIL} problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best {MIL} methods on benchmark {MIL} datasets and it outperforms other methods on a {MNIST}-based {MIL} dataset and two real-life histopathology datasets without sacrificing interpretability.},
	journaltitle = {{arXiv}:1802.04712 [cs, stat]},
	author = {Ilse, Maximilian and Tomczak, Jakub M. and Welling, Max},
	urldate = {2020-05-25},
	date = {2018-06-28},
	eprinttype = {arxiv},
	eprint = {1802.04712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/3G53WN9T/Ilse et al. - 2018 - Attention-based Deep Multiple Instance Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/LXBP6MKG/1802.html:text/html}
}

@article{rymarczyk_kernel_2020,
	title = {Kernel Self-Attention in Deep Multiple Instance Learning},
	url = {http://arxiv.org/abs/2005.12991},
	abstract = {Multiple Instance Learning ({MIL}) is weakly supervised learning, which assumes that there is only one label provided for the entire bag of instances. As such, it appears in many problems of medical image analysis, like the whole-slide images classiﬁcation of biopsy. Most recently, {MIL} was also applied to deep architectures by introducing the aggregation operator, which focuses on crucial instances of a bag. In this paper, we enrich this idea with the self-attention mechanism to take into account dependencies across the instances. We conduct several experiments and show that our method with various types of kernels increases the accuracy, especially in the case of non-standard {MIL} assumptions. This is of importance for real-word medical problems, which usually satisfy presence-based or threshold-based assumptions.},
	journaltitle = {{arXiv}:2005.12991 [cs, stat]},
	author = {Rymarczyk, Dawid and Tabor, Jacek and Zieliński, Bartosz},
	urldate = {2020-06-03},
	date = {2020-05-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.12991},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, toread},
	file = {Rymarczyk et al. - 2020 - Kernel Self-Attention in Deep Multiple Instance Le.pdf:/Users/trislaz/Zotero/storage/Y8352ZFN/Rymarczyk et al. - 2020 - Kernel Self-Attention in Deep Multiple Instance Le.pdf:application/pdf}
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2020-06-03},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Machine Learning, toread, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/NK389Q8M/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/2PNIEKLG/1706.html:text/html}
}

@article{ustinova_learning_2016,
	title = {Learning Deep Embeddings with Histogram Loss},
	url = {http://arxiv.org/abs/1611.00822},
	abstract = {We suggest a loss for learning deep embeddings. The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives.},
	journaltitle = {{arXiv}:1611.00822 [cs]},
	author = {Ustinova, Evgeniya and Lempitsky, Victor},
	urldate = {2020-06-05},
	date = {2016-11-02},
	eprinttype = {arxiv},
	eprint = {1611.00822},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/YXWSLQNY/Ustinova et Lempitsky - 2016 - Learning Deep Embeddings with Histogram Loss.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/V4M4AD8H/1611.html:text/html}
}

@article{avi-aharon_deephist_2020,
	title = {{DeepHist}: Differentiable Joint and Color Histogram Layers for Image-to-Image Translation},
	url = {http://arxiv.org/abs/2005.03995},
	shorttitle = {{DeepHist}},
	abstract = {We present the {DeepHist} - a novel Deep Learning framework for augmenting a network by histogram layers and demonstrate its strength by addressing image-to-image translation problems. Specifically, given an input image and a reference color distribution we aim to generate an output image with the structural appearance (content) of the input (source) yet with the colors of the reference. The key idea is a new technique for a differentiable construction of joint and color histograms of the output images. We further define a color distribution loss based on the Earth Mover's Distance between the output's and the reference's color histograms and a Mutual Information loss based on the joint histograms of the source and the output images. Promising results are shown for the tasks of color transfer, image colorization and edges \${\textbackslash}rightarrow\$ photo, where the color distribution of the output image is controlled. Comparison to Pix2Pix and {CyclyGANs} are shown.},
	journaltitle = {{arXiv}:2005.03995 [cs, eess]},
	author = {Avi-Aharon, Mor and Arbelle, Assaf and Raviv, Tammy Riklin},
	urldate = {2020-06-05},
	date = {2020-05-06},
	eprinttype = {arxiv},
	eprint = {2005.03995},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/REEKDIGL/Avi-Aharon et al. - 2020 - DeepHist Differentiable Joint and Color Histogram.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/Y8IJPLAM/2005.html:text/html}
}

@article{li_dual-stream_2020,
	title = {Dual-stream Maximum Self-attention Multi-instance Learning},
	url = {http://arxiv.org/abs/2006.05538},
	abstract = {Multi-instance learning ({MIL}) is a form of weakly supervised learning where a single class label is assigned to a bag of instances while the instance-level labels are not available. Training classifiers to accurately determine the bag label and instance labels is a challenging but critical task in many practical scenarios, such as computational histopathology. Recently, {MIL} models fully parameterized by neural networks have become popular due to the high flexibility and superior performance. Most of these models rely on attention mechanisms that assign attention scores across the instance embeddings in a bag and produce the bag embedding using an aggregation operator. In this paper, we proposed a dual-stream maximum self-attention {MIL} model ({DSMIL}) parameterized by neural networks. The first stream deploys a simple {MIL} max-pooling while the top-activated instance embedding is determined and used to obtain self-attention scores across instance embeddings in the second stream. Different from most of the previous methods, the proposed model jointly learns an instance classifier and a bag classifier based on the same instance embeddings. The experiments results show that our method achieves superior performance compared to the best {MIL} methods and demonstrates state-of-the-art performance on benchmark {MIL} datasets.},
	journaltitle = {{arXiv}:2006.05538 [cs]},
	author = {Li, Bin and Eliceiri, Kevin W.},
	urldate = {2020-06-16},
	date = {2020-06-09},
	eprinttype = {arxiv},
	eprint = {2006.05538},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, toread, I.2.0},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/589GU9P9/Li et Eliceiri - 2020 - Dual-stream Maximum Self-attention Multi-instance .pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/C59HZ695/2006.html:text/html}
}

@article{wang_comparison_2019,
	title = {A Comparison of Five Multiple Instance Learning Pooling Functions for Sound Event Detection with Weak Labeling},
	url = {http://arxiv.org/abs/1810.09050},
	abstract = {Sound event detection ({SED}) entails two subtasks: recognizing what types of sound events are present in an audio stream (audio tagging), and pinpointing their onset and offset times (localization). In the popular multiple instance learning ({MIL}) framework for {SED} with weak labeling, an important component is the pooling function. This paper compares ﬁve types of pooling functions both theoretically and experimentally, with special focus on their performance of localization. Although the attention pooling function is currently receiving the most attention, we ﬁnd the linear softmax pooling function to perform the best among the ﬁve. Using this pooling function, we build a neural network called {TALNet}. It is the ﬁrst system to reach state-of-the-art audio tagging performance on Audio Set, while exhibiting strong localization performance on the {DCASE} 2017 challenge at the same time.},
	journaltitle = {{arXiv}:1810.09050 [cs, eess]},
	author = {Wang, Yun and Li, Juncheng and Metze, Florian},
	urldate = {2020-06-20},
	date = {2019-02-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.09050},
	keywords = {toread, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Wang et al. - 2019 - A Comparison of Five Multiple Instance Learning Po.pdf:/Users/trislaz/Zotero/storage/A24SNSND/Wang et al. - 2019 - A Comparison of Five Multiple Instance Learning Po.pdf:application/pdf}
}

@article{katharopoulos_transformers_2020,
	title = {Transformers are {RNNs}: Fast Autoregressive Transformers with Linear Attention},
	url = {http://arxiv.org/abs/2006.16236},
	shorttitle = {Transformers are {RNNs}},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	journaltitle = {{arXiv}:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	urldate = {2020-07-15},
	date = {2020-06-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.16236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf:/Users/trislaz/Zotero/storage/ZPL8II94/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf:application/pdf}
}

@article{ruifrok_quantification_nodate,
	title = {Quantification of histochemical staining by color deconvolution},
	abstract = {Objective: To develop a flexible method of separation and quantification of immunohistochemical staining by means of color image analysis. Study Design: An algorithm was developed to deconvolve the color information acquired with {RGB} cameras, to calculate the contribution of each of the applied stains, based on the stain-specific {RGB} absorption. The algorithm was tested using different combinations of {DAB}, hematoxylin and eosin at different staining levels.
Results: Quantification of the different stains was not significantly influenced by the combination of multiple stains in a single sample. The color deconvolution algorithm resulted in comparable quantification independent of the stain combinations, as long as the histochemical procedures did not influence the amount of stain in the sample due to bleaching because of stain solubility, and saturation of staining was prevented.
Conclusion: The presented image analysis algorithm provides a robust and flexible method for objective immunohistochemical analysis of samples stained with up to three different stains, using a laboratory microscope and standard {RGB} camera setup, and the public domain program {NIH} image.},
	pages = {21},
	author = {Ruifrok, Arnout C},
	langid = {english},
	file = {Ruifrok - Quantification of histochemical staining by color .pdf:/Users/trislaz/Zotero/storage/59HFB4AN/Ruifrok - Quantification of histochemical staining by color .pdf:application/pdf}
}

@article{tadrous_digital_2010,
	title = {Digital stain separation for histological images},
	volume = {240},
	rights = {© 2010 The Author Journal compilation © 2010 The Royal Microscopical Society},
	issn = {1365-2818},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2818.2010.03390.x},
	doi = {10.1111/j.1365-2818.2010.03390.x},
	abstract = {It is often desirable to perform digital image analyses on sections prepared for human interpretation, e.g. nuclear chromatin texture analysis or three-dimensional reconstructions using sections requiring human delineation of structures of interest. Unfortunately such analyses are often more effective using stains with less complex contrast. Here an automated selective ‘de-staining’ method for digital images is presented. The method separates an image into its red, green and blue and hue, saturation and intensity components. A mask of stained tissue is prepared by automatic percentile thresholding. A single weighted inverted colour channel is then added to each of the three primary colour channels separately by an iterative algorithm that adjusts the weights to give minimum variance within the mask. The modified red, green and blue channels are then recombined. This method is automatic requiring no pre-definition of stain colours or special hardware. The method is demonstrated to ‘de-stain’ nuclei in haematoxylin and eosin (H\&E) sections (and a separate haematoxylin image can be derived from this). An image of isolated brown reaction product is produced with immunoperoxidase preparations counterstained with haematoxylin. Furthermore trichrome (haematoxylin van Gieson, picrosirius red) and other common stains may be separated into their components with modifications of the same algorithm. Although other methods for colour separation do exist (e.g. spectral pathology and colour deconvolution) these require special apparatus or precise calibration and foreknowledge of pure dye colour spectra. The present method of digital stain separation is fully automatic with no such prerequisites.},
	pages = {164--172},
	number = {2},
	journaltitle = {Journal of Microscopy},
	author = {Tadrous, P. J.},
	urldate = {2020-07-17},
	date = {2010},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2818.2010.03390.x},
	keywords = {Colour, digital image, histology, spectral pathology, stain separation},
	file = {Snapshot:/Users/trislaz/Zotero/storage/IGN92KCX/j.1365-2818.2010.03390.html:text/html;tadrous2010.pdf:/Users/trislaz/Documents/cbio/papiers/tadrous2010.pdf:application/pdf}
}

@inproceedings{mccann_algorithm_2014,
	title = {Algorithm and benchmark dataset for stain separation in histology images},
	doi = {10.1109/ICIP.2014.7025803},
	abstract = {In this work, we present a new algorithm and benchmark dataset for stain separation in histology images. Histology is a critical and ubiquitous task in medical practice and research, serving as a gold standard of diagnosis for many diseases. Automating routine histology analysis tasks could reduce health care costs and improve diagnostic accuracy. One challenge in automation is that histology slides vary in their stain intensity and color; we therefore seek a digital method to normalize the appearance of histology images. As histology slides often have multiple stains on them that must be normalized independently, stain separation must occur before normalization. We propose a new digital stain separation method for the universally-used hematoxylin and eosin stain; this method improves on the state-of-the-art by adjusting the contrast of its eosin-only estimate and including a notion of stain interaction. To validate this method, we have collected a new benchmark dataset via chemical destaining containing ground truth images for stain separation, which we release publicly. Our experiments show that our method achieves more accurate stain separation than two comparison methods and that this improvement in separation accuracy leads to improved normalization.},
	eventtitle = {2014 {IEEE} International Conference on Image Processing ({ICIP})},
	pages = {3953--3957},
	booktitle = {2014 {IEEE} International Conference on Image Processing ({ICIP})},
	author = {{McCann}, Michael T. and Majumdar, Joshita and Peng, Cheng and Castro, Carlos A. and Kovačević, Jelena},
	date = {2014-10},
	note = {{ISSN}: 2381-8549},
	keywords = {histology, stain separation, Accuracy, automating routine histology analysis tasks, benchmark dataset, benchmark testing, Benchmark testing, Biomedical imaging, biomedical optical imaging, chemical destaining, Chemicals, diagnostic accuracy, digital stain separation method, disease diagnosis, diseases, eosin stain, gold standard, ground truth images, health care, health care costs, histology images, histology slides, Image color analysis, image colour analysis, medical image processing, medical practice, Signal to noise ratio, stain intensity, stain interaction, universally-used hematoxylin, unmixing, Vectors},
	file = {IEEE Xplore Abstract Record:/Users/trislaz/Zotero/storage/3XJLAW39/7025803.html:text/html;Version soumise:/Users/trislaz/Zotero/storage/TMJEXERI/McCann et al. - 2014 - Algorithm and benchmark dataset for stain separati.pdf:application/pdf}
}

@article{onder_review_2014,
	title = {A review on color normalization and color deconvolution methods in histopathology},
	volume = {22},
	issn = {1533-4058},
	doi = {10.1097/PAI.0000000000000003},
	abstract = {The histopathologists get the benefits of wide range of colored dyes to have much useful information about the lesions and the tissue compositions. Despite its advantages, the staining process comes up with quite complex variations in staining concentrations and correlations, tissue fixation types, and fixation time periods. Together with the improvements in computing power and with the development of novel image analysis methods, these imperfections have led to the emerging of several color normalization algorithms. This article is a review of the currently available digital color normalization methods for the bright field histopathology. We describe the proposed color normalization methodologies in detail together with the lesion and tissue types used in the corresponding experiments. We also present the quantitative validation approaches for each of the proposed methodology where available.},
	pages = {713--719},
	number = {10},
	journaltitle = {Applied immunohistochemistry \& molecular morphology: {AIMM}},
	shortjournal = {Appl. Immunohistochem. Mol. Morphol.},
	author = {Onder, Devrim and Zengin, Selen and Sarioglu, Sulen},
	date = {2014-12},
	pmid = {24897076},
	keywords = {Algorithms, Animals, Color, Coloring Agents, Humans, Image Processing, Computer-Assisted, Microscopy, Pattern Recognition, Automated, Staining and Labeling}
}

@article{azevedo_tosta_computational_2019,
	title = {Computational normalization of H\&E-stained histological images: Progress, challenges and future potential},
	volume = {95},
	issn = {0933-3657},
	url = {http://www.sciencedirect.com/science/article/pii/S093336571830424X},
	doi = {10.1016/j.artmed.2018.10.004},
	shorttitle = {Computational normalization of H\&E-stained histological images},
	abstract = {Different types of cancer can be diagnosed with the analysis of histological samples stained with hematoxylin–eosin (H\&E). Through this stain, it is possible to identify the architecture of tissue components and analyze cellular morphological aspects that are essential for cancer diagnosis. However, preparation and digitization of histological samples can lead to color variations that influence the performance of segmentation and classification algorithms in histological image analysis systems. Among the determinant factors of these color variations are different staining time, concentration and {pH} of the solutions, and the use of different digitization systems. This has motivated the development of normalization algorithms of histological images for their color adjustments. These methods are designed to guarantee that biological samples are not altered and artifacts are not introduced in the images, thus compromising the lesions diagnosis. In this context, normalization techniques are proposed to minimize color variations in histological images, and they are topics covered by important studies in the literature. In this proposal, it is presented a detailed study of the state of art of computational normalization of H\&E-stained histological images, highlighting the main contributions and limitations of correlated works. Besides, the evaluation of normalization methods published in the literature are depicted and possible directions for new methods are described.},
	pages = {118--132},
	journaltitle = {Artificial Intelligence in Medicine},
	shortjournal = {Artificial Intelligence in Medicine},
	author = {Azevedo Tosta, Thaína A. and de Faria, Paulo Rogério and Neves, Leandro Alves and do Nascimento, Marcelo Zanchetta},
	urldate = {2020-07-17},
	date = {2019-04-01},
	langid = {english},
	keywords = {Color corrections, Hematoxylin–eosin, Histological image analysis, Normalization},
	file = {ScienceDirect Snapshot:/Users/trislaz/Zotero/storage/QI8KDNH6/S093336571830424X.html:text/html;azevedotosta2018.pdf:/Users/trislaz/Documents/cbio/papiers/azevedotosta2018.pdf:application/pdf}
}

@article{roy_study_2018,
	title = {A study about color normalization methods for histopathology images},
	volume = {114},
	issn = {1878-4291},
	doi = {10.1016/j.micron.2018.07.005},
	abstract = {Histopathology images are used for the diagnosis of the cancerous disease by the examination of tissue with the help of Whole Slide Imaging ({WSI}) scanner. A decision support system works well by the analysis of the histopathology images but a lot of problems arise in its decision. Color variation in the histopathology images is occurring due to use of the different scanner, use of various equipments, different stain coloring and reactivity from a different manufacturer. In this paper, detailed study and performance evaluation of color normalization methods on histopathology image datasets are presented. Color normalization of the source image by transferring the mean color of the target image in the source image and also to separate stain present in the source image. Stain separation and color normalization of the histopathology images can be helped for both pathology and computerized decision support system. Quality performances of different color normalization methods are evaluated and compared in terms of quaternion structure similarity index matrix ({QSSIM}), structure similarity index matrix ({SSIM}) and Pearson correlation coefficient ({PCC}) on various histopathology image datasets. Our experimental analysis suggests that structure-preserving color normalization ({SPCN}) provides better qualitatively and qualitatively results in comparison to the all the presented methods for breast and colorectal cancer histopathology image datasets.},
	pages = {42--61},
	journaltitle = {Micron (Oxford, England: 1993)},
	shortjournal = {Micron},
	author = {Roy, Santanu and Kumar Jain, Alok and Lal, Shyam and Kini, Jyoti},
	date = {2018},
	pmid = {30096632},
	keywords = {Algorithms, Color, Coloring Agents, Humans, Image Processing, Computer-Assisted, Staining and Labeling, Breast, Breast Neoplasms, Color variation, Colorectal Neoplasms, Decision Support Systems, Clinical, Female, Gastrointestinal Tract, Histocytochemistry, Histopathology images, Kidney, Kidney Neoplasms, Liver, Liver Neoplasms, Quality metrics, Spectral normalization}
}

@article{jing_self-supervised_2019,
	title = {Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey},
	url = {http://arxiv.org/abs/1902.06162},
	shorttitle = {Self-supervised Visual Feature Learning with Deep Neural Networks},
	abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this ﬁeld are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
	journaltitle = {{arXiv}:1902.06162 [cs]},
	author = {Jing, Longlong and Tian, Yingli},
	urldate = {2020-07-17},
	date = {2019-02-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.06162},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Jing et Tian - 2019 - Self-supervised Visual Feature Learning with Deep .pdf:/Users/trislaz/Zotero/storage/WVGE36FT/Jing et Tian - 2019 - Self-supervised Visual Feature Learning with Deep .pdf:application/pdf}
}

@article{ba_layer_2016,
	title = {Layer Normalization},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	journaltitle = {{arXiv}:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	urldate = {2020-07-21},
	date = {2016-07-21},
	eprinttype = {arxiv},
	eprint = {1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/ACAWDZYN/Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/W3BCLXLQ/1607.html:text/html}
}

@inproceedings{wu_deep_2015,
	location = {Boston, {MA}, {USA}},
	title = {Deep multiple instance learning for image classification and auto-annotation},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298968/},
	doi = {10.1109/CVPR.2015.7298968},
	abstract = {The recent development in learning deep representations has demonstrated its wide applications in traditional vision tasks like classiﬁcation and detection. However, there has been little investigation on how we could build up a deep learning framework in a weakly supervised setting. In this paper, we attempt to model deep learning in a weakly supervised learning (multiple instance learning) framework. In our setting, each image follows a dual multi-instance assumption, where its object proposals and possible text annotations can be regarded as two instance sets. We thus design effective systems to exploit the {MIL} property with deep learning strategies from the two ends; we also try to jointly learn the relationship between object and annotation proposals. We conduct extensive experiments and prove that our weakly supervised deep learning framework not only achieves convincing performance in vision tasks including classiﬁcation and image annotation, but also extracts reasonable region-keyword pairs with little supervision, on both widely used benchmarks like {PASCAL} {VOC} and {MIT} Indoor Scene 67, and also a dataset for imageand patch-level annotations.},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {3460--3469},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Wu, Jiajun and {Yinan Yu} and {Chang Huang} and {Kai Yu}},
	urldate = {2020-07-22},
	date = {2015-06},
	langid = {english},
	file = {Wu et al. - 2015 - Deep multiple instance learning for image classifi.pdf:/Users/trislaz/Zotero/storage/M8BEWJ5V/Wu et al. - 2015 - Deep multiple instance learning for image classifi.pdf:application/pdf}
}

@article{li_patch_2019,
	title = {Patch Transformer for Multi-tagging Whole Slide Histopathology Images},
	url = {http://arxiv.org/abs/1906.04151},
	abstract = {Automated whole slide image ({WSI}) tagging has become a growing demand due to the increasing volume and diversity of {WSIs} collected nowadays in histopathology. Various methods have been studied to classify {WSIs} with single tags but none of them focuses on labeling {WSIs} with multiple tags. To this end, we propose a novel end-to-end trainable deep neural network named Patch Transformer which can effectively predict multiple slide-level tags from {WSI} patches based on both the correlations and the uniqueness between the tags. Speciﬁcally, the proposed method learns patch characteristics considering 1) patchwise relations through a patch transformation module and 2) tag-wise uniqueness for each tagging task through a multi-tag attention module. Extensive experiments on a large and diverse dataset consisting of 4,920 {WSIs} prove the eﬀectiveness of the proposed model.},
	journaltitle = {{arXiv}:1906.04151 [cs]},
	author = {Li, Weijian and Nguyen, Viet-Duy and Liao, Haofu and Wilder, Matt and Cheng, Ke and Luo, Jiebo},
	urldate = {2020-07-22},
	date = {2019-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.04151},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al. - 2019 - Patch Transformer for Multi-tagging Whole Slide Hi.pdf:/Users/trislaz/Zotero/storage/AU2N8K3F/Li et al. - 2019 - Patch Transformer for Multi-tagging Whole Slide Hi.pdf:application/pdf}
}

@article{sakamoto_narrative_nodate,
	title = {A narrative review of digital pathology and artificial intelligence: focusing on lung cancer},
	abstract = {The emergence of whole slide imaging technology allows for pathology diagnosis on a computer screen. The applications of digital pathology are expanding, from supporting remote institutes suffering from a shortage of pathologists to routine use in daily diagnosis including that of lung cancer. Through practice and research large archival databases of digital pathology images have been developed that will facilitate the development of artificial intelligence ({AI}) methods for image analysis. Currently, several {AI} applications have been reported in the field of lung cancer; these include the segmentation of carcinoma foci, detection of lymph node metastasis, counting of tumor cells, and prediction of gene mutations. Although the integration of {AI} algorithms into clinical practice remains a significant challenge, we have implemented tumor cell count for genetic analysis, a helpful application for routine use. Our experience suggests that pathologists often overestimate the contents of tumor cells, and the use of {AI}-based analysis increases the accuracy and makes the tasks less tedious. However, there are several difficulties encountered in the practical use of {AI} in clinical diagnosis. These include the lack of sufficient annotated data for the development and validation of {AI} systems, the explainability of black box {AI} models, such as those based on deep learning that offer the most promising performance, and the difficulty in defining the ground truth data for training and validation owing to inherent ambiguity in most applications. All of these together present significant challenges in the development and clinical translation of {AI} methods in the practice of pathology. Additional research on these problems will help in resolving the barriers to the clinical use of {AI}. Helping pathologists in developing knowledge of the working and limitations of {AI} will benefit the use of {AI} in both diagnostics and research.},
	pages = {23},
	author = {Sakamoto, Taro and Furukawa, Tomoi and Lami, Kris and Pham, Hoa Hoang Ngoc and Uegami, Wataru and Kuroda, Kishio and Kawai, Masataka and Sakanashi, Hidenori and Cooper, Lee Alex Donald and Bychkov, Andrey and Fukuoka, Junya},
	langid = {english},
	file = {Sakamoto et al. - A narrative review of digital pathology and artifi.pdf:/Users/trislaz/Zotero/storage/GLBJAFR2/Sakamoto et al. - A narrative review of digital pathology and artifi.pdf:application/pdf}
}

@inproceedings{xu_weak_2020,
	title = {A Weak Supervision-based Framework for Automatic Lung Cancer Classification on Whole Slide Image},
	doi = {10.1109/EMBC44109.2020.9176620},
	abstract = {Classification of normal lung tissue, lung adenocarcinoma ({LUAD}) and lung squamous cell carcinoma ({LUSC}) by pathological images is significant for clinical diagnosis and treatment. Due to the large scale of pathological images and the absence of definitive morphological features between {LUAD} and {LUSC}, it is time-consuming, laborious and challenging for pathologists to analyze the microscopic histopathology slides by visual observation. In this paper, a pixel-level annotation-free framework was proposed to classify normal tissue, {LUAD} and {LUSC} slides. This framework can be divided into two stages: tumor classification and localization, and subtype classification. In the first stage, {EM}-{CNN} was utilized to distinguish tumor slides from normal tissue slides and locate the discriminative regions for subsequent analysis with only image-level labels provided. In the second stage, a multi-scale network was proposed to improve the accuracy of subtype classification. This method achieved an {AUC} of 0.9978 for tumor classification and an {AUC} of 0.9684 for subtype classification, showing its superiority in lung pathological image classification compared with other methods.},
	eventtitle = {2020 42nd Annual International Conference of the {IEEE} Engineering in Medicine Biology Society ({EMBC})},
	pages = {1372--1375},
	booktitle = {2020 42nd Annual International Conference of the {IEEE} Engineering in Medicine Biology Society ({EMBC})},
	author = {Xu, Xiaowei and Hou, Runping and Zhao, Wangyuan and Teng, Haohua and Sun, Jianqi and Zhao, Jun},
	date = {2020-07},
	note = {{ISSN}: 1558-4615},
	keywords = {Cancer, Computer architecture, Heating systems, Lung, Pathology, Training, Tumors},
	file = {IEEE Xplore Abstract Record:/Users/trislaz/Zotero/storage/BTYYLCY3/9176620.html:text/html}
}

@article{gildenblat_certainty_2020,
	title = {Certainty Pooling for Multiple Instance Learning},
	url = {http://arxiv.org/abs/2008.10548},
	abstract = {Multiple Instance Learning is a form of weakly supervised learning in which the data is arranged in sets of instances called bags with one label assigned per bag. The bag level class prediction is derived from the multiple instances through application of a permutation invariant pooling operator on instance predictions or embeddings. We present a novel pooling operator called Certainty Pooling which incorporates the model certainty into bag predictions resulting in a more robust and explainable model. We compare our proposed method with other pooling operators in controlled experiments with low evidence ratio bags based on {MNIST}, as well as on a real life histopathology dataset - Camelyon16. Our method outperforms other methods in both bag level and instance level prediction, especially when only small training sets are available. We discuss the rationale behind our approach and the reasons for its superiority for these types of datasets.},
	journaltitle = {{arXiv}:2008.10548 [cs]},
	author = {Gildenblat, Jacob and Ben-Shaul, Ido and Lapp, Zvi and Klaiman, Eldad},
	urldate = {2020-09-10},
	date = {2020-08-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.10548},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Gildenblat et al. - 2020 - Certainty Pooling for Multiple Instance Learning.pdf:/Users/trislaz/Zotero/storage/3UZJN3AW/Gildenblat et al. - 2020 - Certainty Pooling for Multiple Instance Learning.pdf:application/pdf}
}

@article{hong_gated_nodate,
	title = {Gated Multi-head Attention Pooling for Weakly Labelled Audio Tagging},
	abstract = {Multiple instance learning ({MIL}) has recently been used for weakly labelled audio tagging, where the spectrogram of an audio signal is divided into segments to form instances in a bag, and then the low-dimensional features of these segments are pooled for tagging. The choice of a pooling scheme is the key to exploiting the weakly labelled data. However, the traditional pooling schemes are usually ﬁxed and unable to distinguish the contributions, making it difﬁcult to adapt to the characteristics of the sound events. In this paper, a novel pooling algorithm is proposed for {MIL}, named gated multi-head attention pooling ({GMAP}), which is able to attend to the information of events from different heads at different positions. Each head allows the model to learn information from different representation subspaces. Furthermore, in order to avoid the redundancy of multi-head information, a gating mechanism is used to fuse individual head features. The proposed {GMAP} increases the modeling power of the single-head attention with no computational overhead. Experiments are carried out on Audioset, which is a large-scale weakly labelled dataset, and show superior results to the non-adaptive pooling and the vanilla attention pooling schemes.},
	pages = {5},
	author = {Hong, Sixin and Zou, Yuexian and Wang, Wenwu},
	langid = {english},
	file = {Hong et al. - Gated Multi-head Attention Pooling for Weakly Labe.pdf:/Users/trislaz/Zotero/storage/A3GTDYEM/Hong et al. - Gated Multi-head Attention Pooling for Weakly Labe.pdf:application/pdf}
}

@article{xu_how_2020,
	title = {How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks},
	url = {http://arxiv.org/abs/2009.11848},
	shorttitle = {How Neural Networks Extrapolate},
	abstract = {We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while multilayer perceptrons ({MLPs}) do not extrapolate well in simple tasks, Graph Neural Networks ({GNNs}), a structured network with {MLP} modules, have some success in more complex tasks. We provide a theoretical explanation and identify conditions under which {MLPs} and {GNNs} extrapolate well. We start by showing {ReLU} {MLPs} trained by gradient descent converge quickly to linear functions along any direction from the origin, which suggests {ReLU} {MLPs} cannot extrapolate well in most non-linear tasks. On the other hand, {ReLU} {MLPs} can provably converge to a linear target function when the training distribution is "diverse" enough. These observations lead to a hypothesis: {GNNs} can extrapolate well in dynamic programming ({DP}) tasks if we encode appropriate non-linearity in the architecture and input representation. We provide theoretical and empirical support for the hypothesis. Our theory explains previous extrapolation success and suggest their limitations: successful extrapolation relies on incorporating task-specific non-linearity, which often requires domain knowledge or extensive model search.},
	journaltitle = {{arXiv}:2009.11848 [cs, stat]},
	author = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
	urldate = {2020-09-28},
	date = {2020-09-24},
	eprinttype = {arxiv},
	eprint = {2009.11848},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/ZAD9D47R/Xu et al. - 2020 - How Neural Networks Extrapolate From Feedforward .pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/EBUEHG6T/2009.html:text/html}
}

@online{noauthor_survey_nodate,
	title = {A survey on semi-supervised learning {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/s10994-019-05855-6},
	urldate = {2020-10-08},
	file = {A survey on semi-supervised learning | SpringerLink:/Users/trislaz/Zotero/storage/RG2YHU2C/s10994-019-05855-6.html:text/html;VanEngelen-Hoos2020_Article_ASurveyOnSemi-supervisedLearni.pdf:/Users/trislaz/Documents/cbio/papiers/VanEngelen-Hoos2020_Article_ASurveyOnSemi-supervisedLearni.pdf:application/pdf}
}

@article{he_momentum_2020,
	title = {Momentum Contrast for Unsupervised Visual Representation Learning},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast ({MoCo}) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. {MoCo} provides competitive results under the common linear protocol on {ImageNet} classification. More importantly, the representations learned by {MoCo} transfer well to downstream tasks. {MoCo} can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on {PASCAL} {VOC}, {COCO}, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	journaltitle = {{arXiv}:1911.05722 [cs]},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	urldate = {2020-10-09},
	date = {2020-03-23},
	eprinttype = {arxiv},
	eprint = {1911.05722},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/SIPB9SVP/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/3RYSE29T/1911.html:text/html}
}

@article{janocha_loss_2017,
	title = {On Loss Functions for Deep Neural Networks in Classification},
	url = {http://arxiv.org/abs/1702.05659},
	abstract = {Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.},
	journaltitle = {{arXiv}:1702.05659 [cs]},
	author = {Janocha, Katarzyna and Czarnecki, Wojciech Marian},
	urldate = {2020-10-22},
	date = {2017-02-18},
	eprinttype = {arxiv},
	eprint = {1702.05659},
	keywords = {Computer Science - Machine Learning, toread},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/6JBTGFLZ/Janocha et Czarnecki - 2017 - On Loss Functions for Deep Neural Networks in Clas.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/GJARJKRP/1702.html:text/html}
}

@article{kukar_cost-sensitive_nodate,
	title = {Cost-Sensitive Learning with Neural Networks},
	abstract = {In the usual setting of Machine Learning, classiﬁers are typically evaluated by estimating their error rate (or equivalently, the classiﬁcation accuracy) on the test data. However, this makes sense only if all errors have equal (uniform) costs. When the costs of errors differ between each other, the classiﬁers should be evaluated by comparing the total costs of the errors.},
	pages = {5},
	journaltitle = {Machine Learning and Data Mining},
	author = {Kukar, Matjazˇ and Kononenko, Igor},
	langid = {english},
	file = {Kukar et Kononenko - Cost-Sensitive Learning with Neural Networks.pdf:/Users/trislaz/Zotero/storage/4CXM2T76/Kukar et Kononenko - Cost-Sensitive Learning with Neural Networks.pdf:application/pdf}
}

@article{zhi-hua_zhou_training_2006,
	title = {Training cost-sensitive neural networks with methods addressing the class imbalance problem},
	volume = {18},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/1549828/},
	doi = {10.1109/TKDE.2006.17},
	abstract = {This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both over-sampling and under-sampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassiﬁed. Moreover, hard-ensemble and soft-ensemble, i.e. the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one {UCI} data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that cost-sensitive learning with multi-class tasks is more difﬁcult than with two-class tasks, and a higher degree of class imbalance may increase the difﬁculty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multi-class tasks. Overall, threshold-moving and softensemble are relatively good choices in training cost-sensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may in fact only be effective on learning with imbalanced two-class data sets.},
	pages = {63--77},
	number = {1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {{Zhi-Hua Zhou} and {Xu-Ying Liu}},
	urldate = {2020-10-26},
	date = {2006-01},
	langid = {english},
	file = {Zhi-Hua Zhou et Xu-Ying Liu - 2006 - Training cost-sensitive neural networks with metho.pdf:/Users/trislaz/Zotero/storage/8P4EBIMR/Zhi-Hua Zhou et Xu-Ying Liu - 2006 - Training cost-sensitive neural networks with metho.pdf:application/pdf}
}

@article{chung_cost-sensitive_2020,
	title = {Cost-Sensitive Deep Learning with Layer-Wise Cost Estimation},
	url = {http://arxiv.org/abs/1611.05134},
	abstract = {While deep neural networks have succeeded in several applications, such as image classiﬁcation, object detection, and speech recognition, by reaching very high classiﬁcation accuracies, it is important to note that many real-world applications demand varying costs for different types of misclassiﬁcation errors, thus requiring cost-sensitive classiﬁcation algorithms. Current models of deep neural networks for cost-sensitive classiﬁcation are restricted to some speciﬁc network structures and limited depth. In this paper, we propose a novel framework that can be applied to deep neural networks with any structure to facilitate their learning of meaningful representations for cost-sensitive classiﬁcation problems. Furthermore, the framework allows endto-end training of deeper networks directly. The framework is designed by augmenting auxiliary neurons to the output of each hidden layer for layer-wise cost estimation, and including the total estimation loss within the optimization objective. Experimental results on public benchmark data sets with two cost information settings demonstrate that the proposed framework outperforms state-of-the-art cost-sensitive deep learning models.},
	journaltitle = {{arXiv}:1611.05134 [cs]},
	author = {Chung, Yu-An and Yang, Shao-Wen and Lin, Hsuan-Tien},
	urldate = {2020-10-26},
	date = {2020-10-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.05134},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chung et al. - 2020 - Cost-Sensitive Deep Learning with Layer-Wise Cost .pdf:/Users/trislaz/Zotero/storage/7PD5WK7N/Chung et al. - 2020 - Cost-Sensitive Deep Learning with Layer-Wise Cost .pdf:application/pdf}
}

@article{appel_improved_2016,
	title = {Improved Multi-Class Cost-Sensitive Boosting via Estimation of the Minimum-Risk Class},
	url = {http://arxiv.org/abs/1607.03547},
	abstract = {We present a simple uniﬁed framework for multi-class cost-sensitive boosting. The minimum-risk class is estimated directly, rather than via an approximation of the posterior distribution. Our method jointly optimizes binary weak learners and their corresponding output vectors, requiring classes to share features at each iteration. By training in a cost-sensitive manner, weak learners are invested in separating classes whose discrimination is important, at the expense of less relevant classiﬁcation boundaries. Additional contributions are a family of loss functions along with proof that our algorithm is Boostable in the theoretical sense, as well as an efﬁcient procedure for growing decision trees for use as weak learners. We evaluate our method on a variety of datasets: a collection of synthetic planar data, common {UCI} datasets, {MNIST} digits, {SUN} scenes, and {CUB}-200 birds. Results show state-of-the-art performance across all datasets against several strong baselines, including non-boosting multi-class approaches.},
	journaltitle = {{arXiv}:1607.03547 [cs]},
	author = {Appel, Ron and Burgos-Artizzu, Xavier and Perona, Pietro},
	urldate = {2020-10-26},
	date = {2016-11-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1607.03547},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Appel et al. - 2016 - Improved Multi-Class Cost-Sensitive Boosting via E.pdf:/Users/trislaz/Zotero/storage/XXNJCSG5/Appel et al. - 2016 - Improved Multi-Class Cost-Sensitive Boosting via E.pdf:application/pdf}
}

@article{khan_cost_2017,
	title = {Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data},
	url = {http://arxiv.org/abs/1508.03422},
	abstract = {Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes is abundant making them an over-represented majority, and data of other classes is scarce, making them an under-represented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this work, we propose a cost sensitive deep neural network which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multi-class problems without any modification. Moreover, as opposed to data level approaches, we do not alter the original data distribution which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and cost sensitive classifiers demonstrate the superior performance of our proposed method.},
	journaltitle = {{arXiv}:1508.03422 [cs]},
	author = {Khan, Salman H. and Hayat, Munawar and Bennamoun, Mohammed and Sohel, Ferdous and Togneri, Roberto},
	urldate = {2020-10-26},
	date = {2017-03-23},
	eprinttype = {arxiv},
	eprint = {1508.03422},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/RYPIVNZU/Khan et al. - 2017 - Cost Sensitive Learning of Deep Feature Representa.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/4QIU3GQM/1508.html:text/html}
}

@article{chung_cost-aware_2016,
	title = {Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning},
	url = {http://arxiv.org/abs/1511.09337},
	abstract = {Deep learning has been one of the most prominent machine learning techniques nowadays, being the state-of-the-art on a broad range of applications where automatic feature extraction is needed. Many such applications also demand varying costs for different types of mis-classification errors, but it is not clear whether or how such cost information can be incorporated into deep learning to improve performance. In this work, we propose a novel cost-aware algorithm that takes into account the cost information into not only the training stage but also the pre-training stage of deep learning. The approach allows deep learning to conduct automatic feature extraction with the cost information effectively. Extensive experimental results demonstrate that the proposed approach outperforms other deep learning models that do not digest the cost information in the pre-training stage.},
	journaltitle = {{arXiv}:1511.09337 [cs]},
	author = {Chung, Yu-An and Lin, Hsuan-Tien and Yang, Shao-Wen},
	urldate = {2020-10-26},
	date = {2016-05-24},
	eprinttype = {arxiv},
	eprint = {1511.09337},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/WGGGPMZA/Chung et al. - 2016 - Cost-aware Pre-training for Multiclass Cost-sensit.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/JS9TGSFK/1511.html:text/html}
}

@article{bhattacharjee_pooling_2020,
	title = {A Pooling Function based on Differential Evolution for Multiple Instance Learning},
	abstract = {While implementing Multiple Instance Learning ({MIL}) through Deep Neural Networks, the most important task is to design the bag level pooling function which defines the instance-to-bag relationship and eventually determines the class label of a bag. In this article, an {MIL} pooling function based on Differential Evolution ({DE}) – a bio-inspired metaheuristics is proposed for optimizing the instance weights parallelly with training the Deep Neural Network.},
	pages = {6},
	author = {Bhattacharjee, Kamanasish and Tiwari, Arti and Pant, Millie and Ahn, Chang Wook},
	date = {2020},
	langid = {english},
	file = {Bhattacharjee et al. - 2020 - A Pooling Function based on Differential Evolution.pdf:/Users/trislaz/Zotero/storage/LXZDQPDE/Bhattacharjee et al. - 2020 - A Pooling Function based on Differential Evolution.pdf:application/pdf}
}

@article{turner_signatures_2017,
	title = {Signatures of {DNA}-Repair Deficiencies in Breast Cancer},
	volume = {377},
	issn = {1533-4406},
	doi = {10.1056/NEJMcibr1710161},
	pages = {2490--2492},
	number = {25},
	journaltitle = {The New England Journal of Medicine},
	shortjournal = {N Engl J Med},
	author = {Turner, Nicholas C.},
	date = {2017},
	pmid = {29262283},
	keywords = {Humans, Breast Neoplasms, Female, {DNA} Repair-Deficiency Disorders, Mutation}
}

@article{manie_genomic_2016-1,
	title = {Genomic hallmarks of homologous recombination deficiency in invasive breast carcinomas},
	volume = {138},
	issn = {1097-0215},
	doi = {10.1002/ijc.29829},
	abstract = {Therapeutic strategies targeting Homologous Recombination Deficiency ({HRD}) in breast cancer requires patient stratification. The {LST} (Large-scale State Transitions) genomic signature previously validated for triple-negative breast carcinomas ({TNBC}) was evaluated as biomarker of {HRD} in luminal (hormone receptor positive) and {HER}2-overexpressing ({HER}2+) tumors. The {LST} genomic signature related to the number of large-scale chromosomal breakpoints in {SNP}-array tumor profile was applied to identify {HRD} in in-house and {TCGA} sets of breast tumors, in which the status of {BRCA}1/2 and other genes was also investigated. In the in-house dataset, {HRD} was predicted in 5\% (20/385) of sporadic tumors luminal or {HER}2+ by the {LST} genomic signature and the inactivation of {BRCA}1, {BRCA}2 or {RAD}51C confirmed this prediction in 75\% (12/16) of the tested cases. In 14\% (6/43) of tumors occurring in {BRCA}1/2 mutant carriers, the corresponding wild-type allele was retained emphasizing the importance of determining the tumor status. In the {TCGA} luminal and {HER}2+ subtypes {HRD} incidence was estimated at 5\% (18/329, 95\%{CI}: 5-8\%) and 2\% (1/59, 95\%{CI}: 2-9\%), respectively. In {TNBC} cisplatin-based neo-adjuvant clinical trials, {HRD} is shown to be a necessary condition for cisplatin sensitivity. This analysis demonstrates the high performance of the {LST} genomic signature for {HRD} detection in breast cancers, which suggests its potential as a biomarker for genetic testing and patient stratification for clinical trials evaluating platinum salts and {PARP} inhibitors.},
	pages = {891--900},
	number = {4},
	journaltitle = {International Journal of Cancer},
	shortjournal = {Int J Cancer},
	author = {Manié, Elodie and Popova, Tatiana and Battistella, Aude and Tarabeux, Julien and Caux-Moncoutier, Virginie and Golmard, Lisa and Smith, Nicholas K. and Mueller, Christopher R. and Mariani, Odette and Sigal-Zafrani, Brigitte and Dubois, Thierry and Vincent-Salomon, Anne and Houdayer, Claude and Stoppa-Lyonnet, Dominique and Stern, Marc-Henri},
	date = {2016-02-15},
	pmid = {26317927},
	keywords = {Humans, Breast Neoplasms, Female, Biomarkers, Tumor, {BRCA}1, {BRCA}2, breast carcinoma, Carcinoma, Chromosome Breakage, Genes, {BRCA}2, homologous recombination deficiency, Oligonucleotide Array Sequence Analysis, Receptor, {ErbB}-2, Recombinational {DNA} Repair, Transcriptome},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/SGKCE6NL/Manié et al. - 2016 - Genomic hallmarks of homologous recombination defi.pdf:application/pdf}
}

@article{ferrari_whole-genome_2016,
	title = {A whole-genome sequence and transcriptome perspective on {HER}2-positive breast cancers},
	volume = {7},
	rights = {2016 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms12222},
	doi = {10.1038/ncomms12222},
	abstract = {{HER}2-positive breast cancer has long proven to be a clinically distinct class of breast cancers for which several targeted therapies are now available. However, resistance to the treatment associated with specific gene expressions or mutations has been observed, revealing the underlying diversity of these cancers. Therefore, understanding the full extent of the {HER}2-positive disease heterogeneity still remains challenging. Here we carry out an in-depth genomic characterization of 64 {HER}2-positive breast tumour genomes that exhibit four subgroups, based on the expression data, with distinctive genomic features in terms of somatic mutations, copy-number changes or structural variations. The results suggest that, despite being clinically defined by a specific gene amplification, {HER}2-positive tumours melt into the whole luminal–basal breast cancer spectrum rather than standing apart. The results also lead to a refined {ERBB}2 amplicon of 106 kb and show that several cases of amplifications are compatible with a breakage–fusion–bridge mechanism.},
	pages = {12222},
	number = {1},
	journaltitle = {Nature Communications},
	author = {Ferrari, Anthony and Vincent-Salomon, Anne and Pivot, Xavier and Sertier, Anne-Sophie and Thomas, Emilie and Tonon, Laurie and Boyault, Sandrine and Mulugeta, Eskeatnaf and Treilleux, Isabelle and {MacGrogan}, Gaëtan and Arnould, Laurent and Kielbassa, Janice and Le Texier, Vincent and Blanché, Hélène and Deleuze, Jean-François and Jacquemier, Jocelyne and Mathieu, Marie-Christine and Penault-Llorca, Frédérique and Bibeau, Frédéric and Mariani, Odette and Mannina, Cécile and Pierga, Jean-Yves and Trédan, Olivier and Bachelot, Thomas and Bonnefoi, Hervé and Romieu, Gilles and Fumoleau, Pierre and Delaloge, Suzette and Rios, Maria and Ferrero, Jean-Marc and Tarpin, Carole and Bouteille, Catherine and Calvo, Fabien and Gut, Ivo Glynne and Gut, Marta and Martin, Sancha and Nik-Zainal, Serena and Stratton, Michael R. and Pauporté, Iris and Saintigny, Pierre and Birnbaum, Daniel and Viari, Alain and Thomas, Gilles},
	urldate = {2020-11-12},
	date = {2016-07-13},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/ANFRSBUV/Ferrari et al. - 2016 - A whole-genome sequence and transcriptome perspect.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/WEPT38YK/ncomms12222.html:text/html}
}

@article{paluch-shimon_targeting_2019,
	title = {Targeting {DNA} repair in breast cancer},
	volume = {47},
	issn = {0960-9776},
	url = {http://www.sciencedirect.com/science/article/pii/S0960977619305284},
	doi = {10.1016/j.breast.2019.06.007},
	abstract = {Targeting of {DNA} repair is an important therapeutic approach in breast cancer, particularly for {BRCA}1/2 associated breast cancers and those characterized by a “{BRCAness}” phenotype including those with “triple negative” subtype. Various assays and scores have been developed to evaluate degree of homologous recombination deficiency in the hope that this would aid in predicting for susceptibility to {DNA} repair targeting agents, and yet, presence of a germline mutation in {BRCA}1/2 remains the strongest predictor for therapeutic efficacy of such agents. Pre-clinical studies suggested increased sensitivity to agents that damage {DNA} in a way that interferes with {DNA} replication forks and which subsequently require {DNA} repair by homologous recombination, such as platinum salts, and this data was further confirmed clinically. Recently published phase {III} data favor the use of {PARP} inhibitors amongst patients with {BRCA}1/2 associated advanced breast cancer. Novel chemotherapeutic agents targeting {DNA} damage repair are under evaluation as well as further combinations of {PARP} inhibitors with immuno-therapeutics and other biological agents.},
	pages = {33--42},
	journaltitle = {The Breast},
	shortjournal = {The Breast},
	author = {Paluch-Shimon, Shani and Evron, Ella},
	urldate = {2020-11-12},
	date = {2019-10-01},
	langid = {english},
	keywords = {{BRCA}1, {BRCA}2, {DNA}, Homologous-repair-deficiency, {PARP}},
	file = {ScienceDirect Snapshot:/Users/trislaz/Zotero/storage/URK9DG2M/S0960977619305284.html:text/html}
}

@article{tutt_carboplatin_2018,
	title = {Carboplatin in {BRCA}1/2-mutated and triple-negative breast cancer {BRCAness} subgroups: the {TNT} Trial},
	volume = {24},
	issn = {1546-170X},
	doi = {10.1038/s41591-018-0009-7},
	shorttitle = {Carboplatin in {BRCA}1/2-mutated and triple-negative breast cancer {BRCAness} subgroups},
	abstract = {Germline mutations in {BRCA}1/2 predispose individuals to breast cancer (termed germline-mutated {BRCA}1/2 breast cancer, {gBRCA}-{BC}) by impairing homologous recombination ({HR}) and causing genomic instability. {HR} also repairs {DNA} lesions caused by platinum agents and {PARP} inhibitors. Triple-negative breast cancers ({TNBCs}) harbor subpopulations with {BRCA}1/2 mutations, hypothesized to be especially platinum-sensitive. Cancers in putative '{BRCAness}' subgroups-tumors with {BRCA}1 methylation; low levels of {BRCA}1 {mRNA} ({BRCA}1 {mRNA}-low); or mutational signatures for {HR} deficiency and those with basal phenotypes-may also be sensitive to platinum. We assessed the efficacy of carboplatin and another mechanistically distinct therapy, docetaxel, in a phase 3 trial in subjects with unselected advanced {TNBC}. A prespecified protocol enabled biomarker-treatment interaction analyses in {gBRCA}-{BC} and {BRCAness} subgroups. The primary endpoint was objective response rate ({ORR}). In the unselected population (376 subjects; 188 carboplatin, 188 docetaxel), carboplatin was not more active than docetaxel ({ORR}, 31.4\% versus 34.0\%, respectively; P = 0.66). In contrast, in subjects with {gBRCA}-{BC}, carboplatin had double the {ORR} of docetaxel (68\% versus 33\%, respectively; biomarker, treatment interaction P = 0.01). Such benefit was not observed for subjects with {BRCA}1 methylation, {BRCA}1 {mRNA}-low tumors or a high score in a Myriad {HRD} assay. Significant interaction between treatment and the basal-like subtype was driven by high docetaxel response in the nonbasal subgroup. We conclude that patients with advanced {TNBC} benefit from characterization of {BRCA}1/2 mutations, but not {BRCA}1 methylation or Myriad {HRD} analyses, to inform choices on platinum-based chemotherapy. Additionally, gene expression analysis of basal-like cancers may also influence treatment selection.},
	pages = {628--637},
	number = {5},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Tutt, Andrew and Tovey, Holly and Cheang, Maggie Chon U. and Kernaghan, Sarah and Kilburn, Lucy and Gazinska, Patrycja and Owen, Julie and Abraham, Jacinta and Barrett, Sophie and Barrett-Lee, Peter and Brown, Robert and Chan, Stephen and Dowsett, Mitchell and Flanagan, James M. and Fox, Lisa and Grigoriadis, Anita and Gutin, Alexander and Harper-Wynne, Catherine and Hatton, Matthew Q. and Hoadley, Katherine A. and Parikh, Jyoti and Parker, Peter and Perou, Charles M. and Roylance, Rebecca and Shah, Vandna and Shaw, Adam and Smith, Ian E. and Timms, Kirsten M. and Wardley, Andrew M. and Wilson, Gregory and Gillett, Cheryl and Lanchbury, Jerry S. and Ashworth, Alan and Rahman, Nazneen and Harries, Mark and Ellis, Paul and Pinder, Sarah E. and Bliss, Judith M.},
	date = {2018},
	pmid = {29713086},
	pmcid = {PMC6372067},
	keywords = {Humans, Female, Mutation, {BRCA}1 Protein, {BRCA}2 Protein, Carboplatin, Homologous Recombination, Progression-Free Survival, Treatment Outcome, Triple Negative Breast Neoplasms},
	file = {Version acceptée:/Users/trislaz/Zotero/storage/63JFDTD8/Tutt et al. - 2018 - Carboplatin in BRCA12-mutated and triple-negative.pdf:application/pdf}
}

@article{schmauch_deep_2020,
	title = {A deep learning model to predict {RNA}-Seq expression of tumours from whole slide images},
	volume = {11},
	rights = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17678-4},
	doi = {10.1038/s41467-020-17678-4},
	abstract = {Deep learning methods for digital pathology analysis are an effective way to address multiple clinical questions, from diagnosis to prediction of treatment outcomes. These methods have also been used to predict gene mutations from pathology images, but no comprehensive evaluation of their potential for extracting molecular features from histology slides has yet been performed. We show that {HE}2RNA, a model based on the integration of multiple data modes, can be trained to systematically predict {RNA}-Seq profiles from whole-slide images alone, without expert annotation. Through its interpretable design, {HE}2RNA provides virtual spatialization of gene expression, as validated by {CD}3- and {CD}20-staining on an independent dataset. The transcriptomic representation learned by {HE}2RNA can also be transferred on other datasets, even of small size, to increase prediction performance for specific molecular phenotypes. We illustrate the use of this approach in clinical diagnosis purposes such as the identification of tumors with microsatellite instability.},
	pages = {3877},
	number = {1},
	journaltitle = {Nature Communications},
	author = {Schmauch, Benoît and Romagnoni, Alberto and Pronier, Elodie and Saillard, Charlie and Maillé, Pascale and Calderaro, Julien and Kamoun, Aurélie and Sefta, Meriem and Toldo, Sylvain and Zaslavskiy, Mikhail and Clozel, Thomas and Moarii, Matahi and Courtiol, Pierre and Wainrib, Gilles},
	urldate = {2020-11-12},
	date = {2020-08-03},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	file = {Snapshot:/Users/trislaz/Zotero/storage/JLQT9G55/s41467-020-17678-4.html:text/html;Full Text PDF:/Users/trislaz/Zotero/storage/VZVDMBU7/Schmauch et al. - 2020 - A deep learning model to predict RNA-Seq expressio.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/XN4I28L6/s41467-020-17678-4.html:text/html}
}

@article{popova_ploidy_2012-1,
	title = {Ploidy and Large-Scale Genomic Instability Consistently Identify Basal-like Breast Carcinomas with {BRCA}1/2 Inactivation},
	volume = {72},
	issn = {0008-5472, 1538-7445},
	url = {http://cancerres.aacrjournals.org/cgi/doi/10.1158/0008-5472.CAN-12-1470},
	doi = {10.1158/0008-5472.CAN-12-1470},
	abstract = {{BRCA}1 inactivation is a frequent event in basal-like breast carcinomas ({BLC}). However, {BRCA}1 can be inactivated by multiple mechanisms and determining its status is not a trivial issue. As an alternate approach, we proﬁled 65 {BLC} cases using single-nucleotide polymorphism arrays to deﬁne a signature of {BRCA}1-associated genomic instability. Large-scale state transitions ({LST}), deﬁned as chromosomal break between adjacent regions of at least 10 Mb, were found to be a robust indicator of {BRCA}1 status in this setting. Two major ploidy-speciﬁc cutoffs in {LST} distributions were sufﬁcient to distinguish highly rearranged {BLCs} with 85\% of proven {BRCA}1-inactivated cases from less rearranged {BLCs} devoid of proven {BRCA}1-inactivated cases. The genomic signature we deﬁned was validated in a second independent series of 55 primary {BLC} cases and 17 {BLC}-derived tumor cell lines. High numbers of {LSTs} resembling {BRCA}1inactivated {BLC} were observed in 4 primary {BLC} cases and 2 {BLC} cell lines that harbored {BRCA}2 mutations. Overall, the genomic signature we deﬁned predicted {BRCA}1/2 inactivation in {BLCs} with 100\% sensitivity and 90\% speciﬁcity (97\% accuracy). This assay may ease the challenge of selecting patients for genetic testing or recruitment to clinical trials of novel emerging therapies that target {DNA} repair deﬁciencies in cancer. Cancer Res; 72(21); 5454–62. Ó2012 {AACR}.},
	pages = {5454--5462},
	number = {21},
	journaltitle = {Cancer Research},
	shortjournal = {Cancer Research},
	author = {Popova, T. and Manie, E. and Rieunier, G. and Caux-Moncoutier, V. and Tirapo, C. and Dubois, T. and Delattre, O. and Sigal-Zafrani, B. and Bollet, M. and Longy, M. and Houdayer, C. and Sastre-Garau, X. and Vincent-Salomon, A. and Stoppa-Lyonnet, D. and Stern, M.-H.},
	urldate = {2020-11-12},
	date = {2012-11-01},
	langid = {english},
	file = {Popova et al. - 2012 - Ploidy and Large-Scale Genomic Instability Consist.pdf:/Users/trislaz/Zotero/storage/9J6I88QH/Popova et al. - 2012 - Ploidy and Large-Scale Genomic Instability Consist.pdf:application/pdf}
}

@article{birkbak_telomeric_2012,
	title = {Telomeric allelic imbalance indicates defective {DNA} repair and sensitivity to {DNA}-damaging agents},
	volume = {2},
	issn = {2159-8290},
	doi = {10.1158/2159-8290.CD-11-0206},
	abstract = {{DNA} repair competency is one determinant of sensitivity to certain chemotherapy drugs, such as cisplatin. Cancer cells with intact {DNA} repair can avoid the accumulation of genome damage during growth and also can repair platinum-induced {DNA} damage. We sought genomic signatures indicative of defective {DNA} repair in cell lines and tumors and correlated these signatures to platinum sensitivity. The number of subchromosomal regions with allelic imbalance extending to the telomere (N({tAI})) predicted cisplatin sensitivity in vitro and pathologic response to preoperative cisplatin treatment in patients with triple-negative breast cancer ({TNBC}). In serous ovarian cancer treated with platinum-based chemotherapy, higher levels of N({tAI}) forecast a better initial response. We found an inverse relationship between {BRCA}1 expression and N({tAI}) in sporadic {TNBC} and serous ovarian cancers without {BRCA}1 or {BRCA}2 mutation. Thus, accumulation of telomeric allelic imbalance is a marker of platinum sensitivity and suggests impaired {DNA} repair.
{SIGNIFICANCE}: Mutations in {BRCA} genes cause defects in {DNA} repair that predict sensitivity to {DNA} damaging agents, including platinum; however, some patients without {BRCA} mutations also benefit from these agents. {NtAI}, a genomic measure of unfaithfully repaired {DNA}, may identify cancer patients likely to benefit from treatments targeting defective {DNA} repair.},
	pages = {366--375},
	number = {4},
	journaltitle = {Cancer Discovery},
	shortjournal = {Cancer Discov},
	author = {Birkbak, Nicolai J. and Wang, Zhigang C. and Kim, Ji-Young and Eklund, Aron C. and Li, Qiyuan and Tian, Ruiyang and Bowman-Colin, Christian and Li, Yang and Greene-Colozzi, April and Iglehart, J. Dirk and Tung, Nadine and Ryan, Paula D. and Garber, Judy E. and Silver, Daniel P. and Szallasi, Zoltan and Richardson, Andrea L.},
	date = {2012-04},
	pmid = {22576213},
	pmcid = {PMC3806629},
	keywords = {Humans, Female, Mutation, Allelic Imbalance, Antineoplastic Agents, Cell Line, Tumor, Chromosome Aberrations, Cisplatin, {DNA} Damage, {DNA} Repair, Drug Resistance, Neoplasm, Genes, {BRCA}1, Models, Biological, Ovarian Neoplasms, {RNA}, Messenger, Telomere},
	file = {Texte intégral:/Users/trislaz/Zotero/storage/AL943KID/Birkbak et al. - 2012 - Telomeric allelic imbalance indicates defective DN.pdf:application/pdf}
}

@article{abkevich_patterns_2012,
	title = {Patterns of genomic loss of heterozygosity predict homologous recombination repair defects in epithelial ovarian cancer},
	volume = {107},
	rights = {2012 The Author(s)},
	issn = {1532-1827},
	url = {https://www.nature.com/articles/bjc2012451},
	doi = {10.1038/bjc.2012.451},
	abstract = {Defects in {BRCA}1, {BRCA}2, and other members of the homologous recombination pathway have potential therapeutic relevance when used to support agents that introduce or exploit double-stranded {DNA} breaks. This study examines the association between homologous recombination defects and genomic patterns of loss of heterozygosity ({LOH}).},
	pages = {1776--1782},
	number = {10},
	journaltitle = {British Journal of Cancer},
	author = {Abkevich, V. and Timms, K. M. and Hennessy, B. T. and Potter, J. and Carey, M. S. and Meyer, L. A. and Smith-{McCune}, K. and Broaddus, R. and Lu, K. H. and Chen, J. and Tran, T. V. and Williams, D. and Iliev, D. and Jammulapati, S. and {FitzGerald}, L. M. and Krivak, T. and {DeLoia}, J. A. and Gutin, A. and Mills, G. B. and Lanchbury, J. S.},
	urldate = {2020-11-12},
	date = {2012-11},
	langid = {english},
	note = {Number: 10
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/trislaz/Zotero/storage/V4FFRFC4/Abkevich et al. - 2012 - Patterns of genomic loss of heterozygosity predict.pdf:application/pdf;Snapshot:/Users/trislaz/Zotero/storage/A7C92UJ8/bjc2012451.html:text/html}
}

@article{zhou_brief_2018,
	title = {A brief introduction to weakly supervised learning},
	volume = {5},
	issn = {2095-5138, 2053-714X},
	url = {https://academic.oup.com/nsr/article/5/1/44/4093912},
	doi = {10.1093/nsr/nwx106},
	abstract = {Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.},
	pages = {44--53},
	number = {1},
	journaltitle = {National Science Review},
	author = {Zhou, Zhi-Hua},
	urldate = {2020-11-12},
	date = {2018-01-01},
	langid = {english},
	file = {Zhou - 2018 - A brief introduction to weakly supervised learning.pdf:/Users/trislaz/Zotero/storage/V8FU9N64/Zhou - 2018 - A brief introduction to weakly supervised learning.pdf:application/pdf}
}

@article{bhattacharjee_pooling_2020-1,
	title = {A Pooling Function based on Differential Evolution for Multiple Instance Learning},
	abstract = {While implementing Multiple Instance Learning ({MIL}) through Deep Neural Networks, the most important task is to design the bag level pooling function which defines the instance-to-bag relationship and eventually determines the class label of a bag. In this article, an {MIL} pooling function based on Differential Evolution ({DE}) – a bio-inspired metaheuristics is proposed for optimizing the instance weights parallelly with training the Deep Neural Network.},
	pages = {6},
	author = {Bhattacharjee, Kamanasish and Tiwari, Arti and Pant, Millie and Ahn, Chang Wook},
	date = {2020},
	langid = {english},
	file = {Bhattacharjee et al. - 2020 - A Pooling Function based on Differential Evolution.pdf:/Users/trislaz/Zotero/storage/ET53TNMC/Bhattacharjee et al. - 2020 - A Pooling Function based on Differential Evolution.pdf:application/pdf}
}

@article{serviansky_set2graph_2020,
	title = {Set2Graph: Learning Graphs From Sets},
	url = {http://arxiv.org/abs/2002.08772},
	shorttitle = {Set2Graph},
	abstract = {Many problems in machine learning can be cast as learning functions from sets to graphs, or more generally to hypergraphs; in short, Set2Graph functions. Examples include clustering, learning vertex and edge features on graphs, and learning features on triplets in a collection. A natural approach for building Set2Graph models is to characterize all linear equivariant set-to-hypergraph layers and stack them with non-linear activations. This poses two challenges: (i) the expressive power of these networks is not well understood; and (ii) these models would suffer from high, often intractable computational and memory complexity, as their dimension grows exponentially. This paper advocates a family of neural network models for learning Set2Graph functions that is both practical and of maximal expressive power (universal), that is, can approximate arbitrary continuous Set2Graph functions over compact sets. Testing these models on different machine learning tasks, mainly an application to particle physics, we find them favorable to existing baselines.},
	journaltitle = {{arXiv}:2002.08772 [cs, stat]},
	author = {Serviansky, Hadar and Segol, Nimrod and Shlomi, Jonathan and Cranmer, Kyle and Gross, Eilam and Maron, Haggai and Lipman, Yaron},
	urldate = {2020-11-13},
	date = {2020-06-24},
	eprinttype = {arxiv},
	eprint = {2002.08772},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/trislaz/Zotero/storage/RYCD3TVI/Serviansky et al. - 2020 - Set2Graph Learning Graphs From Sets.pdf:application/pdf;arXiv.org Snapshot:/Users/trislaz/Zotero/storage/3KK4CH9I/2002.html:text/html}
}